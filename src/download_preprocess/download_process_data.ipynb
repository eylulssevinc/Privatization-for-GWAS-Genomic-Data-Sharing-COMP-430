{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f722a496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PROJECT_ROOT = /Users/erkmenerken/Desktop/proje430\n",
      "\n",
      " Directories:\n",
      "  DATA_DIR      : /Users/erkmenerken/Desktop/proje430/data\n",
      "  RAW_HAPMAP_DIR : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap\n",
      "  GENO_DIR      : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/genotypes\n",
      "  PHASE_DIR     : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED\n",
      "  PHASE_META_DIR: /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2_meta\n",
      "  PROC_DIR      : /Users/erkmenerken/Desktop/proje430/data/processed/hapmap\n",
      "\n",
      " Inputs:\n",
      "  geno_chr2  : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/genotypes/genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz | exists: True\n",
      "  geno_chr10 : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/genotypes/genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz | exists: True\n",
      "  PHASE_CHR2 : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz | exists: False\n",
      "  PHASE_CHR10: /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz | exists: False\n",
      "  PHASE_INFO : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2_meta/hapmap3_r2_b36_fwd.consensus.qc.poly.info | exists: False\n",
      "\n",
      " Processed outputs:\n",
      "  REGION_CHR2 : /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/regions/CEU_chr2_5Mb.npz | exists: True\n",
      "  REGION_CHR10: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/regions/CEU_chr10_1Mb.npz | exists: True\n",
      "  COHORTS_JSON: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/cohorts/ceu_case_control_test_split.json | exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Robust repo root detection\n",
    "# -----------------------------\n",
    "# PROJECT_ROOT = nearest parent directory containing requirements.txt OR .git\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"requirements.txt\").exists() and not (PROJECT_ROOT / \".git\").exists():\n",
    "    for parent in PROJECT_ROOT.parents:\n",
    "        if (parent / \"requirements.txt\").exists() or (parent / \".git\").exists():\n",
    "            PROJECT_ROOT = parent\n",
    "            break\n",
    "\n",
    "print(\" PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "# -----------------------------\n",
    "# Canonical project directories (ALWAYS under repo-root/data)\n",
    "# -----------------------------\n",
    "DATA_DIR       = PROJECT_ROOT / \"data\"\n",
    "RAW_HAPMAP_DIR = DATA_DIR / \"raw\" / \"hapmap\"\n",
    "\n",
    "GENO_DIR       = RAW_HAPMAP_DIR / \"genotypes\"\n",
    "PHASE_DIR      = RAW_HAPMAP_DIR / \"phasing\" / \"HapMap3_r2\" / \"CEU\" / \"UNRELATED\"\n",
    "PHASE_META_DIR = RAW_HAPMAP_DIR / \"phasing\" / \"HapMap3_r2_meta\"\n",
    "\n",
    "PROC_DIR       = DATA_DIR / \"processed\" / \"hapmap\"\n",
    "REGION_DIR     = PROC_DIR / \"regions\"\n",
    "COHORT_DIR     = PROC_DIR / \"cohorts\"\n",
    "BLOCK_OUT_DIR  = PROC_DIR / \"blocks\"\n",
    "HAP_OUT_DIR    = PROC_DIR / \"haplotypes\"\n",
    "\n",
    "# Create folders if missing (safe; does not redo downloads)\n",
    "for d in [\n",
    "    DATA_DIR, RAW_HAPMAP_DIR,\n",
    "    GENO_DIR, PHASE_DIR, PHASE_META_DIR,\n",
    "    PROC_DIR, REGION_DIR, COHORT_DIR, BLOCK_OUT_DIR, HAP_OUT_DIR\n",
    "]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Expected file locations (repo-root/data/...)\n",
    "# -----------------------------\n",
    "# Genotype files (unphased)\n",
    "geno_chr2  = GENO_DIR / \"genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\"\n",
    "geno_chr10 = GENO_DIR / \"genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\"\n",
    "\n",
    "# Phased haplotype files (UNRELATED)\n",
    "PHASE_CHR2  = PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\"\n",
    "PHASE_CHR10 = PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\"\n",
    "\n",
    "# Meta info file\n",
    "PHASE_INFO = PHASE_META_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.info\"\n",
    "\n",
    "# Processed outputs\n",
    "REGION_CHR2  = REGION_DIR / \"CEU_chr2_5Mb.npz\"\n",
    "REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.npz\"\n",
    "COHORTS_JSON = COHORT_DIR / \"ceu_case_control_test_split.json\"\n",
    "\n",
    "# -----------------------------\n",
    "# Print debug summary\n",
    "# -----------------------------\n",
    "print(\"\\n Directories:\")\n",
    "print(\"  DATA_DIR      :\", DATA_DIR)\n",
    "print(\"  RAW_HAPMAP_DIR :\", RAW_HAPMAP_DIR)\n",
    "print(\"  GENO_DIR      :\", GENO_DIR)\n",
    "print(\"  PHASE_DIR     :\", PHASE_DIR)\n",
    "print(\"  PHASE_META_DIR:\", PHASE_META_DIR)\n",
    "print(\"  PROC_DIR      :\", PROC_DIR)\n",
    "\n",
    "print(\"\\n Inputs:\")\n",
    "print(\"  geno_chr2  :\", geno_chr2,  \"| exists:\", geno_chr2.exists())\n",
    "print(\"  geno_chr10 :\", geno_chr10, \"| exists:\", geno_chr10.exists())\n",
    "print(\"  PHASE_CHR2 :\", PHASE_CHR2, \"| exists:\", PHASE_CHR2.exists())\n",
    "print(\"  PHASE_CHR10:\", PHASE_CHR10,\"| exists:\", PHASE_CHR10.exists())\n",
    "print(\"  PHASE_INFO :\", PHASE_INFO, \"| exists:\", PHASE_INFO.exists())\n",
    "\n",
    "print(\"\\n Processed outputs:\")\n",
    "print(\"  REGION_CHR2 :\", REGION_CHR2, \"| exists:\", REGION_CHR2.exists())\n",
    "print(\"  REGION_CHR10:\", REGION_CHR10,\"| exists:\", REGION_CHR10.exists())\n",
    "print(\"  COHORTS_JSON:\", COHORTS_JSON,\"| exists:\", COHORTS_JSON.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f32ffb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Manifest entries: 5\n",
      "- CEU genotypes chr2 (unphased) -> data/raw/hapmap/genotypes/genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\n",
      "- CEU genotypes chr10 (unphased) -> data/raw/hapmap/genotypes/genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\n",
      "- CEU phased haplotypes chr2 (UNRELATED) -> data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      "- CEU phased haplotypes chr10 (UNRELATED) -> data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n",
      "- HapMap3 r2 SNP meta (.info) -> data/raw/hapmap/phasing/HapMap3_r2_meta/hapmap3_r2_b36_fwd.consensus.qc.poly.info\n"
     ]
    }
   ],
   "source": [
    "# Sources used:\n",
    "# - Genotypes (CEU, non-redundant, b36, forward): chr2 + chr10\n",
    "# - Phased haplotypes (HapMap3 r2, CEU, UNRELATED): chr2 + chr10\n",
    "# - Small meta file in HapMap3 r2 directory (.info)\n",
    "\n",
    "MANIFEST = [\n",
    "    {\n",
    "        \"name\": \"CEU genotypes chr2 (unphased)\",\n",
    "        \"url\": \"https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/latest_phaseII+III_ncbi_b36/forward/non-redundant/genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\",\n",
    "        \"dst\": GENO_DIR / \"genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CEU genotypes chr10 (unphased)\",\n",
    "        \"url\": \"https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/latest_phaseII+III_ncbi_b36/forward/non-redundant/genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\",\n",
    "        \"dst\": GENO_DIR / \"genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CEU phased haplotypes chr2 (UNRELATED)\",\n",
    "        \"url\": \"https://ftp.ncbi.nlm.nih.gov/hapmap/phasing/2009-02_phaseIII/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\",\n",
    "        \"dst\": PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CEU phased haplotypes chr10 (UNRELATED)\",\n",
    "        \"url\": \"https://ftp.ncbi.nlm.nih.gov/hapmap/phasing/2009-02_phaseIII/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\",\n",
    "        \"dst\": PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"HapMap3 r2 SNP meta (.info)\",\n",
    "        \"url\": \"https://ftp.ncbi.nlm.nih.gov/hapmap/phasing/2009-02_phaseIII/HapMap3_r2/hapmap3_r2_b36_fwd.consensus.qc.poly.info\",\n",
    "        \"dst\": PHASE_META_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.info\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\" Manifest entries: {len(MANIFEST)}\")\n",
    "for x in MANIFEST:\n",
    "    print(\"-\", x[\"name\"], \"->\", x[\"dst\"].relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87437182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def download_file(url: str, dst: Path, overwrite: bool = False, chunk_size: int = 1024 * 1024) -> None:\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if dst.exists() and not overwrite:\n",
    "        size_bytes = dst.stat().st_size\n",
    "        if size_bytes > 0:\n",
    "            print(f\"  Already exists, skipping: {dst.name} ({size_bytes/1e6:.2f} MB)\")\n",
    "            return\n",
    "        print(f\"  Existing file is empty, re-downloading: {dst.name}\")\n",
    "\n",
    "    tmp = dst.with_suffix(dst.suffix + \".part\")\n",
    "\n",
    "    print(f\"  Downloading: {url}\")\n",
    "    print(f\" Saving to  : {dst}\")\n",
    "\n",
    "    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    with urllib.request.urlopen(req) as resp:\n",
    "        total = resp.headers.get(\"Content-Length\")\n",
    "        total = int(total) if total is not None else None\n",
    "\n",
    "        with open(tmp, \"wb\") as f, tqdm(\n",
    "            total=total, unit=\"B\", unit_scale=True, unit_divisor=1024, desc=dst.name\n",
    "        ) as pbar:\n",
    "            while True:\n",
    "                chunk = resp.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "\n",
    "    os.replace(tmp, dst)  # atomic move\n",
    "    print(f\" Done: {dst.name} ({dst.stat().st_size/1e6:.2f} MB)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10b8402a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting HapMap downloads...\n",
      "\n",
      "=== CEU genotypes chr2 (unphased) ===\n",
      "  Already exists, skipping: genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz (21.10 MB)\n",
      "=== CEU genotypes chr10 (unphased) ===\n",
      "  Already exists, skipping: genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz (13.63 MB)\n",
      "=== CEU phased haplotypes chr2 (UNRELATED) ===\n",
      "  Downloading: https://ftp.ncbi.nlm.nih.gov/hapmap/phasing/2009-02_phaseIII/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      " Saving to  : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.72M/1.72M [00:00<00:00, 2.02MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done: hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz (1.80 MB)\n",
      "\n",
      "=== CEU phased haplotypes chr10 (UNRELATED) ===\n",
      "  Downloading: https://ftp.ncbi.nlm.nih.gov/hapmap/phasing/2009-02_phaseIII/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n",
      " Saving to  : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.07M/1.07M [00:00<00:00, 1.19MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done: hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz (1.13 MB)\n",
      "\n",
      "=== HapMap3 r2 SNP meta (.info) ===\n",
      "  Downloading: https://ftp.ncbi.nlm.nih.gov/hapmap/phasing/2009-02_phaseIII/HapMap3_r2/hapmap3_r2_b36_fwd.consensus.qc.poly.info\n",
      " Saving to  : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2_meta/hapmap3_r2_b36_fwd.consensus.qc.poly.info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hapmap3_r2_b36_fwd.consensus.qc.poly.info: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33.8k/33.8k [00:00<00:00, 256kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done: hapmap3_r2_b36_fwd.consensus.qc.poly.info (0.03 MB)\n",
      "\n",
      " All requested HapMap files are present in data/raw/hapmap/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" Starting HapMap downloads...\\n\")\n",
    "\n",
    "for item in MANIFEST:\n",
    "    print(f\"=== {item['name']} ===\")\n",
    "    download_file(item[\"url\"], item[\"dst\"], overwrite=False)\n",
    "\n",
    "print(\" All requested HapMap files are present in data/raw/hapmap/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1537923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloaded files:\n",
      "data/raw/hapmap/genotypes/genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz - 21.10 MB\n",
      "data/raw/hapmap/genotypes/genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz - 13.63 MB\n",
      "data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz - 1.80 MB\n",
      "data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz - 1.13 MB\n",
      "data/raw/hapmap/phasing/HapMap3_r2_meta/hapmap3_r2_b36_fwd.consensus.qc.poly.info - 0.03 MB\n"
     ]
    }
   ],
   "source": [
    "def human_mb(n_bytes: int) -> str:\n",
    "    return f\"{n_bytes/1e6:.2f} MB\"\n",
    "\n",
    "print(\" Downloaded files:\")\n",
    "for item in MANIFEST:\n",
    "    p = item[\"dst\"]\n",
    "    if p.exists():\n",
    "        print(p.relative_to(PROJECT_ROOT), \"-\", human_mb(p.stat().st_size))\n",
    "    else:\n",
    "        print(\"MISSING:\", p.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80fdf0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Project root: /Users/erkmenerken/Desktop/proje430\n",
      " Raw HapMap  : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap\n",
      " Genotypes   : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/genotypes\n",
      " Phasing     : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED\n",
      " Processed   : /Users/erkmenerken/Desktop/proje430/data/processed/hapmap\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(name=\"430_project\") -> Path:\n",
    "    cwd = Path.cwd()\n",
    "    if cwd.name == name:\n",
    "        return cwd\n",
    "    for p in [cwd] + list(cwd.parents):\n",
    "        if p.name == name:\n",
    "            return p\n",
    "    raise RuntimeError(f\"Could not find project root folder named '{name}' from {cwd}\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(\"proje430\")\n",
    "\n",
    "RAW_HAPMAP = PROJECT_ROOT / \"data\" / \"raw\" / \"hapmap\"\n",
    "GENO_DIR   = RAW_HAPMAP / \"genotypes\"\n",
    "PHASE_DIR  = RAW_HAPMAP / \"phasing\" / \"HapMap3_r2\" / \"CEU\" / \"UNRELATED\"\n",
    "PHASE_META_DIR = RAW_HAPMAP / \"phasing\" / \"HapMap3_r2_meta\"\n",
    "PHASE_META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROC_DIR   = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\"\n",
    "REGION_DIR = PROC_DIR / \"regions\"\n",
    "BLOCK_DIR  = PROC_DIR / \"blocks\"\n",
    "COHORT_DIR = PROC_DIR / \"cohorts\"\n",
    "\n",
    "for d in [PROC_DIR, REGION_DIR, BLOCK_DIR, COHORT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\" Project root:\", PROJECT_ROOT)\n",
    "print(\" Raw HapMap  :\", RAW_HAPMAP)\n",
    "print(\" Genotypes   :\", GENO_DIR)\n",
    "print(\" Phasing     :\", PHASE_DIR)\n",
    "print(\" Processed   :\", PROC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7bd40fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Peeking: genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\n",
      "  Line 1: rs# alleles chrom pos strand assembly# center protLSID assayLSID panelLSID QCcode NA06984 NA06985 NA06986 NA06989 NA06991 NA06993 NA06994 NA06995 NA06997 NA07000 NA07014 NA07019 NA07022 NA07029 NA07031 NA07034 NA07037 NA\n",
      "  Line 2: rs10171150 A/G chr2 2091 + ncbi_b36 mcgill-gqic urn:LSID:illumina.hapmap.org:Protocol:Golden_Gate_1.0.0:1 urn:LSID:mcgill-gqic.hapmap.org:Assay:810448:1 urn:lsid:dcc.hapmap.org:Panel:CEPH-30-trios:1 QC+ NN GG NN NN GG GG\n",
      "\n",
      "ðŸ”Ž Peeking: genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\n",
      "  Line 1: rs# alleles chrom pos strand assembly# center protLSID assayLSID panelLSID QCcode NA06984 NA06985 NA06986 NA06989 NA06991 NA06993 NA06994 NA06995 NA06997 NA07000 NA07014 NA07019 NA07022 NA07029 NA07031 NA07034 NA07037 NA\n",
      "  Line 2: rs11511647 C/T chr10 62765 + ncbi_b36 sanger urn:lsid:illumina.hapmap.org:Protocol:Golden_Gate_1.0.0:1 urn:lsid:sanger.hapmap.org:Assay:4310385:1 urn:lsid:dcc.hapmap.org:Panel:CEPH-30-trios:1 QC+ NN CT NN NN CT CT CT NN\n",
      "\n",
      "ðŸ”Ž Peeking: hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      "  Line 1: rsID position_b36 NA06989_A NA06989_B NA10850_A NA10850_B NA06984_A NA06984_B NA11917_A NA11917_B NA12282_A NA12282_B NA12283_A NA12283_B NA11918_A NA11918_B NA12413_A NA12413_B NA07056_A NA07056_B NA12044_A NA12044_B NA\n",
      "  Line 2: rs10181821 5703 A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n",
      "\n",
      "ðŸ”Ž Peeking: hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n",
      "  Line 1: rsID position_b36 NA06989_A NA06989_B NA10850_A NA10850_B NA06984_A NA06984_B NA11917_A NA11917_B NA12282_A NA12282_B NA12283_A NA12283_B NA11918_A NA11918_B NA12413_A NA12413_B NA07056_A NA07056_B NA12044_A NA12044_B NA\n",
      "  Line 2: rs12255619 88481 A C A A A A A A A A A C A A A A A A A A A A A A A A A A A A A A A A\n",
      "\n",
      " If the header looks like: rs# alleles chrom pos ... QCcode NA06984 ... then weâ€™re good.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "geno_chr2  = GENO_DIR / \"genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\"\n",
    "geno_chr10 = GENO_DIR / \"genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\"\n",
    "phase_chr2  = PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\"\n",
    "phase_chr10 = PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\"\n",
    "\n",
    "def peek_gz(path, n_lines=2, maxchars=220):\n",
    "    print(f\"\\nðŸ”Ž Peeking: {path.name}\")\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for i in range(n_lines):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            print(f\"  Line {i+1}: {line[:maxchars].rstrip()}\")\n",
    "\n",
    "for p in [geno_chr2, geno_chr10, phase_chr2, phase_chr10]:\n",
    "    if p.exists():\n",
    "        peek_gz(p, n_lines=2)\n",
    "    else:\n",
    "        print(\" Missing:\", p)\n",
    "\n",
    "print(\"\\n If the header looks like: rs# alleles chrom pos ... QCcode NA06984 ... then weâ€™re good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bbbc1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reading rs# + pos from genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz (whitespace-separated)...\n",
      " Loaded 329831 SNP positions.\n",
      " Reading rs# + pos from genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz (whitespace-separated)...\n",
      " Loaded 211711 SNP positions.\n",
      "\n",
      " Proposal-style windows selected:\n",
      "chr2 (5Mb, ~311 SNPs): {'start_bp': 242050505, 'end_bp': 247050505, 'snps_in_window': 311, 'end_pos_actual': 242742878, 'total_snps_chr': 329831, 'min_pos': 2091, 'max_pos': 242742878}\n",
      "chr10 (1Mb, ~610 SNPs): {'start_bp': 38322261, 'end_bp': 39322261, 'snps_in_window': 610, 'end_pos_actual': 39194226, 'total_snps_chr': 211711, 'min_pos': 62765, 'max_pos': 135373179}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_positions_rsids(geno_path):\n",
    "    print(f\" Reading rs# + pos from {geno_path.name} (whitespace-separated)...\")\n",
    "    df = pd.read_csv(\n",
    "        geno_path,\n",
    "        sep=r\"\\s+\",\n",
    "        engine=\"python\",\n",
    "        compression=\"gzip\",\n",
    "        usecols=[\"rs#\", \"pos\"],\n",
    "        dtype={\"rs#\": str, \"pos\": int},\n",
    "    )\n",
    "    df = df.dropna().sort_values(\"pos\").reset_index(drop=True)\n",
    "    print(f\" Loaded {len(df)} SNP positions.\")\n",
    "    return df\n",
    "\n",
    "def choose_window_by_bp(df_pos, window_bp, target_snps):\n",
    "    pos = df_pos[\"pos\"].to_numpy(np.int64)\n",
    "    n = len(pos)\n",
    "    ends = np.searchsorted(pos, pos + window_bp, side=\"right\")\n",
    "    counts = ends - np.arange(n)\n",
    "\n",
    "    diff = np.abs(counts - target_snps)\n",
    "    best_i = int(np.argmin(diff))\n",
    "\n",
    "    start = int(pos[best_i])\n",
    "    end = int(start + window_bp)\n",
    "    count = int(counts[best_i])\n",
    "    end_idx = int(ends[best_i] - 1)\n",
    "    end_pos_actual = int(pos[end_idx]) if end_idx >= best_i else start\n",
    "\n",
    "    return {\n",
    "        \"start_bp\": start,\n",
    "        \"end_bp\": end,\n",
    "        \"snps_in_window\": count,\n",
    "        \"end_pos_actual\": end_pos_actual,\n",
    "        \"total_snps_chr\": int(n),\n",
    "        \"min_pos\": int(pos[0]),\n",
    "        \"max_pos\": int(pos[-1]),\n",
    "    }\n",
    "\n",
    "df2  = load_positions_rsids(geno_chr2)\n",
    "df10 = load_positions_rsids(geno_chr10)\n",
    "\n",
    "chr2_plan  = choose_window_by_bp(df2,  window_bp=5_000_000, target_snps=311)\n",
    "chr10_plan = choose_window_by_bp(df10, window_bp=1_000_000, target_snps=610)\n",
    "\n",
    "print(\"\\n Proposal-style windows selected:\")\n",
    "print(\"chr2 (5Mb, ~311 SNPs):\", chr2_plan)\n",
    "print(\"chr10 (1Mb, ~610 SNPs):\", chr10_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff17848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "MISSING_TOKENS = {\"\", \"NN\", \"NA\", \"N\", \"00\", \"--\", \"??\"}\n",
    "\n",
    "def _find_sample_columns(columns):\n",
    "    cols = list(columns)\n",
    "    if \"QCcode\" in cols:\n",
    "        qc_idx = cols.index(\"QCcode\")\n",
    "        return cols[:qc_idx + 1], cols[qc_idx + 1:]\n",
    "    # Fallback (rarely needed for HapMap)\n",
    "    return cols[:11], cols[11:]\n",
    "\n",
    "def genotype_row_to_minor_counts(geno_strs, allele_a, allele_b):\n",
    "    \"\"\"\n",
    "    Convert HapMap genotype strings (e.g., 'AA', 'AG', 'GG', 'NN') to\n",
    "    minor-allele counts in {0,1,2} with -1 for missing.\n",
    "\n",
    "    Minor allele is determined per SNP by observed allele counts in the row.\n",
    "    \"\"\"\n",
    "    g = np.asarray(geno_strs, dtype=object)\n",
    "    g = np.array([x.strip() if isinstance(x, str) else \"\" for x in g], dtype=object)\n",
    "\n",
    "    missing = np.zeros(len(g), dtype=bool)\n",
    "    for t in MISSING_TOKENS:\n",
    "        missing |= (g == t)\n",
    "\n",
    "    # valid genotype strings are length 2 (e.g. \"AG\")\n",
    "    valid = (~missing) & np.array([len(x) == 2 for x in g], dtype=bool)\n",
    "\n",
    "    # count allele occurrences among valid entries\n",
    "    a_count = 0\n",
    "    b_count = 0\n",
    "    for x in g[valid]:\n",
    "        a_count += (x[0] == allele_a) + (x[1] == allele_a)\n",
    "        b_count += (x[0] == allele_b) + (x[1] == allele_b)\n",
    "\n",
    "    # choose minor allele (ties deterministic)\n",
    "    if a_count < b_count:\n",
    "        minor, major = allele_a, allele_b\n",
    "    elif b_count < a_count:\n",
    "        minor, major = allele_b, allele_a\n",
    "    else:\n",
    "        minor, major = allele_b, allele_a\n",
    "\n",
    "    out = np.full(len(g), -1, dtype=np.int8)\n",
    "    for i, x in enumerate(g):\n",
    "        if (not isinstance(x, str)) or (x in MISSING_TOKENS) or len(x) != 2:\n",
    "            continue\n",
    "        out[i] = np.int8((x[0] == minor) + (x[1] == minor))\n",
    "\n",
    "    return out, minor, major\n",
    "\n",
    "def parse_genotypes_window(geno_path, start_bp, end_bp, chunksize=2000):\n",
    "    \"\"\"\n",
    "    Parses HapMap whitespace-separated genotype .txt.gz file and extracts SNPs\n",
    "    within [start_bp, end_bp]. Returns:\n",
    "      G (M x N): int8 minor allele counts, missing=-1\n",
    "      sample_ids (M,)\n",
    "      snp_ids (N,)\n",
    "      positions (N,)\n",
    "      minor_alleles (N,)\n",
    "      major_alleles (N,)\n",
    "    \"\"\"\n",
    "    print(f\"\\n Parsing genotypes from {geno_path.name}\")\n",
    "    print(f\"   Window: [{start_bp}, {end_bp}] bp (inclusive)\")\n",
    "\n",
    "    # Read header row only (detect columns + sample IDs)\n",
    "    header = pd.read_csv(\n",
    "        geno_path,\n",
    "        sep=r\"\\s+\",\n",
    "        engine=\"python\",\n",
    "        compression=\"gzip\",\n",
    "        nrows=1,\n",
    "        dtype=str\n",
    "    )\n",
    "    _, sample_cols = _find_sample_columns(header.columns)\n",
    "    sample_ids = np.array(sample_cols, dtype=object)\n",
    "    print(f\" Individuals detected: {len(sample_ids)}\")\n",
    "\n",
    "    G_cols, snp_ids, positions, minor_alleles, major_alleles = [], [], [], [], []\n",
    "\n",
    "    reader = pd.read_csv(\n",
    "        geno_path,\n",
    "        sep=r\"\\s+\",\n",
    "        engine=\"python\",\n",
    "        compression=\"gzip\",\n",
    "        dtype=str,\n",
    "        chunksize=chunksize,\n",
    "    )\n",
    "\n",
    "    kept = 0\n",
    "    for chunk in tqdm(reader, desc=f\"Reading {geno_path.name}\"):\n",
    "        # Filter by window using numeric positions\n",
    "        pos_int = pd.to_numeric(chunk[\"pos\"], errors=\"coerce\")\n",
    "        mask = (pos_int >= start_bp) & (pos_int <= end_bp)\n",
    "        chunk = chunk.loc[mask]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        for _, row in chunk.iterrows():\n",
    "            rsid = row.get(\"rs#\", None)\n",
    "            alleles = row.get(\"alleles\", None)\n",
    "            pos = row.get(\"pos\", None)\n",
    "\n",
    "            if rsid is None or alleles is None or pos is None or \"/\" not in alleles:\n",
    "                continue\n",
    "\n",
    "            a, b = [x.strip() for x in alleles.split(\"/\", 1)]\n",
    "            if len(a) != 1 or len(b) != 1:\n",
    "                continue\n",
    "\n",
    "            geno_strs = row[sample_cols].values\n",
    "            counts, minor, major = genotype_row_to_minor_counts(geno_strs, a, b)\n",
    "\n",
    "            G_cols.append(counts)\n",
    "            snp_ids.append(rsid)\n",
    "            positions.append(int(pos))\n",
    "            minor_alleles.append(minor)\n",
    "            major_alleles.append(major)\n",
    "            kept += 1\n",
    "\n",
    "    if kept == 0:\n",
    "        raise RuntimeError(\"No SNPs were kept. Check start/end window values.\")\n",
    "\n",
    "    G = np.stack(G_cols, axis=1)  # (M, N)\n",
    "    positions = np.array(positions, dtype=np.int32)\n",
    "\n",
    "    # Sort by position (just in case)\n",
    "    order = np.argsort(positions)\n",
    "    G = G[:, order]\n",
    "    positions = positions[order]\n",
    "    snp_ids = np.array(snp_ids, dtype=object)[order]\n",
    "    minor_alleles = np.array(minor_alleles, dtype=object)[order]\n",
    "    major_alleles = np.array(major_alleles, dtype=object)[order]\n",
    "\n",
    "    print(f\" Kept SNPs: {G.shape[1]} | Individuals: {G.shape[0]}\")\n",
    "    print(f\" Missing rate: {float(np.mean(G == -1)):.4f}\")\n",
    "    print(f\" Kept position range: {int(positions.min())} .. {int(positions.max())}\")\n",
    "\n",
    "    return G, sample_ids, snp_ids, positions, minor_alleles, major_alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2b8decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Parsing genotypes from genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\n",
      "   Window: [242050505, 247050505] bp (inclusive)\n",
      " Individuals detected: 174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz: 165it [00:23,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Kept SNPs: 311 | Individuals: 174\n",
      " Missing rate: 0.2843\n",
      " Kept position range: 242050505 .. 242742878\n",
      " Saved: data/processed/hapmap/regions/CEU_chr2_5Mb.npz\n",
      " Meta : data/processed/hapmap/regions/CEU_chr2_5Mb.meta.json\n",
      "\n",
      "\n",
      " Parsing genotypes from genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\n",
      "   Window: [38322261, 39322261] bp (inclusive)\n",
      " Individuals detected: 174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz: 106it [00:14,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Kept SNPs: 610 | Individuals: 174\n",
      " Missing rate: 0.3864\n",
      " Kept position range: 38322261 .. 39194226\n",
      " Saved: data/processed/hapmap/regions/CEU_chr10_1Mb.npz\n",
      " Meta : data/processed/hapmap/regions/CEU_chr10_1Mb.meta.json\n",
      "\n",
      " Region extraction complete.\n",
      " Check: data/processed/hapmap/regions/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json, time\n",
    "\n",
    "def save_region(region_name, chrom, plan, G, sample_ids, snp_ids, positions, minor, major):\n",
    "    out_npz = REGION_DIR / f\"{region_name}.npz\"\n",
    "    out_meta = REGION_DIR / f\"{region_name}.meta.json\"\n",
    "\n",
    "    np.savez_compressed(\n",
    "        out_npz,\n",
    "        G=G.astype(np.int8),\n",
    "        sample_ids=sample_ids,\n",
    "        snp_ids=snp_ids,\n",
    "        positions=positions,\n",
    "        minor_alleles=minor,\n",
    "        major_alleles=major,\n",
    "        chrom=str(chrom),\n",
    "    )\n",
    "\n",
    "    meta = {\n",
    "        \"region_name\": region_name,\n",
    "        \"chrom\": str(chrom),\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"window_start_bp\": int(plan[\"start_bp\"]),\n",
    "        \"window_end_bp\": int(plan[\"end_bp\"]),\n",
    "        \"snps_in_window\": int(G.shape[1]),\n",
    "        \"individuals\": int(G.shape[0]),\n",
    "        \"missing_rate\": float(np.mean(G == -1)),\n",
    "        \"note\": \"G is minor-allele count in {0,1,2}, missing=-1.\",\n",
    "    }\n",
    "    out_meta.write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "    print(f\" Saved: {out_npz.relative_to(PROJECT_ROOT)}\")\n",
    "    print(f\" Meta : {out_meta.relative_to(PROJECT_ROOT)}\\n\")\n",
    "\n",
    "# chr2\n",
    "G2, ids2, rs2, pos2, min2, maj2 = parse_genotypes_window(geno_chr2, chr2_plan[\"start_bp\"], chr2_plan[\"end_bp\"])\n",
    "save_region(\"CEU_chr2_5Mb\", 2, chr2_plan, G2, ids2, rs2, pos2, min2, maj2)\n",
    "\n",
    "# chr10\n",
    "G10, ids10, rs10, pos10, min10, maj10 = parse_genotypes_window(geno_chr10, chr10_plan[\"start_bp\"], chr10_plan[\"end_bp\"])\n",
    "save_region(\"CEU_chr10_1Mb\", 10, chr10_plan, G10, ids10, rs10, pos10, min10, maj10)\n",
    "\n",
    "print(\" Region extraction complete.\")\n",
    "print(\" Check: data/processed/hapmap/regions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70c6eb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading regions for cohort split:\n",
      "  - data/processed/hapmap/regions/CEU_chr2_5Mb.npz\n",
      "  - data/processed/hapmap/regions/CEU_chr10_1Mb.npz\n",
      " Analysis pool size (chr2 âˆ© chr10 sample_ids): 174\n",
      "\n",
      " Created CEU case/control/test split (HapMap-only)\n",
      "   counts: {'control': 80, 'case': 50, 'test': 44}\n",
      " Saved: data/processed/hapmap/cohorts/ceu_case_control_test_split.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- REQUIREMENTS ----\n",
    "# PROJECT_ROOT must already be defined correctly.\n",
    "\n",
    "REGION_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\" / \"regions\"\n",
    "COHORT_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\" / \"cohorts\"\n",
    "COHORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REGION_CHR2 = REGION_DIR / \"CEU_chr2_5Mb.npz\"\n",
    "REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.npz\"\n",
    "\n",
    "if not REGION_CHR2.exists():\n",
    "    raise RuntimeError(f\"Missing {REGION_CHR2}. Run region extraction first.\")\n",
    "if not REGION_CHR10.exists():\n",
    "    raise RuntimeError(f\"Missing {REGION_CHR10}. Run region extraction first.\")\n",
    "\n",
    "print(\" Loading regions for cohort split:\")\n",
    "print(\"  -\", REGION_CHR2.relative_to(PROJECT_ROOT))\n",
    "print(\"  -\", REGION_CHR10.relative_to(PROJECT_ROOT))\n",
    "\n",
    "# Load sample IDs from saved region files\n",
    "r2 = np.load(REGION_CHR2, allow_pickle=True)\n",
    "r10 = np.load(REGION_CHR10, allow_pickle=True)\n",
    "\n",
    "# Use intersection of individuals available in BOTH regions\n",
    "ids2 = [str(x) for x in r2[\"sample_ids\"]]\n",
    "ids10 = set(str(x) for x in r10[\"sample_ids\"])\n",
    "analysis_pool = sorted(set(ids2).intersection(ids10))\n",
    "\n",
    "print(f\" Analysis pool size (chr2 âˆ© chr10 sample_ids): {len(analysis_pool)}\")\n",
    "if len(analysis_pool) < 20:\n",
    "    raise RuntimeError(\n",
    "        \"Too few individuals to split into case/control/test meaningfully. \"\n",
    "        \"If this happens, re-extract regions or choose a larger window.\"\n",
    "    )\n",
    "\n",
    "# ---- SPLIT (prefer larger control) ----\n",
    "# Target (for n=174): control=80, case=50, test=44\n",
    "# If n differs, we scale these proportions to still work.\n",
    "rng = np.random.default_rng(0)\n",
    "shuffled = list(rng.permutation(analysis_pool))\n",
    "n = len(shuffled)\n",
    "\n",
    "target_total = 174\n",
    "target_control, target_case, target_test = 80, 50, 44\n",
    "\n",
    "if n == target_total:\n",
    "    n_control, n_case, n_test = target_control, target_case, target_test\n",
    "else:\n",
    "    # scale by proportions from the 174-person target\n",
    "    p_control = target_control / target_total\n",
    "    p_case = target_case / target_total\n",
    "\n",
    "    n_control = int(round(n * p_control))\n",
    "    n_case = int(round(n * p_case))\n",
    "    # ensure at least 2 in each group\n",
    "    n_control = max(2, n_control)\n",
    "    n_case = max(2, n_case)\n",
    "    n_test = n - n_control - n_case\n",
    "\n",
    "    # fix rounding issues + ensure test >= 2\n",
    "    if n_test < 2:\n",
    "        need = 2 - n_test\n",
    "        # take from the larger of control/case, but never below 2\n",
    "        if n_control - need >= 2:\n",
    "            n_control -= need\n",
    "        elif n_case - need >= 2:\n",
    "            n_case -= need\n",
    "        else:\n",
    "            # extreme fallback\n",
    "            n_control = max(2, n - 4)\n",
    "            n_case = 2\n",
    "        n_test = n - n_control - n_case\n",
    "\n",
    "# Assign groups (NOTE: order in JSON is not important; they are disjoint)\n",
    "control_ids = shuffled[:n_control]\n",
    "case_ids = shuffled[n_control:n_control + n_case]\n",
    "test_ids = shuffled[n_control + n_case:]\n",
    "\n",
    "assert len(set(control_ids) & set(case_ids)) == 0\n",
    "assert len(set(control_ids) & set(test_ids)) == 0\n",
    "assert len(set(case_ids) & set(test_ids)) == 0\n",
    "assert len(control_ids) + len(case_ids) + len(test_ids) == n\n",
    "\n",
    "# Also store indices into the CEU genotype matrix order (region NPZ order)\n",
    "# We'll define the \"CEU matrix order\" as the sample_ids order in chr2 NPZ.\n",
    "ceu_order = [str(x) for x in r2[\"sample_ids\"]]\n",
    "idx_map = {sid: i for i, sid in enumerate(ceu_order)}\n",
    "\n",
    "def ids_to_indices(ids):\n",
    "    missing = [sid for sid in ids if sid not in idx_map]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Some cohort IDs not found in CEU matrix order (chr2 NPZ): {missing[:10]}\")\n",
    "    return [int(idx_map[sid]) for sid in ids]\n",
    "\n",
    "cohorts = {\n",
    "    \"notes\": {\n",
    "        \"case_source\": \"HapMap CEU (synthetic split; no PGP used)\",\n",
    "        \"control_source\": \"HapMap CEU (synthetic split; larger control for stable reference frequencies)\",\n",
    "        \"test_source\": \"HapMap CEU (synthetic split)\",\n",
    "        \"analysis_pool\": \"intersection of sample_ids in processed chr2/ch10 region NPZ files\",\n",
    "        \"seed\": 0,\n",
    "        \"target_counts_if_n174\": {\"control\": 80, \"case\": 50, \"test\": 44},\n",
    "    },\n",
    "    \"ceu_matrix_order_source\": \"CEU_chr2_5Mb.npz sample_ids order\",\n",
    "    \"pool_order_sorted\": analysis_pool,\n",
    "    \"control\": {\n",
    "        \"sample_ids\": [str(x) for x in control_ids],\n",
    "        \"indices_in_ceu_matrix\": ids_to_indices(control_ids),\n",
    "    },\n",
    "    \"case\": {\n",
    "        \"sample_ids\": [str(x) for x in case_ids],\n",
    "        \"indices_in_ceu_matrix\": ids_to_indices(case_ids),\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"sample_ids\": [str(x) for x in test_ids],\n",
    "        \"indices_in_ceu_matrix\": ids_to_indices(test_ids),\n",
    "    },\n",
    "    \"counts\": {\"control\": len(control_ids), \"case\": len(case_ids), \"test\": len(test_ids)},\n",
    "}\n",
    "\n",
    "out_path = COHORT_DIR / \"ceu_case_control_test_split.json\"\n",
    "out_path.write_text(json.dumps(cohorts, indent=2))\n",
    "\n",
    "print(\"\\n Created CEU case/control/test split (HapMap-only)\")\n",
    "print(\"   counts:\", cohorts[\"counts\"])\n",
    "print(\" Saved:\", out_path.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31839894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded cohorts: data/processed/hapmap/cohorts/ceu_case_control_test_split.json\n",
      "Counts: {'control': 80, 'case': 50, 'test': 44}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "COHORTS_PATH = COHORT_DIR / \"ceu_case_control_test_split.json\"\n",
    "cohorts = json.loads(COHORTS_PATH.read_text())\n",
    "\n",
    "print(\" Loaded cohorts:\", COHORTS_PATH.relative_to(PROJECT_ROOT))\n",
    "print(\"Counts:\", cohorts[\"counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8061413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading region NPZ files + cohort split JSON from disk...\n",
      "chr2 : control n=80 | test n=44\n",
      "chr10: control n=80 | test n=44\n",
      " Computed and saved CEU MAF references (control + test)\n",
      " Saved: data/processed/hapmap/ceu_maf_reference.npz\n",
      "   chr2 mean control MAF = 0.19467990225718848\n",
      "   chr10 mean control MAF = 0.1510133139814285\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Robust PROJECT_ROOT ----\n",
    "def find_project_root(start=None):\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for parent in [p] + list(p.parents):\n",
    "        if (parent / \".git\").exists() or (parent / \"requirements.txt\").exists():\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "PROC_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\"\n",
    "REGION_DIR = PROC_DIR / \"regions\"\n",
    "COHORT_DIR = PROC_DIR / \"cohorts\"\n",
    "COHORTS_JSON = COHORT_DIR / \"ceu_case_control_test_split.json\"\n",
    "\n",
    "REGION_CHR2 = REGION_DIR / \"CEU_chr2_5Mb.npz\"\n",
    "REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.npz\"\n",
    "\n",
    "for p in [REGION_CHR2, REGION_CHR10, COHORTS_JSON]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required file: {p}\")\n",
    "\n",
    "print(\" Loading region NPZ files + cohort split JSON from disk...\")\n",
    "\n",
    "# ---- Load region data (disk-backed; no in-memory dependencies) ----\n",
    "r2 = np.load(REGION_CHR2, allow_pickle=True)\n",
    "r10 = np.load(REGION_CHR10, allow_pickle=True)\n",
    "\n",
    "G2 = r2[\"G\"].astype(np.int8)\n",
    "rs2 = np.array(r2[\"snp_ids\"], dtype=object)\n",
    "pos2 = np.array(r2[\"positions\"], dtype=np.int32)\n",
    "ids2 = np.array([str(x) for x in r2[\"sample_ids\"]], dtype=object)\n",
    "\n",
    "G10 = r10[\"G\"].astype(np.int8)\n",
    "rs10 = np.array(r10[\"snp_ids\"], dtype=object)\n",
    "pos10 = np.array(r10[\"positions\"], dtype=np.int32)\n",
    "ids10 = np.array([str(x) for x in r10[\"sample_ids\"]], dtype=object)\n",
    "\n",
    "# ---- Load cohorts (case/control/test) ----\n",
    "cohorts = json.loads(COHORTS_JSON.read_text())\n",
    "control_ids = set(map(str, cohorts[\"control\"][\"sample_ids\"]))\n",
    "test_ids = set(map(str, cohorts[\"test\"][\"sample_ids\"]))\n",
    "\n",
    "def indices_from_ids(all_ids, wanted_ids_set):\n",
    "    mask = np.array([i in wanted_ids_set for i in all_ids], dtype=bool)\n",
    "    return np.where(mask)[0]\n",
    "\n",
    "control_idx2 = indices_from_ids(ids2, control_ids)\n",
    "test_idx2    = indices_from_ids(ids2, test_ids)\n",
    "\n",
    "control_idx10 = indices_from_ids(ids10, control_ids)\n",
    "test_idx10    = indices_from_ids(ids10, test_ids)\n",
    "\n",
    "print(f\"chr2 : control n={len(control_idx2)} | test n={len(test_idx2)}\")\n",
    "print(f\"chr10: control n={len(control_idx10)} | test n={len(test_idx10)}\")\n",
    "\n",
    "if len(control_idx2) == 0 or len(test_idx2) == 0:\n",
    "    raise RuntimeError(\"chr2 cohort mapping failed: no matched control/test IDs in chr2 region sample_ids.\")\n",
    "if len(control_idx10) == 0 or len(test_idx10) == 0:\n",
    "    raise RuntimeError(\"chr10 cohort mapping failed: no matched control/test IDs in chr10 region sample_ids.\")\n",
    "\n",
    "def maf_from_G(G_sub):\n",
    "    # minor allele frequency from minor-allele counts {0,1,2}, missing=-1\n",
    "    mask = (G_sub >= 0)\n",
    "    col_sum = (G_sub * mask).sum(axis=0).astype(np.float64)\n",
    "    col_n = mask.sum(axis=0).astype(np.float64)\n",
    "\n",
    "    maf = np.full(G_sub.shape[1], np.nan, dtype=np.float64)\n",
    "    ok = col_n > 0\n",
    "    maf[ok] = (col_sum[ok] / col_n[ok]) / 2.0\n",
    "    return maf\n",
    "\n",
    "maf_ctrl_chr2 = maf_from_G(G2[control_idx2])\n",
    "maf_test_chr2 = maf_from_G(G2[test_idx2])\n",
    "\n",
    "maf_ctrl_chr10 = maf_from_G(G10[control_idx10])\n",
    "maf_test_chr10 = maf_from_G(G10[test_idx10])\n",
    "\n",
    "freq_out = PROC_DIR / \"ceu_maf_reference.npz\"\n",
    "np.savez_compressed(\n",
    "    freq_out,\n",
    "    chr2_control_maf=maf_ctrl_chr2,\n",
    "    chr2_test_maf=maf_test_chr2,\n",
    "    chr2_snp_ids=rs2,\n",
    "    chr2_positions=pos2,\n",
    "    chr10_control_maf=maf_ctrl_chr10,\n",
    "    chr10_test_maf=maf_test_chr10,\n",
    "    chr10_snp_ids=rs10,\n",
    "    chr10_positions=pos10,\n",
    ")\n",
    "\n",
    "print(\" Computed and saved CEU MAF references (control + test)\")\n",
    "print(\" Saved:\", freq_out.relative_to(PROJECT_ROOT))\n",
    "print(\"   chr2 mean control MAF =\", float(np.nanmean(maf_ctrl_chr2)))\n",
    "print(\"   chr10 mean control MAF =\", float(np.nanmean(maf_ctrl_chr10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a3205ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Project root: /Users/erkmenerken/Desktop/proje430\n",
      "âœ… Raw phasing dir: /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED\n",
      "âœ… Regions: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/regions\n",
      "âœ… Cohorts: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/cohorts\n",
      "âœ… Output haplotypes: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/haplotypes\n",
      "âœ… Output blocks: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/blocks\n",
      "Exists - data/processed/hapmap/regions/CEU_chr2_5Mb.npz\n",
      "Exists - data/processed/hapmap/regions/CEU_chr10_1Mb.npz\n",
      "Exists - data/processed/hapmap/cohorts/ceu_case_control_test_split.json\n",
      "Exists - data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      "Exists - data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"\n",
    "    Find repo root robustly without relying on folder name.\n",
    "    We search upward from current working directory for common repo markers.\n",
    "    \"\"\"\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for p in [cwd] + list(cwd.parents):\n",
    "        if (p / \".git\").exists():\n",
    "            return p\n",
    "        if (p / \"requirements.txt\").exists():\n",
    "            return p\n",
    "        if (p / \"pyproject.toml\").exists():\n",
    "            return p\n",
    "        # fallback heuristic: repo root usually contains both src/ and data/ (or at least src/)\n",
    "        if (p / \"src\").exists() and (p / \"data\").exists():\n",
    "            return p\n",
    "        if (p / \"src\").exists() and (p / \"notebooks\").exists():\n",
    "            return p\n",
    "    # last resort: use the top-most parent that still contains \"src\"\n",
    "    for p in reversed(list(cwd.parents)):\n",
    "        if (p / \"src\").exists():\n",
    "            return p\n",
    "    return cwd\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "\n",
    "RAW_HAPMAP = PROJECT_ROOT / \"data\" / \"raw\" / \"hapmap\"\n",
    "PHASE_DIR  = RAW_HAPMAP / \"phasing\" / \"HapMap3_r2\" / \"CEU\" / \"UNRELATED\"\n",
    "\n",
    "PROC_DIR      = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\"\n",
    "REGION_DIR    = PROC_DIR / \"regions\"\n",
    "COHORT_DIR    = PROC_DIR / \"cohorts\"\n",
    "HAP_OUT_DIR   = PROC_DIR / \"haplotypes\"\n",
    "BLOCK_OUT_DIR = PROC_DIR / \"blocks\"\n",
    "\n",
    "for d in [RAW_HAPMAP, PHASE_DIR, PROC_DIR, REGION_DIR, COHORT_DIR, HAP_OUT_DIR, BLOCK_OUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REGION_CHR2  = REGION_DIR / \"CEU_chr2_5Mb.npz\"\n",
    "REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.npz\"\n",
    "COHORTS_JSON = COHORT_DIR / \"ceu_case_control_test_split.json\"\n",
    "\n",
    "PHASE_CHR2  = PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\"\n",
    "PHASE_CHR10 = PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\"\n",
    "\n",
    "print(\"âœ… Project root:\", PROJECT_ROOT)\n",
    "print(\"âœ… Raw phasing dir:\", PHASE_DIR)\n",
    "print(\"âœ… Regions:\", REGION_DIR)\n",
    "print(\"âœ… Cohorts:\", COHORT_DIR)\n",
    "print(\"âœ… Output haplotypes:\", HAP_OUT_DIR)\n",
    "print(\"âœ… Output blocks:\", BLOCK_OUT_DIR)\n",
    "\n",
    "for p in [REGION_CHR2, REGION_CHR10, COHORTS_JSON, PHASE_CHR2, PHASE_CHR10]:\n",
    "    status = \"Exists\" if p.exists() else \"Missing\"\n",
    "    try:\n",
    "        rel = p.relative_to(PROJECT_ROOT)\n",
    "        print(status, \"-\", rel)\n",
    "    except Exception:\n",
    "        print(status, \"-\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a3f2d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded regions:\n",
      "  chr2 : (174, 311) | SNPs: 311 | individuals: 174\n",
      "  chr10: (174, 610) | SNPs: 610 | individuals: 174\n",
      "\n",
      " Loaded CEU cohorts (mapped to region sample order):\n",
      "  chr2 : control n = 80 | test n = 44 | case n = 50\n",
      "  chr10: control n = 80 | test n = 44 | case n = 50\n",
      "\n",
      "â„¹ï¸ Note: `control_idx` and `test_idx` are set to chr2 indices for backward compatibility.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_region_npz(path: Path):\n",
    "    z = np.load(path, allow_pickle=True)\n",
    "    out = {k: z[k] for k in z.files}\n",
    "    # make sure types are friendly\n",
    "    out[\"snp_ids\"] = out[\"snp_ids\"].astype(object)\n",
    "    out[\"sample_ids\"] = out[\"sample_ids\"].astype(object)\n",
    "    out[\"minor_alleles\"] = out[\"minor_alleles\"].astype(object)\n",
    "    out[\"major_alleles\"] = out[\"major_alleles\"].astype(object)\n",
    "    return out\n",
    "\n",
    "# Load regions\n",
    "r2 = load_region_npz(REGION_CHR2)\n",
    "r10 = load_region_npz(REGION_CHR10)\n",
    "\n",
    "# Load cohorts JSON (NEW schema: sample_ids only)\n",
    "with open(COHORTS_JSON, \"r\") as f:\n",
    "    cohorts = json.load(f)\n",
    "\n",
    "print(\"âœ… Loaded regions:\")\n",
    "print(\"  chr2 :\", r2[\"G\"].shape, \"| SNPs:\", len(r2[\"snp_ids\"]), \"| individuals:\", len(r2[\"sample_ids\"]))\n",
    "print(\"  chr10:\", r10[\"G\"].shape, \"| SNPs:\", len(r10[\"snp_ids\"]), \"| individuals:\", len(r10[\"sample_ids\"]))\n",
    "\n",
    "# Convert region sample IDs to strings for matching\n",
    "ids2 = np.array([str(x) for x in r2[\"sample_ids\"]], dtype=object)\n",
    "ids10 = np.array([str(x) for x in r10[\"sample_ids\"]], dtype=object)\n",
    "\n",
    "control_ids = set(map(str, cohorts[\"control\"][\"sample_ids\"]))\n",
    "test_ids    = set(map(str, cohorts[\"test\"][\"sample_ids\"]))\n",
    "case_ids    = set(map(str, cohorts[\"case\"][\"sample_ids\"])) if \"case\" in cohorts else set()\n",
    "\n",
    "def idx_from_ids(all_ids, wanted_set):\n",
    "    mask = np.array([x in wanted_set for x in all_ids], dtype=bool)\n",
    "    return np.where(mask)[0]\n",
    "\n",
    "# IMPORTANT: indices are region-specific (donâ€™t assume chr2 == chr10 ordering)\n",
    "control_idx2 = idx_from_ids(ids2, control_ids)\n",
    "test_idx2    = idx_from_ids(ids2, test_ids)\n",
    "case_idx2    = idx_from_ids(ids2, case_ids) if case_ids else np.array([], dtype=int)\n",
    "\n",
    "control_idx10 = idx_from_ids(ids10, control_ids)\n",
    "test_idx10    = idx_from_ids(ids10, test_ids)\n",
    "case_idx10    = idx_from_ids(ids10, case_ids) if case_ids else np.array([], dtype=int)\n",
    "\n",
    "print(\"\\n Loaded CEU cohorts (mapped to region sample order):\")\n",
    "print(\"  chr2 : control n =\", len(control_idx2), \"| test n =\", len(test_idx2), \"| case n =\", len(case_idx2))\n",
    "print(\"  chr10: control n =\", len(control_idx10), \"| test n =\", len(test_idx10), \"| case n =\", len(case_idx10))\n",
    "\n",
    "# Backward compatibility if old code still expects control_idx / test_idx\n",
    "control_idx = control_idx2\n",
    "test_idx = test_idx2\n",
    "print(\"\\nâ„¹ï¸ Note: `control_idx` and `test_idx` are set to chr2 indices for backward compatibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d433896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded regions from disk:\n",
      "  chr2 : (174, 311) | SNPs: 311\n",
      "  chr10: (174, 610) | SNPs: 610\n",
      "\n",
      "ðŸ”§ Building blocks for chr2 (region NPZ)\n",
      " chr2 inferred blocks: 4\n",
      " Saved blocks â†’ data/processed/hapmap/blocks/CEU_chr2_5Mb.blocks.json\n",
      "   CEU_chr2_5Mb: 4 blocks\n",
      "\n",
      "\n",
      "ðŸ”§ Building blocks for chr10 (region NPZ)\n",
      " chr10 inferred blocks: 8\n",
      " Saved blocks â†’ data/processed/hapmap/blocks/CEU_chr10_1Mb.blocks.json\n",
      "   CEU_chr10_1Mb: 8 blocks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Requirements: REGION_CHR2 / REGION_CHR10 already defined and exist ----\n",
    "for p in [REGION_CHR2, REGION_CHR10]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing region NPZ: {p}\")\n",
    "\n",
    "def load_region_npz(path: Path):\n",
    "    z = np.load(path, allow_pickle=True)\n",
    "    out = {k: z[k] for k in z.files}\n",
    "    out[\"snp_ids\"] = out[\"snp_ids\"].astype(object)\n",
    "    out[\"positions\"] = out[\"positions\"].astype(np.int32)\n",
    "    out[\"sample_ids\"] = out[\"sample_ids\"].astype(object)\n",
    "    return out\n",
    "\n",
    "def adjacent_r2_from_G(G):\n",
    "    \"\"\"\n",
    "    Compute r^2 between adjacent SNPs using correlation on dosage {0,1,2},\n",
    "    ignoring missing=-1. Returns length (N-1) array.\n",
    "    \"\"\"\n",
    "    X = G.astype(float)\n",
    "    M, N = X.shape\n",
    "    out = np.zeros(N - 1, dtype=float)\n",
    "\n",
    "    for j in range(N - 1):\n",
    "        x = X[:, j]\n",
    "        y = X[:, j + 1]\n",
    "        mask = (x >= 0) & (y >= 0)\n",
    "        if mask.sum() < 10:\n",
    "            out[j] = 0.0\n",
    "            continue\n",
    "\n",
    "        xv = x[mask] - x[mask].mean()\n",
    "        yv = y[mask] - y[mask].mean()\n",
    "        denom = np.sqrt((xv * xv).sum() * (yv * yv).sum())\n",
    "        if denom == 0:\n",
    "            out[j] = 0.0\n",
    "            continue\n",
    "\n",
    "        r = float((xv * yv).sum() / denom)\n",
    "        out[j] = r * r\n",
    "    return out\n",
    "\n",
    "def build_blocks_from_adjacent_r2(r2, threshold=0.8, min_snps=5, max_snps=80):\n",
    "    \"\"\"\n",
    "    Create blocks by cutting when adjacent r^2 < threshold.\n",
    "    Then:\n",
    "      - merge blocks smaller than min_snps into previous\n",
    "      - split blocks larger than max_snps\n",
    "    Returns list of (start_idx, end_idx) inclusive, in SNP-index space.\n",
    "    \"\"\"\n",
    "    N = len(r2) + 1\n",
    "    cuts = [0]\n",
    "    for j, v in enumerate(r2):\n",
    "        if v < threshold:\n",
    "            cuts.append(j + 1)\n",
    "    cuts.append(N)\n",
    "\n",
    "    blocks = [(cuts[i], cuts[i + 1] - 1) for i in range(len(cuts) - 1)]\n",
    "\n",
    "    # merge tiny blocks into previous\n",
    "    merged = []\n",
    "    for s, e in blocks:\n",
    "        if not merged:\n",
    "            merged.append((s, e))\n",
    "        else:\n",
    "            if (e - s + 1) < min_snps:\n",
    "                ps, pe = merged[-1]\n",
    "                merged[-1] = (ps, e)\n",
    "            else:\n",
    "                merged.append((s, e))\n",
    "\n",
    "    # split huge blocks\n",
    "    final = []\n",
    "    for s, e in merged:\n",
    "        while (e - s + 1) > max_snps:\n",
    "            final.append((s, s + max_snps - 1))\n",
    "            s = s + max_snps\n",
    "        final.append((s, e))\n",
    "\n",
    "    return final\n",
    "\n",
    "def save_blocks_json(region_name, snp_ids, positions, blocks, params):\n",
    "    \"\"\"\n",
    "    Saves blocks JSON using the region NPZ SNP list (disk-backed, reproducible).\n",
    "    \"\"\"\n",
    "    out = BLOCK_OUT_DIR / f\"{region_name}.blocks.json\"\n",
    "\n",
    "    payload = {\n",
    "        \"region_name\": region_name,\n",
    "        \"phased_compatible\": False,\n",
    "        \"block_params\": params,\n",
    "        \"num_snps\": int(len(snp_ids)),\n",
    "        \"num_blocks\": int(len(blocks)),\n",
    "        \"blocks\": [\n",
    "            {\n",
    "                \"block_id\": int(i),\n",
    "                \"start_snp_index\": int(s),\n",
    "                \"end_snp_index\": int(e),\n",
    "                \"num_snps\": int(e - s + 1),\n",
    "                \"start_pos\": int(positions[s]),\n",
    "                \"end_pos\": int(positions[e]),\n",
    "                \"snp_ids\": [str(x) for x in snp_ids[s:e+1]],\n",
    "            }\n",
    "            for i, (s, e) in enumerate(blocks)\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    out.write_text(json.dumps(payload, indent=2))\n",
    "    print(f\" Saved blocks â†’ {out.relative_to(PROJECT_ROOT)}\")\n",
    "    print(f\"   {region_name}: {len(blocks)} blocks\\n\")\n",
    "    return out\n",
    "\n",
    "BLOCK_PARAMS = {\"threshold\": 0.8, \"min_snps\": 5, \"max_snps\": 80}\n",
    "\n",
    "# ---- Load region data ----\n",
    "r2 = load_region_npz(REGION_CHR2)\n",
    "r10 = load_region_npz(REGION_CHR10)\n",
    "\n",
    "print(\" Loaded regions from disk:\")\n",
    "print(\"  chr2 :\", r2[\"G\"].shape, \"| SNPs:\", len(r2[\"snp_ids\"]))\n",
    "print(\"  chr10:\", r10[\"G\"].shape, \"| SNPs:\", len(r10[\"snp_ids\"]))\n",
    "\n",
    "# ---- Build blocks (region-based, reproducible) ----\n",
    "print(\"\\nðŸ”§ Building blocks for chr2 (region NPZ)\")\n",
    "r2_adj = adjacent_r2_from_G(r2[\"G\"])\n",
    "blocks_chr2 = build_blocks_from_adjacent_r2(r2_adj, **BLOCK_PARAMS)\n",
    "print(f\" chr2 inferred blocks: {len(blocks_chr2)}\")\n",
    "blocks_chr2_path = save_blocks_json(\"CEU_chr2_5Mb\", r2[\"snp_ids\"], r2[\"positions\"], blocks_chr2, BLOCK_PARAMS)\n",
    "\n",
    "print(\"\\nðŸ”§ Building blocks for chr10 (region NPZ)\")\n",
    "r10_adj = adjacent_r2_from_G(r10[\"G\"])\n",
    "blocks_chr10 = build_blocks_from_adjacent_r2(r10_adj, **BLOCK_PARAMS)\n",
    "print(f\" chr10 inferred blocks: {len(blocks_chr10)}\")\n",
    "blocks_chr10_path = save_blocks_json(\"CEU_chr10_1Mb\", r10[\"snp_ids\"], r10[\"positions\"], blocks_chr10, BLOCK_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1466136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Project root: /Users/erkmenerken/Desktop/proje430\n",
      " RAW_HAPMAP: /Users/erkmenerken/Desktop/proje430/data/raw/hapmap\n",
      " PROC_DIR: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap\n",
      " REGION_DIR: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/regions\n",
      " COHORT_DIR: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/cohorts\n",
      " BLOCK_OUT_DIR: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/blocks\n",
      " HAP_OUT_DIR: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/haplotypes\n",
      "Exists - data/processed/hapmap/regions/CEU_chr2_5Mb.npz\n",
      "Exists - data/processed/hapmap/regions/CEU_chr10_1Mb.npz\n",
      "Exists - data/processed/hapmap/cohorts/ceu_case_control_test_split.json\n",
      "\n",
      " Using phased files:\n",
      "  chr2 : data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      "  chr10: data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# ---------- PROJECT ROOT ----------\n",
    "def find_project_root() -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for p in [cwd] + list(cwd.parents):\n",
    "        if (p / \".git\").exists() or (p / \"requirements.txt\").exists() or (p / \"pyproject.toml\").exists():\n",
    "            return p\n",
    "        if (p / \"src\").exists() and (p / \"data\").exists():\n",
    "            return p\n",
    "    return cwd\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "\n",
    "RAW_HAPMAP = PROJECT_ROOT / \"data\" / \"raw\" / \"hapmap\"\n",
    "PROC_DIR   = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\"\n",
    "REGION_DIR = PROC_DIR / \"regions\"\n",
    "COHORT_DIR = PROC_DIR / \"cohorts\"\n",
    "\n",
    "HAP_OUT_DIR   = PROC_DIR / \"haplotypes\"\n",
    "BLOCK_OUT_DIR = PROC_DIR / \"blocks\"\n",
    "\n",
    "for d in [RAW_HAPMAP, PROC_DIR, REGION_DIR, COHORT_DIR, HAP_OUT_DIR, BLOCK_OUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REGION_CHR2  = REGION_DIR / \"CEU_chr2_5Mb.npz\"\n",
    "REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.npz\"\n",
    "COHORTS_JSON = COHORT_DIR / \"ceu_case_control_test_split.json\"\n",
    "\n",
    "# ---------- PHASED FILE RESOLUTION ----------\n",
    "PHASING_ROOT = RAW_HAPMAP / \"phasing\"\n",
    "\n",
    "PHASE_NAME_CHR2  = \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\"\n",
    "PHASE_NAME_CHR10 = \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\"\n",
    "\n",
    "def resolve_under(root: Path, filename: str) -> Path:\n",
    "    # first try the old \"expected\" location (fast)\n",
    "    expected = PHASING_ROOT / \"HapMap3_r2\" / \"CEU\" / \"UNRELATED\" / filename\n",
    "    if expected.exists():\n",
    "        return expected\n",
    "\n",
    "    # then search anywhere under data/raw/hapmap (robust)\n",
    "    hits = sorted(root.rglob(filename))\n",
    "    if hits:\n",
    "        # choose the shortest path (usually the intended one)\n",
    "        hits = sorted(hits, key=lambda p: len(str(p)))\n",
    "        return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find phased file '{filename}' under {root}.\\n\"\n",
    "        f\"Run this in terminal to confirm where it is:\\n\"\n",
    "        f\"  find {root} -name '{filename}'\\n\"\n",
    "        f\"If it truly doesn't exist, you must download it into:\\n\"\n",
    "        f\"  {PHASING_ROOT}\\n\"\n",
    "    )\n",
    "\n",
    "PHASE_CHR2  = resolve_under(RAW_HAPMAP, PHASE_NAME_CHR2)\n",
    "PHASE_CHR10 = resolve_under(RAW_HAPMAP, PHASE_NAME_CHR10)\n",
    "\n",
    "print(\" Project root:\", PROJECT_ROOT)\n",
    "print(\" RAW_HAPMAP:\", RAW_HAPMAP)\n",
    "print(\" PROC_DIR:\", PROC_DIR)\n",
    "print(\" REGION_DIR:\", REGION_DIR)\n",
    "print(\" COHORT_DIR:\", COHORT_DIR)\n",
    "print(\" BLOCK_OUT_DIR:\", BLOCK_OUT_DIR)\n",
    "print(\" HAP_OUT_DIR:\", HAP_OUT_DIR)\n",
    "\n",
    "for p in [REGION_CHR2, REGION_CHR10, COHORTS_JSON]:\n",
    "    print((\"Exists\" if p.exists() else \"Missing\"), \"-\", p.relative_to(PROJECT_ROOT))\n",
    "\n",
    "print(\"\\n Using phased files:\")\n",
    "print(\"  chr2 :\", PHASE_CHR2.relative_to(PROJECT_ROOT))\n",
    "print(\"  chr10:\", PHASE_CHR10.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e40fd4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pc2 already exists\n",
      " pc10 already exists\n",
      "\n",
      " Building phased-compatible blocks for chr2\n",
      " chr2 phased-compatible SNPs: 118 | blocks: 3\n",
      "\n",
      " Building phased-compatible blocks for chr10\n",
      " chr10 phased-compatible SNPs: 129 | blocks: 2\n",
      "\n",
      " Building haplotype histograms for CEU_chr2_5Mb (phased-compatible)\n",
      "  phased-compatible individuals: 17\n",
      "  control individuals used:      8\n",
      "   block 0: SNPs 0-79 | unique=16 | top1=1\n",
      "\n",
      " Saved haplotype histograms â†’ data/processed/hapmap/haplotypes/CEU_chr2_5Mb.control_haplotypes.phased_compatible.json\n",
      "\n",
      " Building haplotype histograms for CEU_chr10_1Mb (phased-compatible)\n",
      "  phased-compatible individuals: 17\n",
      "  control individuals used:      8\n",
      "   block 0: SNPs 0-79 | unique=9 | top1=6\n",
      "\n",
      " Saved haplotype histograms â†’ data/processed/hapmap/haplotypes/CEU_chr10_1Mb.control_haplotypes.phased_compatible.json\n",
      "\n",
      " Done building phased-compatible control haplotype histograms.\n",
      " chr2 â†’ data/processed/hapmap/haplotypes/CEU_chr2_5Mb.control_haplotypes.phased_compatible.json\n",
      " chr10 â†’ data/processed/hapmap/haplotypes/CEU_chr10_1Mb.control_haplotypes.phased_compatible.json\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Helpers\n",
    "# -----------------------------\n",
    "def load_phased_df(phase_gz_path):\n",
    "    print(f\"\\nðŸ“¥ Loading phased file: {phase_gz_path.name}\")\n",
    "    df = pd.read_csv(\n",
    "        phase_gz_path,\n",
    "        sep=r\"\\s+\",\n",
    "        engine=\"python\",\n",
    "        compression=\"gzip\",\n",
    "        dtype=str,\n",
    "    )\n",
    "    print(f\"   Phased rows loaded: {len(df)} | columns: {len(df.columns)}\")\n",
    "    if \"rsID\" not in df.columns:\n",
    "        raise ValueError(\"Unexpected phased file columns. Expected 'rsID'.\")\n",
    "    # position column name differs across HapMap releases; try both\n",
    "    if \"position_b36\" not in df.columns and \"position\" not in df.columns:\n",
    "        print(\" No 'position_b36' or 'position' column found. We'll proceed by rsID alignment only.\")\n",
    "    return df\n",
    "\n",
    "def phased_individuals_from_columns(df):\n",
    "    hap_cols = [c for c in df.columns if c.endswith(\"_A\") or c.endswith(\"_B\")]\n",
    "    if len(hap_cols) == 0:\n",
    "        raise ValueError(\"No _A/_B haplotype columns found in phased file.\")\n",
    "    individuals = sorted(set(c[:-2] for c in hap_cols))\n",
    "    individuals = [i for i in individuals if f\"{i}_A\" in df.columns and f\"{i}_B\" in df.columns]\n",
    "    return individuals\n",
    "\n",
    "def make_phased_compatible_region(region, phased_df, region_name):\n",
    "    region_sample_ids = np.array([str(x) for x in region[\"sample_ids\"]], dtype=object)\n",
    "    region_snp_ids    = np.array([str(x) for x in region[\"snp_ids\"]], dtype=object)\n",
    "    region_positions  = np.array(region[\"positions\"], dtype=int)\n",
    "\n",
    "    phased_indivs_all = phased_individuals_from_columns(phased_df)\n",
    "    region_indiv_set = set(region_sample_ids.tolist())\n",
    "    phased_indivs = [i for i in phased_indivs_all if i in region_indiv_set]\n",
    "\n",
    "    print(f\"\\n[{region_name}] Individuals:\")\n",
    "    print(f\"  phased total: {len(phased_indivs_all)}\")\n",
    "    print(f\"  phased âˆ© region: {len(phased_indivs)}\")\n",
    "\n",
    "    if len(phased_indivs) == 0:\n",
    "        raise RuntimeError(f\"[{region_name}] No overlapping individuals between region and phased file.\")\n",
    "\n",
    "    phased_rsids_set = set(phased_df[\"rsID\"].astype(str).tolist())\n",
    "    keep_snp_mask = np.array([rs in phased_rsids_set for rs in region_snp_ids], dtype=bool)\n",
    "\n",
    "    snp_ids_sub = region_snp_ids[keep_snp_mask]\n",
    "    positions_sub = region_positions[keep_snp_mask]\n",
    "\n",
    "    print(f\"\\n[{region_name}] SNPs:\")\n",
    "    print(f\"  region SNPs: {len(region_snp_ids)}\")\n",
    "    print(f\"  matched in phased by rsID: {len(snp_ids_sub)}\")\n",
    "\n",
    "    if len(snp_ids_sub) == 0:\n",
    "        raise RuntimeError(f\"[{region_name}] No SNPs from region found in phased file by rsID.\")\n",
    "\n",
    "    indiv_idx_in_region = np.array([np.where(region_sample_ids == i)[0][0] for i in phased_indivs], dtype=int)\n",
    "    snp_idx_in_region = np.where(keep_snp_mask)[0]\n",
    "\n",
    "    G_sub = region[\"G\"][indiv_idx_in_region][:, snp_idx_in_region]\n",
    "    sample_ids_sub = region_sample_ids[indiv_idx_in_region]\n",
    "\n",
    "    # subset phased rows and reorder to match region order\n",
    "    phased_sub = phased_df[phased_df[\"rsID\"].isin(set(snp_ids_sub.tolist()))].copy()\n",
    "    order_map = {rsid: i for i, rsid in enumerate(snp_ids_sub.tolist())}\n",
    "    phased_sub[\"__order\"] = phased_sub[\"rsID\"].map(order_map)\n",
    "    phased_sub = phased_sub.sort_values(\"__order\").drop(columns=\"__order\")\n",
    "\n",
    "    A_cols = [f\"{i}_A\" for i in phased_indivs]\n",
    "    B_cols = [f\"{i}_B\" for i in phased_indivs]\n",
    "    alleles_A = phased_sub[A_cols].to_numpy(dtype=object)  # (n_snps, n_indiv)\n",
    "    alleles_B = phased_sub[B_cols].to_numpy(dtype=object)\n",
    "\n",
    "    print(f\"[{region_name}] Phased allele matrices: A={alleles_A.shape}, B={alleles_B.shape}\")\n",
    "    return {\n",
    "        \"region_name\": region_name,\n",
    "        \"G_sub\": G_sub,\n",
    "        \"sample_ids_sub\": sample_ids_sub,\n",
    "        \"snp_ids_sub\": snp_ids_sub,\n",
    "        \"positions_sub\": positions_sub,\n",
    "        \"alleles_A\": alleles_A,\n",
    "        \"alleles_B\": alleles_B,\n",
    "        \"phased_individuals\": phased_indivs,\n",
    "        \"indiv_idx_in_region\": indiv_idx_in_region,\n",
    "        \"snp_idx_in_region\": snp_idx_in_region,\n",
    "    }\n",
    "\n",
    "def adjacent_r2_from_G(G):\n",
    "    X = G.astype(float)\n",
    "    M, N = X.shape\n",
    "    out = np.zeros(N - 1, dtype=float)\n",
    "    for j in range(N - 1):\n",
    "        x = X[:, j]\n",
    "        y = X[:, j + 1]\n",
    "        mask = (x >= 0) & (y >= 0)\n",
    "        if mask.sum() < 10:\n",
    "            out[j] = 0.0\n",
    "            continue\n",
    "        xv = x[mask] - x[mask].mean()\n",
    "        yv = y[mask] - y[mask].mean()\n",
    "        denom = np.sqrt((xv * xv).sum() * (yv * yv).sum())\n",
    "        out[j] = 0.0 if denom == 0 else float(((xv * yv).sum() / denom) ** 2)\n",
    "    return out\n",
    "\n",
    "def build_blocks_from_adjacent_r2(r2, threshold=0.8, min_snps=5, max_snps=80):\n",
    "    N = len(r2) + 1\n",
    "    cuts = [0]\n",
    "    for j, v in enumerate(r2):\n",
    "        if v < threshold:\n",
    "            cuts.append(j + 1)\n",
    "    cuts.append(N)\n",
    "\n",
    "    blocks = [(cuts[i], cuts[i + 1] - 1) for i in range(len(cuts) - 1)]\n",
    "\n",
    "    merged = []\n",
    "    for s, e in blocks:\n",
    "        if not merged:\n",
    "            merged.append((s, e))\n",
    "        else:\n",
    "            if (e - s + 1) < min_snps:\n",
    "                ps, pe = merged[-1]\n",
    "                merged[-1] = (ps, e)\n",
    "            else:\n",
    "                merged.append((s, e))\n",
    "\n",
    "    final = []\n",
    "    for s, e in merged:\n",
    "        while (e - s + 1) > max_snps:\n",
    "            final.append((s, s + max_snps - 1))\n",
    "            s = s + max_snps\n",
    "        final.append((s, e))\n",
    "    return final\n",
    "\n",
    "def hap_strings_for_block(allele_matrix, start, end):\n",
    "    block = allele_matrix[start:end+1, :]\n",
    "    return [\"\".join(block[:, j].tolist()) for j in range(block.shape[1])]\n",
    "\n",
    "def build_haplotype_histograms_from_pc(region_name, blocks, pc, cohorts, top_k=50):\n",
    "    print(f\"\\n Building haplotype histograms for {region_name} (phased-compatible)\")\n",
    "\n",
    "    phased_region_ids = [str(x) for x in pc[\"sample_ids_sub\"]]\n",
    "    control_ids_full = set(str(x) for x in cohorts[\"control\"][\"sample_ids\"])\n",
    "    control_cols = [j for j, sid in enumerate(phased_region_ids) if sid in control_ids_full]\n",
    "\n",
    "    print(f\"  phased-compatible individuals: {len(phased_region_ids)}\")\n",
    "    print(f\"  control individuals used:      {len(control_cols)}\")\n",
    "\n",
    "    if len(control_cols) == 0:\n",
    "        raise RuntimeError(\n",
    "            f\"No CONTROL individuals overlap phased subset for {region_name}. \"\n",
    "            \"Fix by redoing the cohort split to include UNRELATED phased people, or use only phased people for all cohorts.\"\n",
    "        )\n",
    "\n",
    "    alleles_A = pc[\"alleles_A\"]\n",
    "    alleles_B = pc[\"alleles_B\"]\n",
    "\n",
    "    block_payload = []\n",
    "    for block_id, (s, e) in enumerate(blocks):\n",
    "        hA_all = hap_strings_for_block(alleles_A, s, e)\n",
    "        hB_all = hap_strings_for_block(alleles_B, s, e)\n",
    "\n",
    "        ctr = Counter()\n",
    "        for col in control_cols:\n",
    "            ctr[hA_all[col]] += 1\n",
    "            ctr[hB_all[col]] += 1\n",
    "\n",
    "        total = int(sum(ctr.values()))\n",
    "        top = ctr.most_common(top_k)\n",
    "        top_haps = [h for h, _ in top]\n",
    "        top_counts = [int(c) for _, c in top]\n",
    "        other_count = int(total - sum(top_counts))\n",
    "\n",
    "        block_payload.append({\n",
    "            \"block_id\": int(block_id),\n",
    "            \"start_snp_index\": int(s),\n",
    "            \"end_snp_index\": int(e),\n",
    "            \"num_snps\": int(e - s + 1),\n",
    "            \"total_haplotypes_counted\": total,\n",
    "            \"top_k\": int(top_k),\n",
    "            \"top_haplotypes\": top_haps,\n",
    "            \"top_counts\": top_counts,\n",
    "            \"other_count\": other_count,\n",
    "        })\n",
    "\n",
    "        if block_id % 10 == 0:\n",
    "            top1 = top_counts[0] if top_counts else 0\n",
    "            print(f\"   block {block_id}: SNPs {s}-{e} | unique={len(ctr)} | top1={top1}\")\n",
    "\n",
    "    out = HAP_OUT_DIR / f\"{region_name}.control_haplotypes.phased_compatible.json\"\n",
    "    out.write_text(json.dumps({\n",
    "        \"region_name\": region_name,\n",
    "        \"note\": (\n",
    "            \"Built on intersection of (region SNPs âˆ© phased SNPs) and \"\n",
    "            \"(region individuals âˆ© phased UNRELATED CEU individuals).\"\n",
    "        ),\n",
    "        \"counts_from\": \"CONTROL cohort only, restricted to phased-compatible individuals\",\n",
    "        \"phased_compatible\": {\n",
    "            \"num_individuals_total\": int(len(phased_region_ids)),\n",
    "            \"num_control_individuals_used\": int(len(control_cols)),\n",
    "            \"num_snps_total\": int(len(pc[\"snp_ids_sub\"])),\n",
    "        },\n",
    "        \"blocks\": block_payload\n",
    "    }, indent=2))\n",
    "\n",
    "    print(f\"\\n Saved haplotype histograms â†’ {out.relative_to(PROJECT_ROOT)}\")\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Ensure pc2/pc10 exist (recompute if missing)\n",
    "# -----------------------------\n",
    "for name in [\"r2\", \"r10\", \"cohorts\"]:\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"Missing `{name}`. Run the region+cohorts loading cell first.\")\n",
    "\n",
    "if \"pc2\" not in globals():\n",
    "    df_phase_chr2 = load_phased_df(PHASE_CHR2)\n",
    "    pc2 = make_phased_compatible_region(r2, df_phase_chr2, \"chr2\")\n",
    "    print(\" Created pc2\")\n",
    "else:\n",
    "    print(\" pc2 already exists\")\n",
    "\n",
    "if \"pc10\" not in globals():\n",
    "    df_phase_chr10 = load_phased_df(PHASE_CHR10)\n",
    "    pc10 = make_phased_compatible_region(r10, df_phase_chr10, \"chr10\")\n",
    "    print(\" Created pc10\")\n",
    "else:\n",
    "    print(\" pc10 already exists\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Build BLOCKS ON phased-compatible SNP subset\n",
    "# -----------------------------\n",
    "BLOCK_PARAMS = {\"threshold\": 0.8, \"min_snps\": 5, \"max_snps\": 80}\n",
    "\n",
    "print(\"\\n Building phased-compatible blocks for chr2\")\n",
    "pc2_adj = adjacent_r2_from_G(pc2[\"G_sub\"])\n",
    "blocks_chr2 = build_blocks_from_adjacent_r2(pc2_adj, **BLOCK_PARAMS)\n",
    "print(f\" chr2 phased-compatible SNPs: {pc2['G_sub'].shape[1]} | blocks: {len(blocks_chr2)}\")\n",
    "\n",
    "print(\"\\n Building phased-compatible blocks for chr10\")\n",
    "pc10_adj = adjacent_r2_from_G(pc10[\"G_sub\"])\n",
    "blocks_chr10 = build_blocks_from_adjacent_r2(pc10_adj, **BLOCK_PARAMS)\n",
    "print(f\" chr10 phased-compatible SNPs: {pc10['G_sub'].shape[1]} | blocks: {len(blocks_chr10)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build control haplotype histograms\n",
    "# -----------------------------\n",
    "hap_chr2_out = build_haplotype_histograms_from_pc(\"CEU_chr2_5Mb\", blocks_chr2, pc2, cohorts, top_k=50)\n",
    "hap_chr10_out = build_haplotype_histograms_from_pc(\"CEU_chr10_1Mb\", blocks_chr10, pc10, cohorts, top_k=50)\n",
    "\n",
    "print(\"\\n Done building phased-compatible control haplotype histograms.\")\n",
    "print(\" chr2 â†’\", hap_chr2_out.relative_to(PROJECT_ROOT))\n",
    "print(\" chr10 â†’\", hap_chr10_out.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "296185c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SUMMARY OF WHAT WE NOW HAVE\n",
      "\n",
      "Raw downloads (existing):\n",
      " - data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      " - data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n",
      "\n",
      "Processed regions (existing):\n",
      " - data/processed/hapmap/regions/CEU_chr2_5Mb.npz\n",
      " - data/processed/hapmap/regions/CEU_chr10_1Mb.npz\n",
      "\n",
      "New outputs created now:\n",
      " - Blocks chr2: data/processed/hapmap/blocks/CEU_chr2_5Mb.blocks.json\n",
      " - Blocks chr10: data/processed/hapmap/blocks/CEU_chr10_1Mb.blocks.json\n",
      " - Hap hist chr2: data/processed/hapmap/haplotypes/CEU_chr2_5Mb.control_haplotypes.phased_compatible.json\n",
      " - Hap hist chr10: data/processed/hapmap/haplotypes/CEU_chr10_1Mb.control_haplotypes.phased_compatible.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SUMMARY OF WHAT WE NOW HAVE\")\n",
    "\n",
    "print(\"\\nRaw downloads (existing):\")\n",
    "print(\" -\", (PHASE_CHR2).relative_to(PROJECT_ROOT))\n",
    "print(\" -\", (PHASE_CHR10).relative_to(PROJECT_ROOT))\n",
    "\n",
    "print(\"\\nProcessed regions (existing):\")\n",
    "print(\" -\", (REGION_CHR2).relative_to(PROJECT_ROOT))\n",
    "print(\" -\", (REGION_CHR10).relative_to(PROJECT_ROOT))\n",
    "\n",
    "print(\"\\nNew outputs created now:\")\n",
    "print(\" - Blocks chr2:\", blocks_chr2_path.relative_to(PROJECT_ROOT))\n",
    "print(\" - Blocks chr10:\", blocks_chr10_path.relative_to(PROJECT_ROOT))\n",
    "print(\" - Hap hist chr2:\", hap_chr2_out.relative_to(PROJECT_ROOT))\n",
    "print(\" - Hap hist chr10:\", hap_chr10_out.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f3b43f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2: phased-compatible SNPs=118, individuals=17\n",
      "   sample_ids_sub=17, snp_ids_sub=118\n",
      "chr10: phased-compatible SNPs=129, individuals=17\n",
      "   sample_ids_sub=17, snp_ids_sub=129\n",
      "\n",
      " Haplotypes output dir: data/processed/hapmap/haplotypes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "HAP_OUT_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\" / \"haplotypes\"\n",
    "HAP_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "required_vars = [\"pc2\", \"pc10\", \"blocks_chr2\", \"blocks_chr10\", \"cohorts\", \"PROJECT_ROOT\"]\n",
    "missing = [v for v in required_vars if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing variables from earlier cells: {missing}\")\n",
    "\n",
    "def _check_pc(pc, name):\n",
    "    A = pc[\"alleles_A\"]; B = pc[\"alleles_B\"]\n",
    "    assert A.shape == B.shape, f\"{name}: A/B shape mismatch\"\n",
    "    n_snps, n_ind = A.shape\n",
    "    print(f\"{name}: phased-compatible SNPs={n_snps}, individuals={n_ind}\")\n",
    "    print(f\"   sample_ids_sub={len(pc['sample_ids_sub'])}, snp_ids_sub={len(pc['snp_ids_sub'])}\")\n",
    "    # Blocks must fit SNP count\n",
    "    max_end = max(e for s, e in (blocks_chr2 if name=='chr2' else blocks_chr10))\n",
    "    if max_end >= n_snps:\n",
    "        raise RuntimeError(f\"{name}: blocks don't fit SNP count (max_end={max_end}, n_snps={n_snps}). Rebuild Cell 3 on pc['G_sub'].\")\n",
    "\n",
    "_check_pc(pc2, \"chr2\")\n",
    "_check_pc(pc10, \"chr10\")\n",
    "\n",
    "print(\"\\n Haplotypes output dir:\", HAP_OUT_DIR.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "135a2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "def hap_strings_for_block(allele_matrix, start, end):\n",
    "    \"\"\"\n",
    "    allele_matrix: (N_snps, N_individuals) of single-letter alleles (strings)\n",
    "    returns list length N_individuals where each element is the haplotype string over SNPs [start:end]\n",
    "    \"\"\"\n",
    "    block = allele_matrix[start:end+1, :]  \n",
    "    return [\"\".join(block[:, j].tolist()) for j in range(block.shape[1])]\n",
    "\n",
    "def compute_control_cols_in_pc(pc, cohorts):\n",
    "    \"\"\"\n",
    "    pc has sample_ids_sub = individuals in phased-compatible subset (same order as allele columns).\n",
    "    cohorts['control']['sample_ids'] are the control IDs from full CEU (unphased split).\n",
    "    Return list of column indices in pc corresponding to CONTROL individuals.\n",
    "    \"\"\"\n",
    "    pc_ids = [str(x) for x in pc[\"sample_ids_sub\"]]\n",
    "    control_set = set(str(x) for x in cohorts[\"control\"][\"sample_ids\"])\n",
    "    control_cols = [j for j, sid in enumerate(pc_ids) if sid in control_set]\n",
    "    return control_cols\n",
    "\n",
    "def save_control_haplotype_histograms(region_name, pc, blocks, cohorts, top_k=50):\n",
    "    \"\"\"\n",
    "    Writes:\n",
    "      data/processed/hapmap/haplotypes/{region_name}.control_haplotypes.phased_compatible.json\n",
    "\n",
    "    Counts are over haplotypes => 2 per person (A and B haplotypes).\n",
    "    Uses CONTROL only (public reference), restricted to phased-compatible individuals.\n",
    "    \"\"\"\n",
    "    A = pc[\"alleles_A\"]\n",
    "    B = pc[\"alleles_B\"]\n",
    "    snp_ids = [str(x) for x in pc[\"snp_ids_sub\"]]\n",
    "    positions = [int(x) for x in pc[\"positions_sub\"]]\n",
    "    pc_ids = [str(x) for x in pc[\"sample_ids_sub\"]]\n",
    "\n",
    "    control_cols = compute_control_cols_in_pc(pc, cohorts)\n",
    "    if len(control_cols) == 0:\n",
    "        raise RuntimeError(\n",
    "            f\"{region_name}: No CONTROL individuals overlap with phased-compatible subset. \"\n",
    "            \"This can happen depending on your split + UNRELATED set.\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\n [{region_name}] CONTROL haplotype histograms (phased-compatible)\")\n",
    "    print(f\" phased-compatible individuals total: {len(pc_ids)}\")\n",
    "    print(f\" control individuals used: {len(control_cols)} (=> {2*len(control_cols)} haplotypes)\")\n",
    "    print(f\" SNPs used: {A.shape[0]}\")\n",
    "\n",
    "    block_payload = []\n",
    "    for block_id, (s, e) in enumerate(blocks):\n",
    "\n",
    "        hA_all = hap_strings_for_block(A, s, e)\n",
    "        hB_all = hap_strings_for_block(B, s, e)\n",
    "\n",
    "        ctr = Counter()\n",
    "        for col in control_cols:\n",
    "            ctr[hA_all[col]] += 1\n",
    "            ctr[hB_all[col]] += 1\n",
    "\n",
    "        total = int(sum(ctr.values()))\n",
    "        top = ctr.most_common(top_k)\n",
    "        top_haps = [h for h, _ in top]\n",
    "        top_counts = [int(c) for _, c in top]\n",
    "        other_count = int(total - sum(top_counts))\n",
    "\n",
    "        block_payload.append({\n",
    "            \"block_id\": int(block_id),\n",
    "            \"start_snp_index\": int(s),\n",
    "            \"end_snp_index\": int(e),\n",
    "            \"num_snps\": int(e - s + 1),\n",
    "            \"start_pos\": int(positions[s]),\n",
    "            \"end_pos\": int(positions[e]),\n",
    "            \"total_haplotypes_counted\": total,  \n",
    "            \"top_k\": int(top_k),\n",
    "            \"top_haplotypes\": top_haps,\n",
    "            \"top_counts\": top_counts,\n",
    "            \"other_count\": other_count,\n",
    "        })\n",
    "\n",
    "        if block_id % 10 == 0:\n",
    "            print(f\"   block {block_id}: SNPs {s}-{e} | unique={len(ctr)} | top1={top_counts[0] if top_counts else 0}\")\n",
    "\n",
    "    out_path = HAP_OUT_DIR / f\"{region_name}.control_haplotypes.phased_compatible.json\"\n",
    "    payload = {\n",
    "        \"region_name\": region_name,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"source\": \"HapMap3 r2 phased (CEU UNRELATED) + CEU control split from unphased genotypes\",\n",
    "        \"counts_from\": \"CONTROL cohort only (public reference), restricted to phased-compatible subset\",\n",
    "        \"phased_compatible\": {\n",
    "            \"num_snps\": int(A.shape[0]),\n",
    "            \"num_individuals_total\": int(A.shape[1]),\n",
    "            \"num_control_individuals_used\": int(len(control_cols)),\n",
    "        },\n",
    "        \"note\": \"Counts are over haplotypes (2 per person: A and B).\",\n",
    "        \"blocks\": block_payload,\n",
    "    }\n",
    "    out_path.write_text(json.dumps(payload, indent=2))\n",
    "    print(f\"\\n Saved â†’ {out_path.relative_to(PROJECT_ROOT)}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f802b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [CEU_chr2_5Mb] CONTROL haplotype histograms (phased-compatible)\n",
      " phased-compatible individuals total: 17\n",
      " control individuals used: 8 (=> 16 haplotypes)\n",
      " SNPs used: 118\n",
      "   block 0: SNPs 0-79 | unique=16 | top1=1\n",
      "\n",
      " Saved â†’ data/processed/hapmap/haplotypes/CEU_chr2_5Mb.control_haplotypes.phased_compatible.json\n",
      "\n",
      " [CEU_chr10_1Mb] CONTROL haplotype histograms (phased-compatible)\n",
      " phased-compatible individuals total: 17\n",
      " control individuals used: 8 (=> 16 haplotypes)\n",
      " SNPs used: 129\n",
      "   block 0: SNPs 0-79 | unique=9 | top1=6\n",
      "\n",
      " Saved â†’ data/processed/hapmap/haplotypes/CEU_chr10_1Mb.control_haplotypes.phased_compatible.json\n",
      "\n",
      " Preprocessing milestone complete for Method 2.\n",
      "chr2: data/processed/hapmap/haplotypes/CEU_chr2_5Mb.control_haplotypes.phased_compatible.json\n",
      "chr10: data/processed/hapmap/haplotypes/CEU_chr10_1Mb.control_haplotypes.phased_compatible.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hap_chr2_out = save_control_haplotype_histograms(\n",
    "    region_name=\"CEU_chr2_5Mb\",\n",
    "    pc=pc2,\n",
    "    blocks=blocks_chr2,\n",
    "    cohorts=cohorts,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "hap_chr10_out = save_control_haplotype_histograms(\n",
    "    region_name=\"CEU_chr10_1Mb\",\n",
    "    pc=pc10,\n",
    "    blocks=blocks_chr10,\n",
    "    cohorts=cohorts,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"\\n Preprocessing milestone complete for Method 2.\")\n",
    "print(\"chr2:\", hap_chr2_out.relative_to(PROJECT_ROOT))\n",
    "print(\"chr10:\", hap_chr10_out.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c52aa4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Rebuilding CONTROL haplotype histograms using the NEW cohort split...\n",
      "\n",
      " [CEU_chr2_5Mb] CONTROL haplotype histograms (phased-compatible)\n",
      " phased-compatible individuals total: 17\n",
      " control individuals used: 8 (=> 16 haplotypes)\n",
      " SNPs used: 118\n",
      "   block 0: SNPs 0-79 | unique=16 | top1=1\n",
      "\n",
      " Saved â†’ data/processed/hapmap/haplotypes/CEU_chr2_5Mb.control_haplotypes.phased_compatible.json\n",
      "\n",
      " [CEU_chr10_1Mb] CONTROL haplotype histograms (phased-compatible)\n",
      " phased-compatible individuals total: 17\n",
      " control individuals used: 8 (=> 16 haplotypes)\n",
      " SNPs used: 129\n",
      "   block 0: SNPs 0-79 | unique=9 | top1=6\n",
      "\n",
      " Saved â†’ data/processed/hapmap/haplotypes/CEU_chr10_1Mb.control_haplotypes.phased_compatible.json\n",
      "\n",
      " Updated control haplotype histograms:\n",
      "chr2 â†’ data/processed/hapmap/haplotypes/CEU_chr2_5Mb.control_haplotypes.phased_compatible.json\n",
      "chr10 â†’ data/processed/hapmap/haplotypes/CEU_chr10_1Mb.control_haplotypes.phased_compatible.json\n"
     ]
    }
   ],
   "source": [
    "# REQUIREMENTS:\n",
    "# save_control_haplotype_histograms must exist (you created it earlier)\n",
    "# blocks_chr2, blocks_chr10 must exist\n",
    "# pc2, pc10 must exist\n",
    "\n",
    "needed = [\"save_control_haplotype_histograms\", \"blocks_chr2\", \"blocks_chr10\", \"pc2\", \"pc10\"]\n",
    "missing = [x for x in needed if x not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required variables/functions: {missing}\")\n",
    "\n",
    "print(\"\\n Rebuilding CONTROL haplotype histograms using the NEW cohort split...\")\n",
    "\n",
    "out_chr2 = save_control_haplotype_histograms(\n",
    "    region_name=\"CEU_chr2_5Mb\",\n",
    "    pc=pc2,\n",
    "    blocks=blocks_chr2,\n",
    "    cohorts=cohorts,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "out_chr10 = save_control_haplotype_histograms(\n",
    "    region_name=\"CEU_chr10_1Mb\",\n",
    "    pc=pc10,\n",
    "    blocks=blocks_chr10,\n",
    "    cohorts=cohorts,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"\\n Updated control haplotype histograms:\")\n",
    "print(\"chr2 â†’\", out_chr2.relative_to(PROJECT_ROOT))\n",
    "print(\"chr10 â†’\", out_chr10.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "061b5190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FILE EXISTENCE CHECK ===\n",
      "âœ… data/raw/hapmap/genotypes/genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\n",
      "âœ… data/raw/hapmap/genotypes/genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\n",
      "âœ… data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      "âœ… data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n",
      "âœ… data/processed/hapmap/regions/CEU_chr2_5Mb.npz\n",
      "âœ… data/processed/hapmap/regions/CEU_chr10_1Mb.npz\n",
      "âœ… data/processed/hapmap/cohorts/ceu_case_control_test_split.json\n",
      "âœ… data/processed/hapmap/blocks/CEU_chr2_5Mb.blocks.json\n",
      "âœ… data/processed/hapmap/blocks/CEU_chr10_1Mb.blocks.json\n",
      "âœ… data/processed/hapmap/haplotypes/CEU_chr2_5Mb.control_haplotypes.phased_compatible.json\n",
      "âœ… data/processed/hapmap/haplotypes/CEU_chr10_1Mb.control_haplotypes.phased_compatible.json\n",
      "\n",
      "=== COHORT SPLIT CHECK ===\n",
      "control/case/test = 80 50 44\n",
      "has indices_in_ceu_matrix? True True True\n",
      "\n",
      "FINAL: âœ… preprocessing outputs are complete\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_HAPMAP_DIR = DATA_DIR / \"raw\" / \"hapmap\"\n",
    "PROC_DIR = DATA_DIR / \"processed\" / \"hapmap\"\n",
    "\n",
    "GENO_DIR = RAW_HAPMAP_DIR / \"genotypes\"\n",
    "PHASE_DIR = RAW_HAPMAP_DIR / \"phasing\" / \"HapMap3_r2\"\n",
    "\n",
    "REGION_DIR = PROC_DIR / \"regions\"\n",
    "COHORT_DIR = PROC_DIR / \"cohorts\"\n",
    "BLOCK_DIR  = PROC_DIR / \"blocks\"\n",
    "HAP_DIR    = PROC_DIR / \"haplotypes\"\n",
    "\n",
    "expected = [\n",
    "    # raw downloads\n",
    "    GENO_DIR / \"genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\",\n",
    "    GENO_DIR / \"genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\",\n",
    "    PHASE_DIR / \"CEU\" / \"UNRELATED\" / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\",\n",
    "    PHASE_DIR / \"CEU\" / \"UNRELATED\" / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\",\n",
    "\n",
    "    # processed regions\n",
    "    REGION_DIR / \"CEU_chr2_5Mb.npz\",\n",
    "    REGION_DIR / \"CEU_chr10_1Mb.npz\",\n",
    "\n",
    "    # cohort split\n",
    "    COHORT_DIR / \"ceu_case_control_test_split.json\",\n",
    "\n",
    "    # blocks + haplotype outputs (names may vary slightly; adjust if yours differ)\n",
    "    BLOCK_DIR / \"CEU_chr2_5Mb.blocks.json\",\n",
    "    BLOCK_DIR / \"CEU_chr10_1Mb.blocks.json\",\n",
    "    HAP_DIR / \"CEU_chr2_5Mb.control_haplotypes.phased_compatible.json\",\n",
    "    HAP_DIR / \"CEU_chr10_1Mb.control_haplotypes.phased_compatible.json\",\n",
    "]\n",
    "\n",
    "print(\"=== FILE EXISTENCE CHECK ===\")\n",
    "ok = True\n",
    "for p in expected:\n",
    "    exists = p.exists()\n",
    "    ok &= exists\n",
    "    print((\"âœ…\" if exists else \"âŒ\"), p.relative_to(PROJECT_ROOT))\n",
    "\n",
    "print(\"\\n=== COHORT SPLIT CHECK ===\")\n",
    "coh_path = COHORT_DIR / \"ceu_case_control_test_split.json\"\n",
    "if coh_path.exists():\n",
    "    cohorts = json.loads(coh_path.read_text())\n",
    "    n_control = len(cohorts[\"control\"][\"sample_ids\"])\n",
    "    n_case    = len(cohorts[\"case\"][\"sample_ids\"])\n",
    "    n_test    = len(cohorts[\"test\"][\"sample_ids\"])\n",
    "    print(\"control/case/test =\", n_control, n_case, n_test)\n",
    "\n",
    "    # these keys are the ones you previously crashed on\n",
    "    print(\"has indices_in_ceu_matrix?\",\n",
    "          \"indices_in_ceu_matrix\" in cohorts[\"control\"],\n",
    "          \"indices_in_ceu_matrix\" in cohorts[\"test\"],\n",
    "          \"indices_in_ceu_matrix\" in cohorts[\"case\"])\n",
    "\n",
    "    if (n_control, n_case, n_test) != (80, 50, 44):\n",
    "        print(\"âš ï¸ Split is not 80/50/44 â€” did you rerun the split cell after editing?\")\n",
    "else:\n",
    "    ok = False\n",
    "    print(\"âŒ Missing cohort split JSON.\")\n",
    "\n",
    "print(\"\\nFINAL:\", \"âœ… preprocessing outputs are complete\" if ok else \"âŒ something is missing above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
