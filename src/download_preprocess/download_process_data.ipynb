{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f15cadc",
      "metadata": {},
      "source": [
        "# HapMap preprocessing (Phase III polymorphic)\n",
        "\n",
        "This notebook downloads HapMap Phase III *polymorphic hapmap_format* genotypes, auto-selects a CASE population\n",
        "with the largest cohort size present in both chr2 and chr10, extracts two regions (chr2_5Mb, chr10_1Mb),\n",
        "creates per-region case train/test splits, and builds control haplotype blocks/histograms for Method 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0476e11b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_ROOT = /Users/erkmenerken/Desktop/proje430\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import json\n",
        "import gzip\n",
        "import time\n",
        "import urllib.request\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Robust PROJECT_ROOT detection\n",
        "\n",
        "def find_project_root(start: Path = None) -> Path:\n",
        "    cur = Path.cwd() if start is None else Path(start)\n",
        "    cur = cur.resolve()\n",
        "    for p in [cur] + list(cur.parents):\n",
        "        if (p / \".git\").exists() or (p / \"requirements.txt\").exists():\n",
        "            return p\n",
        "    return cur\n",
        "\n",
        "PROJECT_ROOT = find_project_root()\n",
        "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
        "\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "RAW_DIR = DATA_DIR / \"raw\" / \"hapmap\"\n",
        "PROC_DIR = DATA_DIR / \"processed\" / \"hapmap\"\n",
        "\n",
        "GENO_DIR = RAW_DIR / \"genotypes\"\n",
        "PHASE_DIR = RAW_DIR / \"phasing\" / \"HapMap3_r2\"\n",
        "PHASE_META_DIR = RAW_DIR / \"phasing\" / \"HapMap3_r2_meta\"\n",
        "\n",
        "REGION_DIR = PROC_DIR / \"regions\"\n",
        "COHORT_DIR = PROC_DIR / \"cohorts\"\n",
        "BLOCK_DIR = PROC_DIR / \"blocks\"\n",
        "HAP_DIR = PROC_DIR / \"haplotypes\"\n",
        "BLOCK_DIR.mkdir(parents=True, exist_ok=True)\n",
        "HAP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PHASE_INFO_DIR = PROC_DIR / \"phasing\"\n",
        "\n",
        "for d in [GENO_DIR, PHASE_DIR, PHASE_META_DIR, REGION_DIR, COHORT_DIR, BLOCK_DIR, HAP_DIR, PHASE_INFO_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "FORCE_REDOWNLOAD = False\n",
        "FORCE_REBUILD = False\n",
        "SPLIT_SEED = 0\n",
        "TRAIN_FRAC = 0.7\n",
        "\n",
        "TARGET_SNPS_CHR2 = 311\n",
        "TARGET_SNPS_CHR10 = 610\n",
        "WINDOW_BP_CHR2 = 5_000_000\n",
        "WINDOW_BP_CHR10 = 1_000_000\n",
        "\n",
        "TOP_K = 50  \n",
        "\n",
        "POLYMORPHIC_BASE = \"https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/latest_phaseIII_ncbi_b36/hapmap_format/polymorphic/\"\n",
        "PHASE_BASE = \"https://ftp.ncbi.nlm.nih.gov/hapmap/phasing/2009-02_phaseIII/HapMap3_r2\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "809f9ed1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def download_file(url: str, dst: Path, overwrite: bool = False):\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if dst.exists() and dst.stat().st_size > 0 and not overwrite:\n",
        "        print(f\"  Already exists, skipping: {dst.name} ({dst.stat().st_size/1e6:.2f} MB)\")\n",
        "        return\n",
        "\n",
        "    tmp = dst.with_suffix(dst.suffix + \".part\")\n",
        "    if tmp.exists():\n",
        "        tmp.unlink()\n",
        "\n",
        "    print(f\"  Downloading: {url}\")\n",
        "    print(f\"  Saving to  : {dst}\")\n",
        "\n",
        "    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    with urllib.request.urlopen(req) as resp, open(tmp, \"wb\") as f:\n",
        "        while True:\n",
        "            chunk = resp.read(1024 * 1024)\n",
        "            if not chunk:\n",
        "                break\n",
        "            f.write(chunk)\n",
        "\n",
        "    tmp.rename(dst)\n",
        "\n",
        "    # Gzip magic check for .gz\n",
        "    if dst.suffix == \".gz\":\n",
        "        with open(dst, \"rb\") as fh:\n",
        "            magic = fh.read(2)\n",
        "        if magic != b\"\\x1f\\x8b\":\n",
        "            raise RuntimeError(f\"Downloaded file does not look like gzip: {dst}\")\n",
        "\n",
        "\n",
        "def _find_sample_columns(columns: List[str]):\n",
        "    cols = list(columns)\n",
        "    if \"QCcode\" in cols:\n",
        "        qc_idx = cols.index(\"QCcode\")\n",
        "        return cols[: qc_idx + 1], cols[qc_idx + 1 :]\n",
        "    # Fallback (rarely needed)\n",
        "    return cols[:11], cols[11:]\n",
        "\n",
        "\n",
        "def list_polymorphic_files() -> List[str]:\n",
        "    req = urllib.request.Request(POLYMORPHIC_BASE, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    with urllib.request.urlopen(req) as resp:\n",
        "        html = resp.read().decode(\"utf-8\", errors=\"ignore\")\n",
        "    pattern = r\"genotypes_chr(\\d+)_([A-Z]{3})_phase3\\.2_nr\\.b36_fwd\\.txt\\.gz\"\n",
        "    return re.findall(pattern, html)\n",
        "\n",
        "\n",
        "def count_individuals_from_remote_header(url: str) -> int:\n",
        "    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    with urllib.request.urlopen(req) as resp:\n",
        "        with gzip.GzipFile(fileobj=resp) as gz:\n",
        "            header = gz.readline().decode(\"utf-8\", errors=\"replace\").strip()\n",
        "    cols = header.split()\n",
        "    _, sample_cols = _find_sample_columns(cols)\n",
        "    return len(sample_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "96f32d13",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top candidate CASE populations (by min(n_chr2, n_chr10)):\n",
            "  MKK: n_chr2=171, n_chr10=171, min=171\n",
            "  YRI: n_chr2=167, n_chr10=167, min=167\n",
            "  LWK: n_chr2=90, n_chr10=90, min=90\n",
            "  GIH: n_chr2=88, n_chr10=88, min=88\n",
            "  TSI: n_chr2=88, n_chr10=88, min=88\n",
            "Selected CASE_POP = MKK\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Discover CASE population (largest cohort in chr2 & chr10)\n",
        "\n",
        "\n",
        "matches = list_polymorphic_files()\n",
        "if not matches:\n",
        "    raise RuntimeError(\"Could not list polymorphic HapMap files. Check network access or base URL.\")\n",
        "\n",
        "pops_by_chr = {}\n",
        "file_map = {}\n",
        "for chrom_str, pop in matches:\n",
        "    chrom = int(chrom_str)\n",
        "    pops_by_chr.setdefault(chrom, set()).add(pop)\n",
        "    file_map[(chrom, pop)] = f\"genotypes_chr{chrom}_{pop}_phase3.2_nr.b36_fwd.txt.gz\"\n",
        "\n",
        "cand_pops = sorted((pops_by_chr.get(2, set()) & pops_by_chr.get(10, set())) - {\"CEU\"})\n",
        "if not cand_pops:\n",
        "    \n",
        "    cand_pops = sorted(pops_by_chr.get(2, set()) & pops_by_chr.get(10, set()))\n",
        "    if not cand_pops:\n",
        "        raise RuntimeError(\"No populations present in both chr2 and chr10 in the polymorphic directory.\")\n",
        "\n",
        "pop_sizes = []\n",
        "for pop in cand_pops:\n",
        "    url2 = POLYMORPHIC_BASE + file_map[(2, pop)]\n",
        "    url10 = POLYMORPHIC_BASE + file_map[(10, pop)]\n",
        "    n2 = count_individuals_from_remote_header(url2)\n",
        "    n10 = count_individuals_from_remote_header(url10)\n",
        "    pop_sizes.append({\"pop\": pop, \"n_chr2\": n2, \"n_chr10\": n10, \"min_n\": min(n2, n10)})\n",
        "\n",
        "pop_sizes_sorted = sorted(pop_sizes, key=lambda d: (-d[\"min_n\"], d[\"pop\"]))\n",
        "\n",
        "top5 = pop_sizes_sorted[:5]\n",
        "print(\"Top candidate CASE populations (by min(n_chr2, n_chr10)):\")\n",
        "for row in top5:\n",
        "    print(f\"  {row['pop']}: n_chr2={row['n_chr2']}, n_chr10={row['n_chr10']}, min={row['min_n']}\")\n",
        "\n",
        "CASE_POP = pop_sizes_sorted[0][\"pop\"]\n",
        "print(\"Selected CASE_POP =\", CASE_POP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f8c2f390",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting genotype downloads...\n",
            "=== CEU genotypes chr2 ===\n",
            "  Already exists, skipping: genotypes_chr2_CEU_phase3.2_nr.b36_fwd.txt.gz (6.85 MB)\n",
            "=== MKK genotypes chr2 ===\n",
            "  Already exists, skipping: genotypes_chr2_MKK_phase3.2_nr.b36_fwd.txt.gz (8.24 MB)\n",
            "=== CEU genotypes chr10 ===\n",
            "  Already exists, skipping: genotypes_chr10_CEU_phase3.2_nr.b36_fwd.txt.gz (4.36 MB)\n",
            "=== MKK genotypes chr10 ===\n",
            "  Already exists, skipping: genotypes_chr10_MKK_phase3.2_nr.b36_fwd.txt.gz (5.12 MB)\n",
            "Genotype downloads ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Download CEU + CASE_POP genotype files\n",
        "\n",
        "\n",
        "MANIFEST = []\n",
        "for chrom in (2, 10):\n",
        "    for pop in (\"CEU\", CASE_POP):\n",
        "        fname = f\"genotypes_chr{chrom}_{pop}_phase3.2_nr.b36_fwd.txt.gz\"\n",
        "        url = POLYMORPHIC_BASE + fname\n",
        "        MANIFEST.append({\n",
        "            \"name\": f\"{pop} genotypes chr{chrom}\",\n",
        "            \"url\": url,\n",
        "            \"dst\": GENO_DIR / fname,\n",
        "        })\n",
        "\n",
        "print(\"Starting genotype downloads...\")\n",
        "for item in MANIFEST:\n",
        "    print(f\"=== {item['name']} ===\")\n",
        "    download_file(item[\"url\"], item[\"dst\"], overwrite=FORCE_REDOWNLOAD)\n",
        "\n",
        "print(\"Genotype downloads ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "16fc0d8e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Reading rs# + pos from genotypes_chr2_CEU_phase3.2_nr.b36_fwd.txt.gz\n",
            " Loaded 113979 SNP positions.\n",
            " Reading rs# + pos from genotypes_chr10_CEU_phase3.2_nr.b36_fwd.txt.gz\n",
            " Loaded 73116 SNP positions.\n",
            "windows selected:\n",
            "chr2 (5Mb, ~311 SNPs): {'start_bp': 88196854, 'end_bp': 93196854, 'snps_in_window': 311, 'end_pos_actual': 91649657, 'total_snps_chr': 113979, 'min_pos': 5703, 'max_pos': 242710183}\n",
            "chr10 (1Mb, ~610 SNPs): {'start_bp': 8729009, 'end_bp': 9729009, 'snps_in_window': 610, 'end_pos_actual': 9726663, 'total_snps_chr': 73116, 'min_pos': 68983, 'max_pos': 135372380}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Region selection\n",
        "\n",
        "\n",
        "def load_positions_rsids(geno_path: Path) -> pd.DataFrame:\n",
        "    print(f\" Reading rs# + pos from {geno_path.name}\")\n",
        "    df = pd.read_csv(\n",
        "        geno_path,\n",
        "        sep=r\"\\s+\",\n",
        "        engine=\"python\",\n",
        "        compression=\"gzip\",\n",
        "        usecols=[\"rs#\", \"pos\"],\n",
        "        dtype={\"rs#\": str, \"pos\": int},\n",
        "    )\n",
        "    df = df.dropna().sort_values(\"pos\").reset_index(drop=True)\n",
        "    print(f\" Loaded {len(df)} SNP positions.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def choose_window_by_bp(df_pos: pd.DataFrame, window_bp: int, target_snps: int) -> Dict[str, int]:\n",
        "    pos = df_pos[\"pos\"].to_numpy(np.int64)\n",
        "    n = len(pos)\n",
        "    ends = np.searchsorted(pos, pos + window_bp, side=\"right\")\n",
        "    counts = ends - np.arange(n)\n",
        "\n",
        "    diff = np.abs(counts - target_snps)\n",
        "    best_i = int(np.argmin(diff))\n",
        "\n",
        "    start = int(pos[best_i])\n",
        "    end = int(start + window_bp)\n",
        "    count = int(counts[best_i])\n",
        "    end_idx = int(ends[best_i] - 1)\n",
        "    end_pos_actual = int(pos[end_idx]) if end_idx >= best_i else start\n",
        "\n",
        "    return {\n",
        "        \"start_bp\": start,\n",
        "        \"end_bp\": end,\n",
        "        \"snps_in_window\": count,\n",
        "        \"end_pos_actual\": end_pos_actual,\n",
        "        \"total_snps_chr\": int(n),\n",
        "        \"min_pos\": int(pos[0]),\n",
        "        \"max_pos\": int(pos[-1]),\n",
        "    }\n",
        "\n",
        "ceu_chr2_path = GENO_DIR / f\"genotypes_chr2_CEU_phase3.2_nr.b36_fwd.txt.gz\"\n",
        "ceu_chr10_path = GENO_DIR / f\"genotypes_chr10_CEU_phase3.2_nr.b36_fwd.txt.gz\"\n",
        "\n",
        "if not ceu_chr2_path.exists() or not ceu_chr10_path.exists():\n",
        "    raise FileNotFoundError(\"Missing CEU genotype files. Re-run the download cell.\")\n",
        "\n",
        "chr2_df = load_positions_rsids(ceu_chr2_path)\n",
        "chr10_df = load_positions_rsids(ceu_chr10_path)\n",
        "\n",
        "chr2_plan = choose_window_by_bp(chr2_df, WINDOW_BP_CHR2, TARGET_SNPS_CHR2)\n",
        "chr10_plan = choose_window_by_bp(chr10_df, WINDOW_BP_CHR10, TARGET_SNPS_CHR10)\n",
        "\n",
        "print(\"windows selected:\")\n",
        "print(\"chr2 (5Mb, ~311 SNPs):\", chr2_plan)\n",
        "print(\"chr10 (1Mb, ~610 SNPs):\", chr10_plan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0f2fc444",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "MISSING_TOKENS = {\"\", \"NN\", \"NA\", \"N\", \"00\", \"--\", \"??\"}\n",
        "\n",
        "\n",
        "def genotype_row_to_counts(geno_strs, allele_a, allele_b, count_mode=\"minor\"):\n",
        "    g = np.asarray(geno_strs, dtype=object)\n",
        "    g = np.array([x.strip() if isinstance(x, str) else \"\" for x in g], dtype=object)\n",
        "\n",
        "    missing = np.zeros(len(g), dtype=bool)\n",
        "    for t in MISSING_TOKENS:\n",
        "        missing |= (g == t)\n",
        "\n",
        "    valid = (~missing) & np.array([len(x) == 2 for x in g], dtype=bool)\n",
        "\n",
        "    a_count = 0\n",
        "    b_count = 0\n",
        "    for x in g[valid]:\n",
        "        a_count += (x[0] == allele_a) + (x[1] == allele_a)\n",
        "        b_count += (x[0] == allele_b) + (x[1] == allele_b)\n",
        "\n",
        "    if count_mode == \"minor\":\n",
        "        if a_count < b_count:\n",
        "            counted, other = allele_a, allele_b\n",
        "        elif b_count < a_count:\n",
        "            counted, other = allele_b, allele_a\n",
        "        else:\n",
        "            counted, other = allele_b, allele_a\n",
        "    elif count_mode == \"allele_a\":\n",
        "        counted, other = allele_a, allele_b\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown count_mode={count_mode}\")\n",
        "\n",
        "    out = np.full(len(g), -1, dtype=np.int8)\n",
        "    for i, x in enumerate(g):\n",
        "        if (not isinstance(x, str)) or (x in MISSING_TOKENS) or len(x) != 2:\n",
        "            continue\n",
        "        out[i] = np.int8((x[0] == counted) + (x[1] == counted))\n",
        "\n",
        "    return out, counted, other\n",
        "\n",
        "\n",
        "def parse_genotypes_window(geno_path: Path, start_bp: int, end_bp: int, chunksize: int = 2000, count_mode=\"minor\"):\n",
        "    print(f\"Parsing genotypes from {geno_path.name}\")\n",
        "    print(f\"   Window: [{start_bp}, {end_bp}] bp (inclusive)\")\n",
        "    print(f\"   Count mode: {count_mode}\")\n",
        "\n",
        "    header = pd.read_csv(\n",
        "        geno_path,\n",
        "        sep=r\"\\s+\",\n",
        "        engine=\"python\",\n",
        "        compression=\"gzip\",\n",
        "        nrows=1,\n",
        "        dtype=str,\n",
        "    )\n",
        "    _, sample_cols = _find_sample_columns(header.columns)\n",
        "    sample_ids = np.array(sample_cols, dtype=object)\n",
        "    print(f\" Individuals detected: {len(sample_ids)}\")\n",
        "\n",
        "    G_cols, snp_ids, positions, counted_alleles, other_alleles = [], [], [], [], []\n",
        "\n",
        "    reader = pd.read_csv(\n",
        "        geno_path,\n",
        "        sep=r\"\\s+\",\n",
        "        engine=\"python\",\n",
        "        compression=\"gzip\",\n",
        "        dtype=str,\n",
        "        chunksize=chunksize,\n",
        "    )\n",
        "\n",
        "    kept = 0\n",
        "    for chunk in reader:\n",
        "        pos_int = pd.to_numeric(chunk[\"pos\"], errors=\"coerce\")\n",
        "        mask = (pos_int >= start_bp) & (pos_int <= end_bp)\n",
        "        chunk = chunk.loc[mask]\n",
        "        if chunk.empty:\n",
        "            continue\n",
        "\n",
        "        for _, row in chunk.iterrows():\n",
        "            rsid = row.get(\"rs#\", None)\n",
        "            alleles = row.get(\"alleles\", None)\n",
        "            pos = row.get(\"pos\", None)\n",
        "\n",
        "            if rsid is None or alleles is None or pos is None or \"/\" not in alleles:\n",
        "                continue\n",
        "\n",
        "            a, b = [x.strip() for x in alleles.split(\"/\", 1)]\n",
        "            if len(a) != 1 or len(b) != 1:\n",
        "                continue\n",
        "\n",
        "            geno_strs = row[sample_cols].values\n",
        "            counts, counted, other = genotype_row_to_counts(geno_strs, a, b, count_mode=count_mode)\n",
        "\n",
        "            G_cols.append(counts)\n",
        "            snp_ids.append(rsid)\n",
        "            positions.append(int(pos))\n",
        "            counted_alleles.append(counted)\n",
        "            other_alleles.append(other)\n",
        "            kept += 1\n",
        "\n",
        "    if kept == 0:\n",
        "        raise RuntimeError(\"No SNPs were kept. Check start/end window values.\")\n",
        "\n",
        "    G = np.stack(G_cols, axis=1)  # (M, N)\n",
        "    positions = np.array(positions, dtype=np.int32)\n",
        "\n",
        "    order = np.argsort(positions)\n",
        "    G = G[:, order]\n",
        "    positions = positions[order]\n",
        "    snp_ids = np.array(snp_ids, dtype=object)[order]\n",
        "    counted_alleles = np.array(counted_alleles, dtype=object)[order]\n",
        "    other_alleles = np.array(other_alleles, dtype=object)[order]\n",
        "\n",
        "    print(f\" Kept SNPs: {G.shape[1]} | Individuals: {G.shape[0]}\")\n",
        "    print(f\" Missing rate: {float(np.mean(G == -1)):.4f}\")\n",
        "    print(f\" Kept position range: {int(positions.min())} .. {int(positions.max())}\")\n",
        "\n",
        "    return G, sample_ids, snp_ids, positions, counted_alleles, other_alleles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5467fbc4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Using existing CEU region file for chr2\n",
            " Using existing MKK region file for chr2\n",
            " Using existing aligned files for chr2\n",
            " Using existing CEU region file for chr10\n",
            " Using existing MKK region file for chr10\n",
            " Using existing aligned files for chr10\n",
            "Alignment complete: CEU/CASE_POP common SNP files ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Extract + align CEU vs CASE_POP regions\n",
        "\n",
        "\n",
        "ceu_chr2_path = GENO_DIR / f\"genotypes_chr2_CEU_phase3.2_nr.b36_fwd.txt.gz\"\n",
        "ceu_chr10_path = GENO_DIR / f\"genotypes_chr10_CEU_phase3.2_nr.b36_fwd.txt.gz\"\n",
        "case_chr2_path = GENO_DIR / f\"genotypes_chr2_{CASE_POP}_phase3.2_nr.b36_fwd.txt.gz\"\n",
        "case_chr10_path = GENO_DIR / f\"genotypes_chr10_{CASE_POP}_phase3.2_nr.b36_fwd.txt.gz\"\n",
        "\n",
        "CEU_REGION_CHR2 = REGION_DIR / \"CEU_chr2_5Mb.npz\"\n",
        "CEU_REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.npz\"\n",
        "CASE_REGION_CHR2 = REGION_DIR / f\"{CASE_POP}_chr2_5Mb.npz\"\n",
        "CASE_REGION_CHR10 = REGION_DIR / f\"{CASE_POP}_chr10_1Mb.npz\"\n",
        "\n",
        "ALIGNED_CEU_CHR2 = REGION_DIR / f\"CEU_chr2_5Mb.common_with_{CASE_POP}.npz\"\n",
        "ALIGNED_CASE_CHR2 = REGION_DIR / f\"{CASE_POP}_chr2_5Mb.common_with_CEU.npz\"\n",
        "ALIGNED_CEU_CHR10 = REGION_DIR / f\"CEU_chr10_1Mb.common_with_{CASE_POP}.npz\"\n",
        "ALIGNED_CASE_CHR10 = REGION_DIR / f\"{CASE_POP}_chr10_1Mb.common_with_CEU.npz\"\n",
        "\n",
        "\n",
        "def _parse_region_for_alignment(geno_path, start_bp, end_bp):\n",
        "    G, sample_ids, snp_ids, positions, counted, other = parse_genotypes_window(\n",
        "        geno_path, start_bp, end_bp, count_mode=\"allele_a\"\n",
        "    )\n",
        "    return {\n",
        "        \"G\": G,\n",
        "        \"sample_ids\": sample_ids,\n",
        "        \"snp_ids\": snp_ids,\n",
        "        \"positions\": positions,\n",
        "        \"counted_alleles\": counted,\n",
        "        \"other_alleles\": other,\n",
        "    }\n",
        "\n",
        "\n",
        "def _align_by_snp(ceu, case, region_tag):\n",
        "    ceu_rsids = [str(x) for x in ceu[\"snp_ids\"]]\n",
        "    case_rsids = [str(x) for x in case[\"snp_ids\"]]\n",
        "    ceu_idx = {rs: i for i, rs in enumerate(ceu_rsids)}\n",
        "    case_idx = {rs: i for i, rs in enumerate(case_rsids)}\n",
        "\n",
        "    common_rsids = [rs for rs in ceu_rsids if rs in case_idx]\n",
        "    dropped_only_ceu = len(ceu_rsids) - len(common_rsids)\n",
        "    dropped_only_case = len(case_rsids) - len(common_rsids)\n",
        "\n",
        "    idx_ceu = []\n",
        "    idx_case = []\n",
        "    mismatch = 0\n",
        "    for rs in common_rsids:\n",
        "        i = ceu_idx[rs]\n",
        "        j = case_idx[rs]\n",
        "        if (ceu[\"positions\"][i] != case[\"positions\"][j]):\n",
        "            mismatch += 1\n",
        "            continue\n",
        "        if (ceu[\"counted_alleles\"][i] != case[\"counted_alleles\"][j]) or (ceu[\"other_alleles\"][i] != case[\"other_alleles\"][j]):\n",
        "            mismatch += 1\n",
        "            continue\n",
        "        idx_ceu.append(i)\n",
        "        idx_case.append(j)\n",
        "\n",
        "    print(f\"[{region_tag}] CEU SNPs: {len(ceu_rsids)} | {CASE_POP} SNPs: {len(case_rsids)}\")\n",
        "    print(f\"[{region_tag}] Common rsIDs: {len(common_rsids)} | mismatched dropped: {mismatch}\")\n",
        "    print(f\"[{region_tag}] Dropped only-in-CEU: {dropped_only_ceu} | only-in-{CASE_POP}: {dropped_only_case}\")\n",
        "\n",
        "    if len(idx_ceu) == 0:\n",
        "        raise RuntimeError(f\"No aligned SNPs remain for {region_tag} after filtering.\")\n",
        "\n",
        "    ceu_aligned = {\n",
        "        \"G\": ceu[\"G\"][:, idx_ceu],\n",
        "        \"sample_ids\": ceu[\"sample_ids\"],\n",
        "        \"snp_ids\": ceu[\"snp_ids\"][idx_ceu],\n",
        "        \"positions\": ceu[\"positions\"][idx_ceu],\n",
        "        \"counted_alleles\": ceu[\"counted_alleles\"][idx_ceu],\n",
        "        \"other_alleles\": ceu[\"other_alleles\"][idx_ceu],\n",
        "    }\n",
        "    case_aligned = {\n",
        "        \"G\": case[\"G\"][:, idx_case],\n",
        "        \"sample_ids\": case[\"sample_ids\"],\n",
        "        \"snp_ids\": case[\"snp_ids\"][idx_case],\n",
        "        \"positions\": case[\"positions\"][idx_case],\n",
        "        \"counted_alleles\": case[\"counted_alleles\"][idx_case],\n",
        "        \"other_alleles\": case[\"other_alleles\"][idx_case],\n",
        "    }\n",
        "\n",
        "    assert np.array_equal(ceu_aligned[\"snp_ids\"], case_aligned[\"snp_ids\"])\n",
        "    assert np.array_equal(ceu_aligned[\"positions\"], case_aligned[\"positions\"])\n",
        "    assert np.array_equal(ceu_aligned[\"counted_alleles\"], case_aligned[\"counted_alleles\"])\n",
        "\n",
        "    return ceu_aligned, case_aligned\n",
        "\n",
        "\n",
        "def _save_region(out_path, region_tag, chrom, plan, payload, pop_label):\n",
        "    np.savez_compressed(\n",
        "        out_path,\n",
        "        G=payload[\"G\"].astype(np.int8),\n",
        "        sample_ids=payload[\"sample_ids\"],\n",
        "        snp_ids=payload[\"snp_ids\"],\n",
        "        positions=payload[\"positions\"],\n",
        "        counted_alleles=payload[\"counted_alleles\"],\n",
        "        other_alleles=payload[\"other_alleles\"],\n",
        "        minor_alleles=payload[\"counted_alleles\"],\n",
        "        major_alleles=payload[\"other_alleles\"],\n",
        "        chrom=str(chrom),\n",
        "    )\n",
        "    meta = {\n",
        "        \"region_name\": out_path.stem,\n",
        "        \"chrom\": str(chrom),\n",
        "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"window_start_bp\": int(plan[\"start_bp\"]),\n",
        "        \"window_end_bp\": int(plan[\"end_bp\"]),\n",
        "        \"snps_in_window\": int(payload[\"G\"].shape[1]),\n",
        "        \"individuals\": int(payload[\"G\"].shape[0]),\n",
        "        \"count_mode\": \"allele_a\",\n",
        "        \"note\": f\"Raw {pop_label} window using CEU-derived region bounds (not aligned).\",\n",
        "    }\n",
        "    out_path.with_suffix(\".meta.json\").write_text(json.dumps(meta, indent=2))\n",
        "    print(f\" Saved region: {out_path.relative_to(PROJECT_ROOT)}\")\n",
        "\n",
        "\n",
        "def _save_aligned_region(out_path, region_tag, chrom, plan, payload, other_pop):\n",
        "    np.savez_compressed(\n",
        "        out_path,\n",
        "        G=payload[\"G\"].astype(np.int8),\n",
        "        sample_ids=payload[\"sample_ids\"],\n",
        "        snp_ids=payload[\"snp_ids\"],\n",
        "        positions=payload[\"positions\"],\n",
        "        counted_alleles=payload[\"counted_alleles\"],\n",
        "        other_alleles=payload[\"other_alleles\"],\n",
        "        minor_alleles=payload[\"counted_alleles\"],\n",
        "        major_alleles=payload[\"other_alleles\"],\n",
        "        chrom=str(chrom),\n",
        "    )\n",
        "    meta = {\n",
        "        \"region_name\": out_path.stem,\n",
        "        \"chrom\": str(chrom),\n",
        "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"window_start_bp\": int(plan[\"start_bp\"]),\n",
        "        \"window_end_bp\": int(plan[\"end_bp\"]),\n",
        "        \"snps_in_window\": int(payload[\"G\"].shape[1]),\n",
        "        \"individuals\": int(payload[\"G\"].shape[0]),\n",
        "        \"count_mode\": \"allele_a\",\n",
        "        \"note\": f\"Aligned with {other_pop} on common SNPs; G counts allele_a from the alleles column.\",\n",
        "    }\n",
        "    out_path.with_suffix(\".meta.json\").write_text(json.dumps(meta, indent=2))\n",
        "    print(f\" Saved aligned: {out_path.relative_to(PROJECT_ROOT)}\")\n",
        "\n",
        "# chr2\n",
        "need_ceu2_raw = (not CEU_REGION_CHR2.exists()) or FORCE_REBUILD\n",
        "need_case2_raw = (not CASE_REGION_CHR2.exists()) or FORCE_REBUILD\n",
        "need_align_chr2 = (not ALIGNED_CEU_CHR2.exists()) or (not ALIGNED_CASE_CHR2.exists()) or FORCE_REBUILD\n",
        "\n",
        "ceu2_raw = None\n",
        "case2_raw = None\n",
        "\n",
        "if need_ceu2_raw or need_align_chr2:\n",
        "    ceu2_raw = _parse_region_for_alignment(ceu_chr2_path, chr2_plan[\"start_bp\"], chr2_plan[\"end_bp\"])\n",
        "if need_case2_raw or need_align_chr2:\n",
        "    case2_raw = _parse_region_for_alignment(case_chr2_path, chr2_plan[\"start_bp\"], chr2_plan[\"end_bp\"])\n",
        "\n",
        "if need_ceu2_raw:\n",
        "    _save_region(CEU_REGION_CHR2, \"chr2_5Mb\", 2, chr2_plan, ceu2_raw, \"CEU\")\n",
        "else:\n",
        "    print(\" Using existing CEU region file for chr2\")\n",
        "\n",
        "if need_case2_raw:\n",
        "    _save_region(CASE_REGION_CHR2, \"chr2_5Mb\", 2, chr2_plan, case2_raw, CASE_POP)\n",
        "else:\n",
        "    print(f\" Using existing {CASE_POP} region file for chr2\")\n",
        "\n",
        "if need_align_chr2:\n",
        "    ceu2_aligned, case2_aligned = _align_by_snp(ceu2_raw, case2_raw, \"chr2_5Mb\")\n",
        "    _save_aligned_region(ALIGNED_CEU_CHR2, \"chr2_5Mb\", 2, chr2_plan, ceu2_aligned, CASE_POP)\n",
        "    _save_aligned_region(ALIGNED_CASE_CHR2, \"chr2_5Mb\", 2, chr2_plan, case2_aligned, \"CEU\")\n",
        "else:\n",
        "    print(\" Using existing aligned files for chr2\")\n",
        "    ceu2_aligned = np.load(ALIGNED_CEU_CHR2, allow_pickle=True)\n",
        "    case2_aligned = np.load(ALIGNED_CASE_CHR2, allow_pickle=True)\n",
        "    assert np.array_equal(ceu2_aligned[\"snp_ids\"], case2_aligned[\"snp_ids\"])\n",
        "    assert np.array_equal(ceu2_aligned[\"positions\"], case2_aligned[\"positions\"])\n",
        "\n",
        "# chr10\n",
        "need_ceu10_raw = (not CEU_REGION_CHR10.exists()) or FORCE_REBUILD\n",
        "need_case10_raw = (not CASE_REGION_CHR10.exists()) or FORCE_REBUILD\n",
        "need_align_chr10 = (not ALIGNED_CEU_CHR10.exists()) or (not ALIGNED_CASE_CHR10.exists()) or FORCE_REBUILD\n",
        "\n",
        "ceu10_raw = None\n",
        "case10_raw = None\n",
        "\n",
        "if need_ceu10_raw or need_align_chr10:\n",
        "    ceu10_raw = _parse_region_for_alignment(ceu_chr10_path, chr10_plan[\"start_bp\"], chr10_plan[\"end_bp\"])\n",
        "if need_case10_raw or need_align_chr10:\n",
        "    case10_raw = _parse_region_for_alignment(case_chr10_path, chr10_plan[\"start_bp\"], chr10_plan[\"end_bp\"])\n",
        "\n",
        "if need_ceu10_raw:\n",
        "    _save_region(CEU_REGION_CHR10, \"chr10_1Mb\", 10, chr10_plan, ceu10_raw, \"CEU\")\n",
        "else:\n",
        "    print(\" Using existing CEU region file for chr10\")\n",
        "\n",
        "if need_case10_raw:\n",
        "    _save_region(CASE_REGION_CHR10, \"chr10_1Mb\", 10, chr10_plan, case10_raw, CASE_POP)\n",
        "else:\n",
        "    print(f\" Using existing {CASE_POP} region file for chr10\")\n",
        "\n",
        "if need_align_chr10:\n",
        "    ceu10_aligned, case10_aligned = _align_by_snp(ceu10_raw, case10_raw, \"chr10_1Mb\")\n",
        "    _save_aligned_region(ALIGNED_CEU_CHR10, \"chr10_1Mb\", 10, chr10_plan, ceu10_aligned, CASE_POP)\n",
        "    _save_aligned_region(ALIGNED_CASE_CHR10, \"chr10_1Mb\", 10, chr10_plan, case10_aligned, \"CEU\")\n",
        "else:\n",
        "    print(\" Using existing aligned files for chr10\")\n",
        "    ceu10_aligned = np.load(ALIGNED_CEU_CHR10, allow_pickle=True)\n",
        "    case10_aligned = np.load(ALIGNED_CASE_CHR10, allow_pickle=True)\n",
        "    assert np.array_equal(ceu10_aligned[\"snp_ids\"], case10_aligned[\"snp_ids\"])\n",
        "    assert np.array_equal(ceu10_aligned[\"positions\"], case10_aligned[\"positions\"])\n",
        "\n",
        "print(\"Alignment complete: CEU/CASE_POP common SNP files ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "929778ae",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cohort sizes (chr2): CEU control= 165 CASE train= 120 CASE test= 51\n",
            "Cohort sizes (chr10): CEU control= 165 CASE train= 120 CASE test= 51\n",
            "Saved CASE_POP cohort summary: data/processed/hapmap/cohorts/hapmap_case_pop.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Cohort splits per region (CASE_POP train/test, CEU control)\n",
        "\n",
        "\n",
        "def load_region_npz(path: Path) -> Dict[str, np.ndarray]:\n",
        "    z = np.load(path, allow_pickle=True)\n",
        "    return {k: z[k] for k in z.files}\n",
        "\n",
        "\n",
        "def split_case(sample_ids: List[str], seed: int = 0, frac_train: float = 0.7):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    ids = list(sample_ids)\n",
        "    idx = rng.permutation(len(ids))\n",
        "    n = len(ids)\n",
        "    n_train = int(round(n * frac_train))\n",
        "    n_train = max(1, min(n - 1, n_train))\n",
        "    train_idx = idx[:n_train]\n",
        "    test_idx = idx[n_train:]\n",
        "    train_ids = [ids[i] for i in train_idx]\n",
        "    test_ids = [ids[i] for i in test_idx]\n",
        "    return train_idx.tolist(), test_idx.tolist(), train_ids, test_ids\n",
        "\n",
        "ceu_chr2 = load_region_npz(ALIGNED_CEU_CHR2)\n",
        "ceu_chr10 = load_region_npz(ALIGNED_CEU_CHR10)\n",
        "case_chr2 = load_region_npz(ALIGNED_CASE_CHR2)\n",
        "case_chr10 = load_region_npz(ALIGNED_CASE_CHR10)\n",
        "\n",
        "# CEU control: all individuals (public reference)\n",
        "ceu_ctrl_idx_chr2 = list(range(len(ceu_chr2[\"sample_ids\"])))\n",
        "ceu_ctrl_idx_chr10 = list(range(len(ceu_chr10[\"sample_ids\"])))\n",
        "\n",
        "# CASE_POP train/test per region\n",
        "case_train_idx_chr2, case_test_idx_chr2, case_train_ids_chr2, case_test_ids_chr2 = split_case(\n",
        "    [str(x) for x in case_chr2[\"sample_ids\"]], seed=SPLIT_SEED, frac_train=TRAIN_FRAC\n",
        ")\n",
        "case_train_idx_chr10, case_test_idx_chr10, case_train_ids_chr10, case_test_ids_chr10 = split_case(\n",
        "    [str(x) for x in case_chr10[\"sample_ids\"]], seed=SPLIT_SEED, frac_train=TRAIN_FRAC\n",
        ")\n",
        "\n",
        "print(\"Cohort sizes (chr2): CEU control=\", len(ceu_ctrl_idx_chr2), \"CASE train=\", len(case_train_idx_chr2), \"CASE test=\", len(case_test_idx_chr2))\n",
        "print(\"Cohort sizes (chr10): CEU control=\", len(ceu_ctrl_idx_chr10), \"CASE train=\", len(case_train_idx_chr10), \"CASE test=\", len(case_test_idx_chr10))\n",
        "\n",
        "# Save NPZs\n",
        "ceu_ctrl_chr2_npz = COHORT_DIR / \"hapmap_CEU_control_chr2_5Mb.npz\"\n",
        "ceu_ctrl_chr10_npz = COHORT_DIR / \"hapmap_CEU_control_chr10_1Mb.npz\"\n",
        "case_train_chr2_npz = COHORT_DIR / f\"hapmap_{CASE_POP}_case_train_chr2_5Mb.npz\"\n",
        "case_test_chr2_npz = COHORT_DIR / f\"hapmap_{CASE_POP}_case_test_chr2_5Mb.npz\"\n",
        "case_train_chr10_npz = COHORT_DIR / f\"hapmap_{CASE_POP}_case_train_chr10_1Mb.npz\"\n",
        "case_test_chr10_npz = COHORT_DIR / f\"hapmap_{CASE_POP}_case_test_chr10_1Mb.npz\"\n",
        "\n",
        "np.savez_compressed(ceu_ctrl_chr2_npz, sample_ids=ceu_chr2[\"sample_ids\"], indices=np.array(ceu_ctrl_idx_chr2, dtype=int))\n",
        "np.savez_compressed(ceu_ctrl_chr10_npz, sample_ids=ceu_chr10[\"sample_ids\"], indices=np.array(ceu_ctrl_idx_chr10, dtype=int))\n",
        "np.savez_compressed(case_train_chr2_npz, sample_ids=np.array(case_train_ids_chr2, dtype=object), indices=np.array(case_train_idx_chr2, dtype=int))\n",
        "np.savez_compressed(case_test_chr2_npz, sample_ids=np.array(case_test_ids_chr2, dtype=object), indices=np.array(case_test_idx_chr2, dtype=int))\n",
        "np.savez_compressed(case_train_chr10_npz, sample_ids=np.array(case_train_ids_chr10, dtype=object), indices=np.array(case_train_idx_chr10, dtype=int))\n",
        "np.savez_compressed(case_test_chr10_npz, sample_ids=np.array(case_test_ids_chr10, dtype=object), indices=np.array(case_test_idx_chr10, dtype=int))\n",
        "\n",
        "case_pop_json = COHORT_DIR / \"hapmap_case_pop.json\"\n",
        "case_pop_payload = {\n",
        "    \"case_pop\": CASE_POP,\n",
        "    \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"split_seed\": SPLIT_SEED,\n",
        "    \"split_frac_train\": TRAIN_FRAC,\n",
        "    \"phased_paths_json\": str((PHASE_INFO_DIR / f\"{CASE_POP.lower()}_phased_paths.json\").relative_to(PROJECT_ROOT)),\n",
        "    \"regions\": {\n",
        "        \"chr2_5Mb\": {\n",
        "            \"ceu_region\": str(ALIGNED_CEU_CHR2.relative_to(PROJECT_ROOT)),\n",
        "            \"case_region\": str(ALIGNED_CASE_CHR2.relative_to(PROJECT_ROOT)),\n",
        "            \"ceu_control_npz\": str(ceu_ctrl_chr2_npz.relative_to(PROJECT_ROOT)),\n",
        "            \"case_train_npz\": str(case_train_chr2_npz.relative_to(PROJECT_ROOT)),\n",
        "            \"case_test_npz\": str(case_test_chr2_npz.relative_to(PROJECT_ROOT)),\n",
        "            \"n_ceu\": len(ceu_ctrl_idx_chr2),\n",
        "            \"n_case_train\": len(case_train_idx_chr2),\n",
        "            \"n_case_test\": len(case_test_idx_chr2),\n",
        "        },\n",
        "        \"chr10_1Mb\": {\n",
        "            \"ceu_region\": str(ALIGNED_CEU_CHR10.relative_to(PROJECT_ROOT)),\n",
        "            \"case_region\": str(ALIGNED_CASE_CHR10.relative_to(PROJECT_ROOT)),\n",
        "            \"ceu_control_npz\": str(ceu_ctrl_chr10_npz.relative_to(PROJECT_ROOT)),\n",
        "            \"case_train_npz\": str(case_train_chr10_npz.relative_to(PROJECT_ROOT)),\n",
        "            \"case_test_npz\": str(case_test_chr10_npz.relative_to(PROJECT_ROOT)),\n",
        "            \"n_ceu\": len(ceu_ctrl_idx_chr10),\n",
        "            \"n_case_train\": len(case_train_idx_chr10),\n",
        "            \"n_case_test\": len(case_test_idx_chr10),\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "case_pop_json.write_text(json.dumps(case_pop_payload, indent=2))\n",
        "print(\"Saved CASE_POP cohort summary:\", case_pop_json.relative_to(PROJECT_ROOT))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bcf27329",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz (1.80 MB)\n",
            "Phased individuals (CEU, chr2, UNRELATED): 17\n",
            "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.phased.gz (4.59 MB)\n",
            "Phased individuals (CEU, chr2, TRIOS): 88\n",
            "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz (1.13 MB)\n",
            "Phased individuals (CEU, chr10, UNRELATED): 17\n",
            "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.phased.gz (2.88 MB)\n",
            "Phased individuals (CEU, chr10, TRIOS): 88\n",
            "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_mkk.unr.phased.gz (4.77 MB)\n",
            "Phased individuals (MKK, chr2, UNRELATED): 87\n",
            "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_mkk.unr.phased.gz (2.97 MB)\n",
            "Phased individuals (MKK, chr10, UNRELATED): 87\n",
            "Saved phased path info: data/processed/hapmap/phasing/mkk_phased_paths.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Resolve + download phased files for CEU and CASE_POP\n",
        "\n",
        "import ftplib\n",
        "import gzip\n",
        "import re\n",
        "\n",
        "FTP_HOST = \"ftp.ncbi.nlm.nih.gov\"\n",
        "PHASE_BASE_DIR = \"/hapmap/phasing/2009-02_phaseIII/HapMap3_r2\"\n",
        "MIN_PHASED_INDIV = 30\n",
        "\n",
        "\n",
        "def ftp_list_files(dirpath: str):\n",
        "    files = []\n",
        "    with ftplib.FTP(FTP_HOST) as ftp:\n",
        "        ftp.login()\n",
        "        ftp.cwd(dirpath)\n",
        "        names = ftp.nlst()\n",
        "        for name in names:\n",
        "            if name in (\".\", \"..\"):\n",
        "                continue\n",
        "            size = 0\n",
        "            try:\n",
        "                size = ftp.size(name) or 0\n",
        "            except Exception:\n",
        "                size = 0\n",
        "            files.append((name, size))\n",
        "    return files\n",
        "\n",
        "\n",
        "def phased_individual_count(path: Path) -> int:\n",
        "    with gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        header = f.readline().strip().split()\n",
        "    hap_cols = [c for c in header if c.endswith(\"_A\") or c.endswith(\"_B\")]\n",
        "    indivs = set(c[:-2] for c in hap_cols)\n",
        "    return len(indivs)\n",
        "\n",
        "\n",
        "def pick_phased_file_in_dir(pop: str, chrom: int, subdir: str):\n",
        "    dirpath = f\"{PHASE_BASE_DIR}/{pop}\"\n",
        "    if subdir:\n",
        "        dirpath = f\"{dirpath}/{subdir}\"\n",
        "    try:\n",
        "        files = ftp_list_files(dirpath)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "    candidates = []\n",
        "    chrom_pat = re.compile(rf\"chr{chrom}(?!\\d)\")\n",
        "    for name, size in files:\n",
        "        if chrom_pat.search(name) and name.endswith(\".phased.gz\"):\n",
        "            score = 1 if \"qc.poly\" in name else 0\n",
        "            candidates.append({\n",
        "                \"dirpath\": dirpath,\n",
        "                \"name\": name,\n",
        "                \"size\": size,\n",
        "                \"score\": score,\n",
        "                \"subdir\": subdir,\n",
        "            })\n",
        "    if not candidates:\n",
        "        return None\n",
        "\n",
        "    candidates.sort(key=lambda d: (d[\"score\"], d[\"size\"]), reverse=True)\n",
        "    return candidates[0]\n",
        "\n",
        "\n",
        "def resolve_phased_for_chrom(pop: str, chrom: int, min_ind: int = 30):\n",
        "    for subdir in [\"UNRELATED\", \"TRIOS\", \"\"]:\n",
        "        info = pick_phased_file_in_dir(pop, chrom, subdir)\n",
        "        if info is None:\n",
        "            continue\n",
        "\n",
        "        local_dir = PHASE_DIR / pop\n",
        "        if subdir:\n",
        "            local_dir = local_dir / subdir\n",
        "        local_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        url = f\"{PHASE_BASE}/{pop}\"\n",
        "        if subdir:\n",
        "            url = f\"{url}/{subdir}\"\n",
        "        url = f\"{url}/{info['name']}\"\n",
        "\n",
        "        dst = local_dir / info[\"name\"]\n",
        "        download_file(url, dst, overwrite=FORCE_REDOWNLOAD)\n",
        "        n_ind = phased_individual_count(dst)\n",
        "        print(f\"Phased individuals ({pop}, chr{chrom}, {subdir or 'ROOT'}): {n_ind}\")\n",
        "\n",
        "        if n_ind >= min_ind:\n",
        "            return {\"path\": str(dst), \"url\": url, \"n_ind\": n_ind, \"source\": subdir or \"ROOT\"}\n",
        "\n",
        "    raise RuntimeError(f\"Phased cohort for {pop} chr{chrom} still too small (<{min_ind}) across directories.\")\n",
        "\n",
        "# CEU phased (public)\n",
        "ceu_chr2_info = resolve_phased_for_chrom(\"CEU\", 2, min_ind=MIN_PHASED_INDIV)\n",
        "ceu_chr10_info = resolve_phased_for_chrom(\"CEU\", 10, min_ind=MIN_PHASED_INDIV)\n",
        "\n",
        "# CASE_POP phased\n",
        "case_chr2_info = resolve_phased_for_chrom(CASE_POP, 2, min_ind=MIN_PHASED_INDIV)\n",
        "case_chr10_info = resolve_phased_for_chrom(CASE_POP, 10, min_ind=MIN_PHASED_INDIV)\n",
        "\n",
        "phase_info = {\n",
        "    \"case_pop\": CASE_POP,\n",
        "    \"CEU\": {\"chr2\": ceu_chr2_info, \"chr10\": ceu_chr10_info},\n",
        "    \"case\": {\"chr2\": case_chr2_info, \"chr10\": case_chr10_info},\n",
        "}\n",
        "\n",
        "phase_info_path = PHASE_INFO_DIR / f\"{CASE_POP.lower()}_phased_paths.json\"\n",
        "phase_info_path.write_text(json.dumps(phase_info, indent=2))\n",
        "print(\"Saved phased path info:\", phase_info_path.relative_to(PROJECT_ROOT))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7cb49a02",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phased columns: file=hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.phased.gz, rsID_col=rsID, pos_col=None, rows=116430\n",
            "[chr2_5Mb] Phased individuals: 88\n",
            "[chr2_5Mb] SNPs matched: 274 / 300 (method=rsid)\n",
            "[chr2_5Mb] Phased allele matrices: A=(274, 88), B=(274, 88)\n",
            " Created pc2\n",
            "Phased columns: file=hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.phased.gz, rsID_col=rsID, pos_col=None, rows=73832\n",
            "[chr10_1Mb] Phased individuals: 88\n",
            "[chr10_1Mb] SNPs matched: 541 / 588 (method=rsid)\n",
            "[chr10_1Mb] Phased allele matrices: A=(541, 88), B=(541, 88)\n",
            " Created pc10\n",
            "[chr2_5Mb] SNPs matched=274, phased individuals=88, match=rsid\n",
            "[chr10_1Mb] SNPs matched=541, phased individuals=88, match=rsid\n",
            "Building phased-compatible blocks for chr2\n",
            " chr2 phased-compatible SNPs: 274 | blocks: 5\n",
            "Building phased-compatible blocks for chr10\n",
            " chr10 phased-compatible SNPs: 541 | blocks: 10\n",
            "Saved blocks -> data/processed/hapmap/blocks/CEU_chr2_5Mb.common_with_MKK.blocks.json\n",
            "Saved blocks -> data/processed/hapmap/blocks/CEU_chr10_1Mb.common_with_MKK.blocks.json\n",
            "  phased-compatible individuals used for control: 88\n",
            "Saved control haplotypes → data/processed/hapmap/haplotypes/CEU_chr2_5Mb.common_with_MKK.control_haplotypes.phased_compatible.json\n",
            "  phased-compatible individuals used for control: 88\n",
            "Saved control haplotypes → data/processed/hapmap/haplotypes/CEU_chr10_1Mb.common_with_MKK.control_haplotypes.phased_compatible.json\n",
            "Done building phased-compatible control haplotype histograms.\n",
            "Saved blocks: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/blocks/CEU_chr2_5Mb.common_with_MKK.blocks.json /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/blocks/CEU_chr10_1Mb.common_with_MKK.blocks.json\n",
            "Saved haplotypes: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/haplotypes/CEU_chr2_5Mb.common_with_MKK.control_haplotypes.phased_compatible.json /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/haplotypes/CEU_chr10_1Mb.common_with_MKK.control_haplotypes.phased_compatible.json\n",
            " chr2 → data/processed/hapmap/haplotypes/CEU_chr2_5Mb.common_with_MKK.control_haplotypes.phased_compatible.json\n",
            " chr10 → data/processed/hapmap/haplotypes/CEU_chr10_1Mb.common_with_MKK.control_haplotypes.phased_compatible.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from collections import Counter\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "def load_phased_df(phase_gz_path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(\n",
        "        phase_gz_path,\n",
        "        sep=r\"\\s+\",\n",
        "        engine=\"python\",\n",
        "        compression=\"gzip\",\n",
        "        dtype=str,\n",
        "    )\n",
        "\n",
        "\n",
        "    rs_candidates = [\"rsid\", \"rs#\", \"snp\", \"marker\"]\n",
        "    pos_candidates = [\"pos\", \"position\", \"bp\"]\n",
        "\n",
        "    lower_map = {c.lower(): c for c in df.columns}\n",
        "    rs_col = None\n",
        "    pos_col = None\n",
        "\n",
        "    for cand in rs_candidates:\n",
        "        if cand in lower_map:\n",
        "            rs_col = lower_map[cand]\n",
        "            break\n",
        "    for cand in pos_candidates:\n",
        "        if cand in lower_map:\n",
        "            pos_col = lower_map[cand]\n",
        "            break\n",
        "\n",
        "    if rs_col is None and pos_col is None:\n",
        "        raise ValueError(f\"Phased file missing rsID/pos columns. Columns: {list(df.columns)}\")\n",
        "\n",
        "    if rs_col is not None and rs_col != \"rsID\":\n",
        "        df = df.rename(columns={rs_col: \"rsID\"})\n",
        "    if pos_col is not None and pos_col != \"pos\":\n",
        "        df = df.rename(columns={pos_col: \"pos\"})\n",
        "\n",
        "    print(\n",
        "        f\"Phased columns: file={phase_gz_path.name}, rsID_col={rs_col or 'None'}, pos_col={pos_col or 'None'}, rows={len(df)}\"\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def phased_individuals_from_columns(df: pd.DataFrame) -> list:\n",
        "    hap_cols = [c for c in df.columns if c.endswith(\"_A\") or c.endswith(\"_B\")]\n",
        "    if len(hap_cols) == 0:\n",
        "        raise ValueError(\"No _A/_B haplotype columns found in phased file.\")\n",
        "    individuals = sorted(set(c[:-2] for c in hap_cols))\n",
        "    individuals = [i for i in individuals if f\"{i}_A\" in df.columns and f\"{i}_B\" in df.columns]\n",
        "    return individuals\n",
        "\n",
        "\n",
        "def make_phased_compatible_region(region, phased_df: pd.DataFrame, region_name: str, phase_name: str = \"\"):\n",
        "    region_sample_ids = np.array([str(x) for x in region[\"sample_ids\"]], dtype=object)\n",
        "    region_snp_ids = np.array([str(x) for x in region[\"snp_ids\"]], dtype=object)\n",
        "    region_positions = np.array(region[\"positions\"], dtype=int)\n",
        "\n",
        "    phased_indivs = phased_individuals_from_columns(phased_df)\n",
        "    print(f\"[{region_name}] Phased individuals: {len(phased_indivs)}\")\n",
        "\n",
        "\n",
        "    if \"rsID\" not in phased_df.columns:\n",
        "        raise ValueError(f\"[{region_name}] Phased file missing rsID column after normalization.\")\n",
        "\n",
        "    phased_rsids_set = set(phased_df[\"rsID\"].astype(str).tolist())\n",
        "    keep_snp_mask = np.array([rs in phased_rsids_set for rs in region_snp_ids], dtype=bool)\n",
        "    match_method = \"rsid\"\n",
        "\n",
        "    if int(keep_snp_mask.sum()) < 5:\n",
        "        if \"pos\" not in phased_df.columns:\n",
        "            raise ValueError(f\"[{region_name}] rsID match failed and phased file has no position column.\")\n",
        "        print(f\"[{region_name}] rsID matching found 0 SNPs; falling back to position matching.\")\n",
        "        phased_pos = pd.to_numeric(phased_df[\"pos\"], errors=\"coerce\")\n",
        "        phased_pos_set = set(phased_pos.dropna().astype(int).tolist())\n",
        "        keep_snp_mask = np.array([int(p) in phased_pos_set for p in region_positions], dtype=bool)\n",
        "        match_method = \"pos\"\n",
        "\n",
        "    snp_ids_sub = region_snp_ids[keep_snp_mask]\n",
        "    positions_sub = region_positions[keep_snp_mask]\n",
        "\n",
        "    print(f\"[{region_name}] SNPs matched: {len(snp_ids_sub)} / {len(region_snp_ids)} (method={match_method})\")\n",
        "\n",
        "    if len(snp_ids_sub) == 0:\n",
        "        phased_rsids_head = phased_df[\"rsID\"].astype(str).tolist()[:10] if \"rsID\" in phased_df.columns else []\n",
        "        phased_pos_head = (\n",
        "            pd.to_numeric(phased_df[\"pos\"], errors=\"coerce\").dropna().astype(int).tolist()[:10]\n",
        "            if \"pos\" in phased_df.columns\n",
        "            else []\n",
        "        )\n",
        "        raise RuntimeError(\n",
        "            f\"[{region_name}] No SNPs matched. Phased file={phase_name}. \"\n",
        "            f\"Region rsIDs head={region_snp_ids[:10].tolist()}, phased rsIDs head={phased_rsids_head}. \"\n",
        "            f\"Region pos head={region_positions[:10].tolist()}, phased pos head={phased_pos_head}.\"\n",
        "        )\n",
        "\n",
        "    snp_idx_in_region = np.where(keep_snp_mask)[0]\n",
        "\n",
        "\n",
        "    G = region[\"G\"]\n",
        "    if G.shape[0] == len(region_sample_ids):\n",
        "        G_sub = G[:, snp_idx_in_region]\n",
        "    elif G.shape[1] == len(region_sample_ids):\n",
        "        G_sub = G[snp_idx_in_region, :].T\n",
        "    else:\n",
        "        raise RuntimeError(f\"[{region_name}] Unexpected G shape: {G.shape}\")\n",
        "\n",
        "    sample_ids_sub = region_sample_ids\n",
        "\n",
        "\n",
        "    if match_method == \"rsid\":\n",
        "        phased_sub = phased_df[phased_df[\"rsID\"].isin(set(snp_ids_sub.tolist()))].copy()\n",
        "        order_map = {rsid: i for i, rsid in enumerate(snp_ids_sub.tolist())}\n",
        "        phased_sub[\"__order\"] = phased_sub[\"rsID\"].map(order_map)\n",
        "    else:\n",
        "        phased_pos = pd.to_numeric(phased_df[\"pos\"], errors=\"coerce\")\n",
        "        phased_sub = phased_df[phased_pos.isin(set(positions_sub.tolist()))].copy()\n",
        "        order_map = {int(pos): i for i, pos in enumerate(positions_sub.tolist())}\n",
        "        phased_sub[\"__order\"] = pd.to_numeric(phased_sub[\"pos\"], errors=\"coerce\").map(order_map)\n",
        "\n",
        "    phased_sub = phased_sub.sort_values(\"__order\").drop(columns=\"__order\")\n",
        "\n",
        "    A_cols = [f\"{i}_A\" for i in phased_indivs]\n",
        "    B_cols = [f\"{i}_B\" for i in phased_indivs]\n",
        "    alleles_A = phased_sub[A_cols].to_numpy(dtype=object)  \n",
        "    alleles_B = phased_sub[B_cols].to_numpy(dtype=object)\n",
        "\n",
        "    minor_all = region.get(\"minor_alleles\", region.get(\"counted_alleles\"))\n",
        "    minor_alleles_sub = minor_all[snp_idx_in_region] if minor_all is not None else None\n",
        "\n",
        "    print(f\"[{region_name}] Phased allele matrices: A={alleles_A.shape}, B={alleles_B.shape}\")\n",
        "\n",
        "    return {\n",
        "        \"region_name\": region_name,\n",
        "        \"G_sub\": G_sub,\n",
        "        \"sample_ids_sub\": sample_ids_sub,\n",
        "        \"snp_ids_sub\": snp_ids_sub,\n",
        "        \"positions_sub\": positions_sub,\n",
        "        \"minor_alleles_sub\": minor_alleles_sub,\n",
        "        \"alleles_A\": alleles_A,\n",
        "        \"alleles_B\": alleles_B,\n",
        "        \"phased_individuals\": phased_indivs,\n",
        "        \"snp_idx_in_region\": snp_idx_in_region,\n",
        "        \"match_method\": match_method,\n",
        "    }\n",
        "\n",
        "\n",
        "def adjacent_r2_from_G(G):\n",
        "    X = G.astype(float)\n",
        "    M, N = X.shape\n",
        "    out = np.zeros(N - 1, dtype=float)\n",
        "    for j in range(N - 1):\n",
        "        x = X[:, j]\n",
        "        y = X[:, j + 1]\n",
        "        mask = (x >= 0) & (y >= 0)\n",
        "        if mask.sum() < 10:\n",
        "            out[j] = 0.0\n",
        "            continue\n",
        "        xv = x[mask] - x[mask].mean()\n",
        "        yv = y[mask] - y[mask].mean()\n",
        "        denom = np.sqrt((xv * xv).sum() * (yv * yv).sum())\n",
        "        out[j] = 0.0 if denom == 0 else float(((xv * yv).sum() / denom) ** 2)\n",
        "    return out\n",
        "\n",
        "\n",
        "def build_blocks_from_adjacent_r2(r2, threshold=0.8, min_snps=5, max_snps=80):\n",
        "    N = len(r2) + 1\n",
        "    cuts = [0]\n",
        "    for j, v in enumerate(r2):\n",
        "        if v < threshold:\n",
        "            cuts.append(j + 1)\n",
        "    cuts.append(N)\n",
        "\n",
        "    blocks = [(cuts[i], cuts[i + 1] - 1) for i in range(len(cuts) - 1)]\n",
        "\n",
        "    merged = []\n",
        "    for s, e in blocks:\n",
        "        if not merged:\n",
        "            merged.append((s, e))\n",
        "        else:\n",
        "            if (e - s + 1) < min_snps:\n",
        "                ps, pe = merged[-1]\n",
        "                merged[-1] = (ps, e)\n",
        "            else:\n",
        "                merged.append((s, e))\n",
        "\n",
        "    final = []\n",
        "    for s, e in merged:\n",
        "        while (e - s + 1) > max_snps:\n",
        "            final.append((s, s + max_snps - 1))\n",
        "            s = s + max_snps\n",
        "        final.append((s, e))\n",
        "    return final\n",
        "\n",
        "\n",
        "def hap_strings_for_block(allele_matrix, start, end):\n",
        "    block = allele_matrix[start:end+1, :]\n",
        "    return [\"\".join(block[:, j].tolist()) for j in range(block.shape[1])]\n",
        "\n",
        "\n",
        "def compute_maf_from_G(G, idx):\n",
        "    sub = G[:, idx]\n",
        "    mask = sub >= 0\n",
        "    called = mask.sum(axis=1).astype(np.int32)\n",
        "    mac = np.where(mask, sub, 0).sum(axis=1).astype(np.float64)\n",
        "    denom = 2.0 * np.maximum(called, 1)\n",
        "    return mac / denom\n",
        "\n",
        "\n",
        "def save_control_haplotypes(region_name, blocks, pc, control_ids, top_k=50):\n",
        "    phased_ids = [str(x) for x in pc[\"phased_individuals\"]]\n",
        "    control_cols = list(range(len(phased_ids)))\n",
        "\n",
        "    print(f\"  phased-compatible individuals used for control: {len(control_cols)}\")\n",
        "\n",
        "    alleles_A = pc[\"alleles_A\"]\n",
        "    alleles_B = pc[\"alleles_B\"]\n",
        "    minor = pc[\"minor_alleles_sub\"]\n",
        "\n",
        "\n",
        "    p_ctrl_pc = compute_maf_from_G(pc[\"G_sub\"], list(range(pc[\"G_sub\"].shape[0])))\n",
        "\n",
        "    block_payload = []\n",
        "    for block_id, (s, e) in enumerate(blocks):\n",
        "        hA_all = hap_strings_for_block(alleles_A, s, e)\n",
        "        hB_all = hap_strings_for_block(alleles_B, s, e)\n",
        "\n",
        "        ctr = Counter()\n",
        "        for col in control_cols:\n",
        "            ctr[hA_all[col]] += 1\n",
        "            ctr[hB_all[col]] += 1\n",
        "\n",
        "        total = int(sum(ctr.values()))\n",
        "        top = ctr.most_common(top_k)\n",
        "        top_haps = [h for h, _ in top]\n",
        "        top_counts = [int(c) for _, c in top]\n",
        "        top_set = set(top_haps)\n",
        "        other_count = int(total - sum(top_counts))\n",
        "\n",
        "        block_size = e - s + 1\n",
        "        other_minor_counts = np.zeros(block_size, dtype=np.float64)\n",
        "        other_haps = 0\n",
        "        for col in control_cols:\n",
        "            for h in (hA_all[col], hB_all[col]):\n",
        "                if h in top_set:\n",
        "                    continue\n",
        "                other_haps += 1\n",
        "                for j, allele in enumerate(h):\n",
        "                    if allele == minor[s + j]:\n",
        "                        other_minor_counts[j] += 1.0\n",
        "\n",
        "        if other_haps > 0:\n",
        "            other_minor_frac = (other_minor_counts / other_haps).tolist()\n",
        "        else:\n",
        "            other_minor_frac = p_ctrl_pc[s : e + 1].tolist()\n",
        "\n",
        "        block_payload.append({\n",
        "            \"block_id\": int(block_id),\n",
        "            \"start_snp_index\": int(s),\n",
        "            \"end_snp_index\": int(e),\n",
        "            \"num_snps\": int(block_size),\n",
        "            \"total_haplotypes_counted\": total,\n",
        "            \"top_k\": int(top_k),\n",
        "            \"top_haplotypes\": top_haps,\n",
        "            \"top_counts\": top_counts,\n",
        "            \"other_count\": other_count,\n",
        "            \"other_minor_frac\": other_minor_frac,\n",
        "        })\n",
        "\n",
        "    out = HAP_DIR / f\"{region_name}.control_haplotypes.phased_compatible.json\"\n",
        "    payload = {\n",
        "        \"region_name\": region_name,\n",
        "        \"note\": \"Built on phased SNP intersection; control uses all phased individuals (no ID overlap required).\",\n",
        "        \"counts_from\": \"CONTROL cohort only, restricted to phased-compatible individuals\",\n",
        "        \"phased_compatible\": {\n",
        "            \"num_individuals_total\": int(len(phased_ids)),\n",
        "            \"num_control_individuals_used\": int(len(control_cols)),\n",
        "            \"num_snps_total\": int(len(pc[\"snp_ids_sub\"])),\n",
        "        },\n",
        "        \"blocks\": block_payload,\n",
        "    }\n",
        "    out.write_text(json.dumps(payload, indent=2))\n",
        "    print(f\"Saved control haplotypes → {out.relative_to(PROJECT_ROOT)}\")\n",
        "    return out\n",
        "\n",
        "\n",
        "phase_info = json.loads((PHASE_INFO_DIR / f\"{CASE_POP.lower()}_phased_paths.json\").read_text())\n",
        "PHASE_CHR2 = Path(phase_info[\"CEU\"][\"chr2\"][\"path\"])\n",
        "PHASE_CHR10 = Path(phase_info[\"CEU\"][\"chr10\"][\"path\"])\n",
        "\n",
        "for name in [\"ceu_chr2\", \"ceu_chr10\"]:\n",
        "    if name not in globals():\n",
        "        raise RuntimeError(f\"Missing `{name}`. Run the region+cohorts loading cell first.\")\n",
        "\n",
        "if \"pc2\" not in globals() or \"match_method\" not in pc2 or \"phased_individuals\" not in pc2:\n",
        "    df_phase_chr2 = load_phased_df(PHASE_CHR2)\n",
        "    pc2 = make_phased_compatible_region(ceu_chr2, df_phase_chr2, \"chr2_5Mb\", PHASE_CHR2.name)\n",
        "    print(\" Created pc2\")\n",
        "else:\n",
        "    print(\" pc2 already exists\")\n",
        "\n",
        "if \"pc10\" not in globals() or \"match_method\" not in pc10 or \"phased_individuals\" not in pc10:\n",
        "    df_phase_chr10 = load_phased_df(PHASE_CHR10)\n",
        "    pc10 = make_phased_compatible_region(ceu_chr10, df_phase_chr10, \"chr10_1Mb\", PHASE_CHR10.name)\n",
        "    print(\" Created pc10\")\n",
        "else:\n",
        "    print(\" pc10 already exists\")\n",
        "\n",
        "print(f\"[chr2_5Mb] SNPs matched={len(pc2['snp_ids_sub'])}, phased individuals={len(pc2['phased_individuals'])}, match={pc2['match_method']}\")\n",
        "print(f\"[chr10_1Mb] SNPs matched={len(pc10['snp_ids_sub'])}, phased individuals={len(pc10['phased_individuals'])}, match={pc10['match_method']}\")\n",
        "\n",
        "\n",
        "TAG_CHR2 = f\"CEU_chr2_5Mb.common_with_{CASE_POP}\"\n",
        "TAG_CHR10 = f\"CEU_chr10_1Mb.common_with_{CASE_POP}\"\n",
        "\n",
        "\n",
        "BLOCK_PARAMS = {\"threshold\": 0.8, \"min_snps\": 5, \"max_snps\": 80}\n",
        "\n",
        "print(\"Building phased-compatible blocks for chr2\")\n",
        "pc2_adj = adjacent_r2_from_G(pc2[\"G_sub\"])\n",
        "blocks_chr2 = build_blocks_from_adjacent_r2(pc2_adj, **BLOCK_PARAMS)\n",
        "print(f\" chr2 phased-compatible SNPs: {pc2['G_sub'].shape[1]} | blocks: {len(blocks_chr2)}\")\n",
        "\n",
        "print(\"Building phased-compatible blocks for chr10\")\n",
        "pc10_adj = adjacent_r2_from_G(pc10[\"G_sub\"])\n",
        "blocks_chr10 = build_blocks_from_adjacent_r2(pc10_adj, **BLOCK_PARAMS)\n",
        "print(f\" chr10 phased-compatible SNPs: {pc10['G_sub'].shape[1]} | blocks: {len(blocks_chr10)}\")\n",
        "\n",
        "\n",
        "\n",
        "def save_blocks(tag, blocks, block_params, pc):\n",
        "    out = BLOCK_DIR / f\"{tag}.blocks.json\"\n",
        "    payload = {\n",
        "        \"region_tag\": tag,\n",
        "        \"block_params\": block_params,\n",
        "        \"num_snps_phased_compatible\": int(pc[\"G_sub\"].shape[1]),\n",
        "        \"num_blocks\": int(len(blocks)),\n",
        "        \"blocks\": [\n",
        "            {\n",
        "                \"start\": int(s),\n",
        "                \"end\": int(e),\n",
        "                \"num_snps\": int(e - s + 1),\n",
        "            }\n",
        "            for (s, e) in blocks\n",
        "        ],\n",
        "        \"snp_ids_sub\": [str(x) for x in pc[\"snp_ids_sub\"]],\n",
        "        \"match_method\": pc.get(\"match_method\", \"unknown\"),\n",
        "        \"phased_source\": pc.get(\"phased_source\", \"unknown\"),\n",
        "    }\n",
        "    out.write_text(json.dumps(payload, indent=2))\n",
        "    print(f\"Saved blocks -> {out.relative_to(PROJECT_ROOT)}\")\n",
        "    return out\n",
        "\n",
        "\n",
        "blocks_chr2_out = save_blocks(TAG_CHR2, blocks_chr2, BLOCK_PARAMS, pc2)\n",
        "blocks_chr10_out = save_blocks(TAG_CHR10, blocks_chr10, BLOCK_PARAMS, pc10)\n",
        "\n",
        "hap_chr2_out = save_control_haplotypes(TAG_CHR2, blocks_chr2, pc2, None, top_k=TOP_K)\n",
        "hap_chr10_out = save_control_haplotypes(TAG_CHR10, blocks_chr10, pc10, None, top_k=TOP_K)\n",
        "\n",
        "print(\"Done building phased-compatible control haplotype histograms.\")\n",
        "print(\"Saved blocks:\", blocks_chr2_out, blocks_chr10_out)\n",
        "print(\"Saved haplotypes:\", hap_chr2_out, hap_chr10_out)\n",
        "print(\" chr2 →\", hap_chr2_out.relative_to(PROJECT_ROOT))\n",
        "print(\" chr10 →\", hap_chr10_out.relative_to(PROJECT_ROOT))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "47177952",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw genotypes:\n",
            "OK: CEU genotypes chr2: data/raw/hapmap/genotypes/genotypes_chr2_CEU_phase3.2_nr.b36_fwd.txt.gz\n",
            "OK: CEU genotypes chr10: data/raw/hapmap/genotypes/genotypes_chr10_CEU_phase3.2_nr.b36_fwd.txt.gz\n",
            "OK: MKK genotypes chr2: data/raw/hapmap/genotypes/genotypes_chr2_MKK_phase3.2_nr.b36_fwd.txt.gz\n",
            "OK: MKK genotypes chr10: data/raw/hapmap/genotypes/genotypes_chr10_MKK_phase3.2_nr.b36_fwd.txt.gz\n",
            "Aligned regions:\n",
            "OK: CEU chr2 aligned: data/processed/hapmap/regions/CEU_chr2_5Mb.common_with_MKK.npz\n",
            "OK: CEU chr10 aligned: data/processed/hapmap/regions/CEU_chr10_1Mb.common_with_MKK.npz\n",
            "OK: MKK chr2 aligned: data/processed/hapmap/regions/MKK_chr2_5Mb.common_with_CEU.npz\n",
            "OK: MKK chr10 aligned: data/processed/hapmap/regions/MKK_chr10_1Mb.common_with_CEU.npz\n",
            "Cohorts:\n",
            "OK: case_pop JSON: data/processed/hapmap/cohorts/hapmap_case_pop.json\n",
            "OK: CEU control chr2: data/processed/hapmap/cohorts/hapmap_CEU_control_chr2_5Mb.npz\n",
            "OK: CEU control chr10: data/processed/hapmap/cohorts/hapmap_CEU_control_chr10_1Mb.npz\n",
            "OK: MKK case train chr2: data/processed/hapmap/cohorts/hapmap_MKK_case_train_chr2_5Mb.npz\n",
            "OK: MKK case test chr2: data/processed/hapmap/cohorts/hapmap_MKK_case_test_chr2_5Mb.npz\n",
            "OK: MKK case train chr10: data/processed/hapmap/cohorts/hapmap_MKK_case_train_chr10_1Mb.npz\n",
            "OK: MKK case test chr10: data/processed/hapmap/cohorts/hapmap_MKK_case_test_chr10_1Mb.npz\n",
            "Blocks + haplotypes:\n",
            "OK: Blocks chr2: data/processed/hapmap/blocks/CEU_chr2_5Mb.common_with_MKK.blocks.json\n",
            "OK: Blocks chr10: data/processed/hapmap/blocks/CEU_chr10_1Mb.common_with_MKK.blocks.json\n",
            "OK: Haplotypes chr2: data/processed/hapmap/haplotypes/CEU_chr2_5Mb.common_with_MKK.control_haplotypes.phased_compatible.json\n",
            "OK: Haplotypes chr10: data/processed/hapmap/haplotypes/CEU_chr10_1Mb.common_with_MKK.control_haplotypes.phased_compatible.json\n",
            "Phased paths JSON:\n",
            "OK: MKK phased paths: data/processed/hapmap/phasing/mkk_phased_paths.json\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Final file existence check\n",
        "# -----------------------------\n",
        "\n",
        "def _check(path: Path, label: str):\n",
        "    ok = path.exists()\n",
        "    status = \"OK:\" if ok else \"PROBLEM:\"\n",
        "    print(f\"{status} {label}: {path.relative_to(PROJECT_ROOT)}\")\n",
        "\n",
        "print(\"Raw genotypes:\")\n",
        "_check(GENO_DIR / f\"genotypes_chr2_CEU_phase3.2_nr.b36_fwd.txt.gz\", \"CEU genotypes chr2\")\n",
        "_check(GENO_DIR / f\"genotypes_chr10_CEU_phase3.2_nr.b36_fwd.txt.gz\", \"CEU genotypes chr10\")\n",
        "_check(GENO_DIR / f\"genotypes_chr2_{CASE_POP}_phase3.2_nr.b36_fwd.txt.gz\", f\"{CASE_POP} genotypes chr2\")\n",
        "_check(GENO_DIR / f\"genotypes_chr10_{CASE_POP}_phase3.2_nr.b36_fwd.txt.gz\", f\"{CASE_POP} genotypes chr10\")\n",
        "\n",
        "print(\"Aligned regions:\")\n",
        "_check(ALIGNED_CEU_CHR2, \"CEU chr2 aligned\")\n",
        "_check(ALIGNED_CEU_CHR10, \"CEU chr10 aligned\")\n",
        "_check(ALIGNED_CASE_CHR2, f\"{CASE_POP} chr2 aligned\")\n",
        "_check(ALIGNED_CASE_CHR10, f\"{CASE_POP} chr10 aligned\")\n",
        "\n",
        "print(\"Cohorts:\")\n",
        "_check(COHORT_DIR / \"hapmap_case_pop.json\", \"case_pop JSON\")\n",
        "_check(COHORT_DIR / \"hapmap_CEU_control_chr2_5Mb.npz\", \"CEU control chr2\")\n",
        "_check(COHORT_DIR / \"hapmap_CEU_control_chr10_1Mb.npz\", \"CEU control chr10\")\n",
        "_check(COHORT_DIR / f\"hapmap_{CASE_POP}_case_train_chr2_5Mb.npz\", f\"{CASE_POP} case train chr2\")\n",
        "_check(COHORT_DIR / f\"hapmap_{CASE_POP}_case_test_chr2_5Mb.npz\", f\"{CASE_POP} case test chr2\")\n",
        "_check(COHORT_DIR / f\"hapmap_{CASE_POP}_case_train_chr10_1Mb.npz\", f\"{CASE_POP} case train chr10\")\n",
        "_check(COHORT_DIR / f\"hapmap_{CASE_POP}_case_test_chr10_1Mb.npz\", f\"{CASE_POP} case test chr10\")\n",
        "\n",
        "print(\"Blocks + haplotypes:\")\n",
        "_check(BLOCK_DIR / f\"CEU_chr2_5Mb.common_with_{CASE_POP}.blocks.json\", \"Blocks chr2\")\n",
        "_check(BLOCK_DIR / f\"CEU_chr10_1Mb.common_with_{CASE_POP}.blocks.json\", \"Blocks chr10\")\n",
        "_check(HAP_DIR / f\"CEU_chr2_5Mb.common_with_{CASE_POP}.control_haplotypes.phased_compatible.json\", \"Haplotypes chr2\")\n",
        "_check(HAP_DIR / f\"CEU_chr10_1Mb.common_with_{CASE_POP}.control_haplotypes.phased_compatible.json\", \"Haplotypes chr10\")\n",
        "\n",
        "print(\"Phased paths JSON:\")\n",
        "_check(PHASE_INFO_DIR / f\"{CASE_POP.lower()}_phased_paths.json\", f\"{CASE_POP} phased paths\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
