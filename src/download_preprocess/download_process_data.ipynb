{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f722a496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PROJECT_ROOT = /Users/erkmenerken/Desktop/proje430\n",
      "Directories:\n",
      "  DATA_DIR      : /Users/erkmenerken/Desktop/proje430/data\n",
      "  RAW_HAPMAP_DIR : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap\n",
      "  GENO_DIR      : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/genotypes\n",
      "  PHASE_DIR     : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED\n",
      "  PHASE_YRI_DIR : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/YRI/UNRELATED\n",
      "  PHASE_META_DIR: /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2_meta\n",
      "  PROC_DIR      : /Users/erkmenerken/Desktop/proje430/data/processed/hapmap\n",
      "Inputs:\n",
      "  geno_chr2  : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/genotypes/genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz | exists: True\n",
      "  geno_chr10 : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/genotypes/genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz | exists: True\n",
      "  PHASE_CHR2 : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz | exists: True\n",
      "  PHASE_CHR10: /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz | exists: True\n",
      "  PHASE_YRI_CHR2 : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/YRI/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_yri.unr.phased.gz | exists: True\n",
      "  PHASE_YRI_CHR10: /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/YRI/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_yri.unr.phased.gz | exists: True\n",
      "  PHASE_INFO : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2_meta/hapmap3_r2_b36_fwd.consensus.qc.poly.info | exists: True\n",
      "Processed outputs:\n",
      "  REGION_CHR2 : /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/regions/CEU_chr2_5Mb.common_with_YRI.npz | exists: True\n",
      "  REGION_CHR10: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/regions/CEU_chr10_1Mb.common_with_YRI.npz | exists: True\n",
      "  COHORTS_JSON: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/cohorts/hapmap_CEU_control_test__YRI_case.json | exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Robust repo root detection\n",
    "# -----------------------------\n",
    "# PROJECT_ROOT = nearest parent directory containing requirements.txt OR .git\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"requirements.txt\").exists() and not (PROJECT_ROOT / \".git\").exists():\n",
    "    for parent in PROJECT_ROOT.parents:\n",
    "        if (parent / \"requirements.txt\").exists() or (parent / \".git\").exists():\n",
    "            PROJECT_ROOT = parent\n",
    "            break\n",
    "\n",
    "print(\" PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "# -----------------------------\n",
    "# Canonical project directories (ALWAYS under repo-root/data)\n",
    "# -----------------------------\n",
    "DATA_DIR       = PROJECT_ROOT / \"data\"\n",
    "RAW_HAPMAP_DIR = DATA_DIR / \"raw\" / \"hapmap\"\n",
    "\n",
    "GENO_DIR       = RAW_HAPMAP_DIR / \"genotypes\"\n",
    "PHASE_DIR      = RAW_HAPMAP_DIR / \"phasing\" / \"HapMap3_r2\" / \"CEU\" / \"UNRELATED\"\n",
    "PHASE_YRI_DIR  = RAW_HAPMAP_DIR / \"phasing\" / \"HapMap3_r2\" / \"YRI\" / \"UNRELATED\"\n",
    "PHASE_META_DIR = RAW_HAPMAP_DIR / \"phasing\" / \"HapMap3_r2_meta\"\n",
    "\n",
    "PROC_DIR       = DATA_DIR / \"processed\" / \"hapmap\"\n",
    "REGION_DIR     = PROC_DIR / \"regions\"\n",
    "COHORT_DIR     = PROC_DIR / \"cohorts\"\n",
    "BLOCK_OUT_DIR  = PROC_DIR / \"blocks\"\n",
    "HAP_OUT_DIR    = PROC_DIR / \"haplotypes\"\n",
    "\n",
    "# Create folders if missing (safe; does not redo downloads)\n",
    "for d in [\n",
    "    DATA_DIR, RAW_HAPMAP_DIR,\n",
    "    GENO_DIR, PHASE_DIR, PHASE_YRI_DIR, PHASE_META_DIR,\n",
    "    PROC_DIR, REGION_DIR, COHORT_DIR, BLOCK_OUT_DIR, HAP_OUT_DIR\n",
    "]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Expected file locations (repo-root/data/...)\n",
    "# -----------------------------\n",
    "# Genotype files (unphased)\n",
    "geno_chr2  = GENO_DIR / \"genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\"\n",
    "geno_chr10 = GENO_DIR / \"genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\"\n",
    "\n",
    "# Phased haplotype files (UNRELATED)\n",
    "PHASE_CHR2  = PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\"\n",
    "PHASE_CHR10 = PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\"\n",
    "\n",
    "PHASE_YRI_CHR2  = PHASE_YRI_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_yri.unr.phased.gz\"\n",
    "PHASE_YRI_CHR10 = PHASE_YRI_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_yri.unr.phased.gz\"\n",
    "\n",
    "# Meta info file\n",
    "PHASE_INFO = PHASE_META_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.info\"\n",
    "\n",
    "# Processed outputs\n",
    "REGION_CHR2  = REGION_DIR / \"CEU_chr2_5Mb.common_with_YRI.npz\"\n",
    "REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.common_with_YRI.npz\"\n",
    "COHORTS_JSON = COHORT_DIR / \"hapmap_CEU_control_test__YRI_case.json\"\n",
    "\n",
    "# -----------------------------\n",
    "# Print debug summary\n",
    "# -----------------------------\n",
    "print(\"Directories:\")\n",
    "print(\"  DATA_DIR      :\", DATA_DIR)\n",
    "print(\"  RAW_HAPMAP_DIR :\", RAW_HAPMAP_DIR)\n",
    "print(\"  GENO_DIR      :\", GENO_DIR)\n",
    "print(\"  PHASE_DIR     :\", PHASE_DIR)\n",
    "print(\"  PHASE_YRI_DIR :\", PHASE_YRI_DIR)\n",
    "print(\"  PHASE_META_DIR:\", PHASE_META_DIR)\n",
    "print(\"  PROC_DIR      :\", PROC_DIR)\n",
    "\n",
    "print(\"Inputs:\")\n",
    "print(\"  geno_chr2  :\", geno_chr2,  \"| exists:\", geno_chr2.exists())\n",
    "print(\"  geno_chr10 :\", geno_chr10, \"| exists:\", geno_chr10.exists())\n",
    "print(\"  PHASE_CHR2 :\", PHASE_CHR2, \"| exists:\", PHASE_CHR2.exists())\n",
    "print(\"  PHASE_CHR10:\", PHASE_CHR10,\"| exists:\", PHASE_CHR10.exists())\n",
    "print(\"  PHASE_YRI_CHR2 :\", PHASE_YRI_CHR2, \"| exists:\", PHASE_YRI_CHR2.exists())\n",
    "print(\"  PHASE_YRI_CHR10:\", PHASE_YRI_CHR10,\"| exists:\", PHASE_YRI_CHR10.exists())\n",
    "print(\"  PHASE_INFO :\", PHASE_INFO, \"| exists:\", PHASE_INFO.exists())\n",
    "\n",
    "print(\"Processed outputs:\")\n",
    "print(\"  REGION_CHR2 :\", REGION_CHR2, \"| exists:\", REGION_CHR2.exists())\n",
    "print(\"  REGION_CHR10:\", REGION_CHR10,\"| exists:\", REGION_CHR10.exists())\n",
    "print(\"  COHORTS_JSON:\", COHORTS_JSON,\"| exists:\", COHORTS_JSON.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f32ffb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Manifest entries: 7\n",
      "- CEU genotypes chr2 (unphased) -> data/raw/hapmap/genotypes/genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\n",
      "- CEU genotypes chr10 (unphased) -> data/raw/hapmap/genotypes/genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\n",
      "- YRI genotypes chr2 (unphased) -> data/raw/hapmap/genotypes/genotypes_chr2_YRI_r27_nr.b36_fwd.txt.gz\n",
      "- YRI genotypes chr10 (unphased) -> data/raw/hapmap/genotypes/genotypes_chr10_YRI_r27_nr.b36_fwd.txt.gz\n",
      "- CEU phased haplotypes chr2 (UNRELATED) -> data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      "- CEU phased haplotypes chr10 (UNRELATED) -> data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n",
      "- HapMap3 r2 SNP meta (.info) -> data/raw/hapmap/phasing/HapMap3_r2_meta/hapmap3_r2_b36_fwd.consensus.qc.poly.info\n"
     ]
    }
   ],
   "source": [
    "# Sources used:\n",
    "# - Genotypes (CEU/YRI, non-redundant, b36, forward): chr2 + chr10\n",
    "# - Phased haplotypes (HapMap3 r2, CEU, UNRELATED): chr2 + chr10\n",
    "# - Small meta file in HapMap3 r2 directory (.info)\n",
    "\n",
    "# Idempotent downloads\n",
    "FORCE_REDOWNLOAD = False\n",
    "\n",
    "MANIFEST = [\n",
    "    {\n",
    "        \"name\": \"CEU genotypes chr2 (unphased)\",\n",
    "        \"url\": \"https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/latest_phaseII+III_ncbi_b36/forward/non-redundant/genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\",\n",
    "        \"dst\": GENO_DIR / \"genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CEU genotypes chr10 (unphased)\",\n",
    "        \"url\": \"https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/latest_phaseII+III_ncbi_b36/forward/non-redundant/genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\",\n",
    "        \"dst\": GENO_DIR / \"genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"YRI genotypes chr2 (unphased)\",\n",
    "        \"url\": \"https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/latest_phaseII+III_ncbi_b36/forward/non-redundant/genotypes_chr2_YRI_r27_nr.b36_fwd.txt.gz\",\n",
    "        \"dst\": GENO_DIR / \"genotypes_chr2_YRI_r27_nr.b36_fwd.txt.gz\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"YRI genotypes chr10 (unphased)\",\n",
    "        \"url\": \"https://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/latest_phaseII+III_ncbi_b36/forward/non-redundant/genotypes_chr10_YRI_r27_nr.b36_fwd.txt.gz\",\n",
    "        \"dst\": GENO_DIR / \"genotypes_chr10_YRI_r27_nr.b36_fwd.txt.gz\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CEU phased haplotypes chr2 (UNRELATED)\",\n",
    "        \"url\": \"https://ftp.ncbi.nlm.nih.gov/hapmap/phasing/2009-02_phaseIII/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\",\n",
    "        \"dst\": PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CEU phased haplotypes chr10 (UNRELATED)\",\n",
    "        \"url\": \"https://ftp.ncbi.nlm.nih.gov/hapmap/phasing/2009-02_phaseIII/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\",\n",
    "        \"dst\": PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"HapMap3 r2 SNP meta (.info)\",\n",
    "        \"url\": \"https://ftp.ncbi.nlm.nih.gov/hapmap/phasing/2009-02_phaseIII/HapMap3_r2/hapmap3_r2_b36_fwd.consensus.qc.poly.info\",\n",
    "        \"dst\": PHASE_META_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.info\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\" Manifest entries: {len(MANIFEST)}\")\n",
    "for x in MANIFEST:\n",
    "    print(\"-\", x[\"name\"], \"->\", x[\"dst\"].relative_to(PROJECT_ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87437182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "def _looks_like_html(text: str) -> bool:\n",
    "    t = text.strip().lower()\n",
    "    return (\"<html\" in t) or (t.startswith(\"<!doctype\")) or (\"not found\" in t)\n",
    "\n",
    "\n",
    "def validate_download(path: Path) -> None:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file after download: {path}\")\n",
    "\n",
    "    size_bytes = path.stat().st_size\n",
    "    if path.suffix == \".gz\":\n",
    "        if size_bytes < 1024:\n",
    "            raise RuntimeError(f\"File too small to be valid gzip: {path} ({size_bytes} bytes)\")\n",
    "\n",
    "        with open(path, \"rb\") as f:\n",
    "            head = f.read(2)\n",
    "\n",
    "        # gzip magic bytes = 1f 8b\n",
    "        if head != b\"\\x1f\\x8b\":\n",
    "            raise RuntimeError(f\"Not a valid gzip file (bad magic): {path}\")\n",
    "\n",
    "        try:\n",
    "            with gzip.open(path, \"rt\", errors=\"ignore\") as f:\n",
    "                line = f.readline(200)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to read gzip file: {path} ({e})\")\n",
    "\n",
    "        if _looks_like_html(line):\n",
    "            raise RuntimeError(f\"Downloaded gzip appears to be HTML/404: {path}\")\n",
    "\n",
    "    else:\n",
    "        if size_bytes < 100:\n",
    "            raise RuntimeError(f\"File too small to be valid: {path} ({size_bytes} bytes)\")\n",
    "\n",
    "        with open(path, \"rb\") as f:\n",
    "            head = f.read(200).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        if _looks_like_html(head):\n",
    "            raise RuntimeError(f\"Downloaded file appears to be HTML/404: {path}\")\n",
    "\n",
    "def download_file(url: str, dst: Path, overwrite: bool = False, chunk_size: int = 1024 * 1024) -> None:\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if dst.exists() and not overwrite:\n",
    "        size_bytes = dst.stat().st_size\n",
    "        if size_bytes > 0:\n",
    "            print(f\"  Already exists, skipping: {dst.name} ({size_bytes/1e6:.2f} MB)\")\n",
    "            validate_download(dst)\n",
    "            return\n",
    "        print(f\"  Existing file is empty, re-downloading: {dst.name}\")\n",
    "\n",
    "    tmp = dst.with_suffix(dst.suffix + \".part\")\n",
    "\n",
    "    print(f\"  Downloading: {url}\")\n",
    "    print(f\" Saving to  : {dst}\")\n",
    "\n",
    "    req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    with urllib.request.urlopen(req) as resp:\n",
    "        total = resp.headers.get(\"Content-Length\")\n",
    "        total = int(total) if total is not None else None\n",
    "\n",
    "        with open(tmp, \"wb\") as f, tqdm(\n",
    "            total=total, unit=\"B\", unit_scale=True, unit_divisor=1024, desc=dst.name\n",
    "        ) as pbar:\n",
    "            while True:\n",
    "                chunk = resp.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "\n",
    "    os.replace(tmp, dst)  # atomic move\n",
    "    validate_download(dst)\n",
    "    print(f\" Done: {dst.name} ({dst.stat().st_size/1e6:.2f} MB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10b8402a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting HapMap downloads...\n",
      "=== CEU genotypes chr2 (unphased) ===\n",
      "  Already exists, skipping: genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz (21.10 MB)\n",
      "=== CEU genotypes chr10 (unphased) ===\n",
      "  Already exists, skipping: genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz (13.63 MB)\n",
      "=== YRI genotypes chr2 (unphased) ===\n",
      "  Already exists, skipping: genotypes_chr2_YRI_r27_nr.b36_fwd.txt.gz (21.43 MB)\n",
      "=== YRI genotypes chr10 (unphased) ===\n",
      "  Already exists, skipping: genotypes_chr10_YRI_r27_nr.b36_fwd.txt.gz (13.69 MB)\n",
      "=== CEU phased haplotypes chr2 (UNRELATED) ===\n",
      "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz (1.80 MB)\n",
      "=== CEU phased haplotypes chr10 (UNRELATED) ===\n",
      "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz (1.13 MB)\n",
      "=== HapMap3 r2 SNP meta (.info) ===\n",
      "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.info (0.03 MB)\n",
      " All requested HapMap files are present in data/raw/hapmap/.\n"
     ]
    }
   ],
   "source": [
    "print(\" Starting HapMap downloads...\")\n",
    "\n",
    "for item in MANIFEST:\n",
    "    print(f\"=== {item['name']} ===\")\n",
    "    download_file(item[\"url\"], item[\"dst\"], overwrite=FORCE_REDOWNLOAD)\n",
    "\n",
    "print(\" All requested HapMap files are present in data/raw/hapmap/.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1537923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloaded files:\n",
      "data/raw/hapmap/genotypes/genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz - 21.10 MB\n",
      "data/raw/hapmap/genotypes/genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz - 13.63 MB\n",
      "data/raw/hapmap/genotypes/genotypes_chr2_YRI_r27_nr.b36_fwd.txt.gz - 21.43 MB\n",
      "data/raw/hapmap/genotypes/genotypes_chr10_YRI_r27_nr.b36_fwd.txt.gz - 13.69 MB\n",
      "data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz - 1.80 MB\n",
      "data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz - 1.13 MB\n",
      "data/raw/hapmap/phasing/HapMap3_r2_meta/hapmap3_r2_b36_fwd.consensus.qc.poly.info - 0.03 MB\n"
     ]
    }
   ],
   "source": [
    "def human_mb(n_bytes: int) -> str:\n",
    "    return f\"{n_bytes/1e6:.2f} MB\"\n",
    "\n",
    "print(\" Downloaded files:\")\n",
    "for item in MANIFEST:\n",
    "    p = item[\"dst\"]\n",
    "    if p.exists():\n",
    "        print(p.relative_to(PROJECT_ROOT), \"-\", human_mb(p.stat().st_size))\n",
    "    else:\n",
    "        print(\"MISSING:\", p.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72ce38f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading YRI chr2 from /hapmap/phasing/2009-02_phaseIII/HapMap3_r2/YRI/UNRELATED -> /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/YRI/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_yri.unr.phased.gz\n",
      "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_yri.unr.phased.gz (1.53 MB)\n",
      "hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_yri.unr.phased.gz: 1.53 MB | header cols=20 | individuals=9\n",
      "[WARN] YRI chr2 individuals=9 < 30; trying next directory.\n",
      "Downloading YRI chr2 from /hapmap/phasing/2009-02_phaseIII/HapMap3_r2/YRI/TRIOS -> /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/YRI/TRIOS/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_yri.phased.gz\n",
      "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_yri.phased.gz (5.41 MB)\n",
      "hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_yri.phased.gz: 5.41 MB | header cols=202 | individuals=100\n",
      "Downloading YRI chr10 from /hapmap/phasing/2009-02_phaseIII/HapMap3_r2/YRI/UNRELATED -> /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/YRI/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_yri.unr.phased.gz\n",
      "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_yri.unr.phased.gz (0.94 MB)\n",
      "hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_yri.unr.phased.gz: 0.94 MB | header cols=20 | individuals=9\n",
      "[WARN] YRI chr10 individuals=9 < 30; trying next directory.\n",
      "Downloading YRI chr10 from /hapmap/phasing/2009-02_phaseIII/HapMap3_r2/YRI/TRIOS -> /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/YRI/TRIOS/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_yri.phased.gz\n",
      "  Already exists, skipping: hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_yri.phased.gz (3.36 MB)\n",
      "hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_yri.phased.gz: 3.36 MB | header cols=202 | individuals=100\n",
      "Saved YRI phased path info: data/processed/hapmap/phasing/yri_phased_paths.json\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Resolve + download YRI phased files using FTP directory listing\n",
    "# -----------------------------\n",
    "import ftplib\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "FTP_HOST = \"ftp.ncbi.nlm.nih.gov\"\n",
    "BASE_DIR = \"/hapmap/phasing/2009-02_phaseIII/HapMap3_r2\"\n",
    "\n",
    "PHASE_YRI_DIR = RAW_HAPMAP_DIR / \"phasing\" / \"HapMap3_r2\" / \"YRI\"\n",
    "PHASE_YRI_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MIN_YRI_PHASED = 30\n",
    "\n",
    "\n",
    "def ftp_list_files(dirpath):\n",
    "    files = []\n",
    "    with ftplib.FTP(FTP_HOST) as ftp:\n",
    "        ftp.login()\n",
    "        ftp.cwd(dirpath)\n",
    "        try:\n",
    "            for name, facts in ftp.mlsd():\n",
    "                if facts.get(\"type\") == \"file\":\n",
    "                    size = facts.get(\"size\")\n",
    "                    size = int(size) if size is not None else None\n",
    "                    files.append((name, size))\n",
    "        except Exception:\n",
    "            def parse_line(line):\n",
    "                parts = line.split()\n",
    "                if len(parts) < 9:\n",
    "                    return\n",
    "                name = parts[-1]\n",
    "                try:\n",
    "                    size = int(parts[4])\n",
    "                except Exception:\n",
    "                    size = None\n",
    "                files.append((name, size))\n",
    "            ftp.retrlines(\"LIST\", parse_line)\n",
    "    return files\n",
    "\n",
    "\n",
    "def pick_phased_file(pop: str, chrom: int, dirpath: str):\n",
    "    files = ftp_list_files(dirpath)\n",
    "    matches = [f for f in files if f[0].endswith(\".phased.gz\") and f\"chr{chrom}\" in f[0]]\n",
    "    if not matches:\n",
    "        return None\n",
    "    preferred = [m for m in matches if \"qc.poly\" in m[0]]\n",
    "    if preferred:\n",
    "        matches = preferred\n",
    "    matches.sort(key=lambda x: (x[1] is None, -(x[1] or 0)))\n",
    "    return matches[0]\n",
    "\n",
    "\n",
    "def phased_header_info(path: Path):\n",
    "    with gzip.open(path, \"rt\") as f:\n",
    "        header = f.readline().strip().split()\n",
    "    n_cols = len(header)\n",
    "    cols = header[2:]\n",
    "    ids = sorted(set(c[:-2] for c in cols if c.endswith(\"_A\") or c.endswith(\"_B\")))\n",
    "    return n_cols, len(ids)\n",
    "\n",
    "\n",
    "def resolve_phased_for_chrom(pop: str, chrom: int, min_ind: int = 30):\n",
    "    dirs = [\n",
    "        f\"{BASE_DIR}/{pop}/UNRELATED\",\n",
    "        f\"{BASE_DIR}/{pop}/TRIOS\",\n",
    "        f\"{BASE_DIR}/{pop}\",\n",
    "    ]\n",
    "    last_err = None\n",
    "\n",
    "    for d in dirs:\n",
    "        try:\n",
    "            pick = pick_phased_file(pop, chrom, d)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] FTP list failed {d}: {e}\")\n",
    "            last_err = e\n",
    "            continue\n",
    "\n",
    "        if not pick:\n",
    "            print(f\"[INFO] No chr{chrom} .phased.gz in {d}\")\n",
    "            continue\n",
    "\n",
    "        name, size = pick\n",
    "        url = f\"https://{FTP_HOST}{d}/{name}\"\n",
    "        local_subdir = Path(d).name\n",
    "        if local_subdir == pop:\n",
    "            local_subdir = \"ALL\"\n",
    "        dst_dir = PHASE_YRI_DIR / local_subdir\n",
    "        dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "        dst = dst_dir / name\n",
    "\n",
    "        print(f\"Downloading {pop} chr{chrom} from {d} -> {dst}\")\n",
    "        download_file(url, dst, overwrite=FORCE_REDOWNLOAD)\n",
    "\n",
    "        size_mb = dst.stat().st_size / 1e6\n",
    "        n_cols, n_ind = phased_header_info(dst)\n",
    "        print(f\"{dst.name}: {size_mb:.2f} MB | header cols={n_cols} | individuals={n_ind}\")\n",
    "\n",
    "        if n_ind >= min_ind:\n",
    "            return {\n",
    "                \"ftp_dir\": d,\n",
    "                \"filename\": name,\n",
    "                \"local_path\": str(dst),\n",
    "                \"size_mb\": float(size_mb),\n",
    "                \"header_cols\": int(n_cols),\n",
    "                \"n_individuals\": int(n_ind),\n",
    "            }\n",
    "\n",
    "        print(f\"[WARN] {pop} chr{chrom} individuals={n_ind} < {min_ind}; trying next directory.\")\n",
    "\n",
    "    raise RuntimeError(f\"Could not find suitable {pop} chr{chrom} phased file (min_ind={min_ind}). Last error: {last_err}\")\n",
    "\n",
    "\n",
    "yri_chr2_info = resolve_phased_for_chrom(\"YRI\", 2, min_ind=MIN_YRI_PHASED)\n",
    "yri_chr10_info = resolve_phased_for_chrom(\"YRI\", 10, min_ind=MIN_YRI_PHASED)\n",
    "\n",
    "PHASE_INFO_DIR = PROC_DIR / \"phasing\"\n",
    "PHASE_INFO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "YRI_PHASED_JSON = PHASE_INFO_DIR / \"yri_phased_paths.json\"\n",
    "YRI_PHASED_JSON.write_text(json.dumps({\n",
    "    \"chr2\": yri_chr2_info,\n",
    "    \"chr10\": yri_chr10_info,\n",
    "}, indent=2))\n",
    "\n",
    "print(\"Saved YRI phased path info:\", YRI_PHASED_JSON.relative_to(PROJECT_ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80fdf0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Project root: /Users/erkmenerken/Desktop/proje430\n",
      " Raw HapMap  : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap\n",
      " Genotypes   : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/genotypes\n",
      " Phasing     : /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED\n",
      " Processed   : /Users/erkmenerken/Desktop/proje430/data/processed/hapmap\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(name=\"430_project\") -> Path:\n",
    "    cwd = Path.cwd()\n",
    "    if cwd.name == name:\n",
    "        return cwd\n",
    "    for p in [cwd] + list(cwd.parents):\n",
    "        if p.name == name:\n",
    "            return p\n",
    "    raise RuntimeError(f\"Could not find project root folder named '{name}' from {cwd}\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(\"proje430\")\n",
    "\n",
    "RAW_HAPMAP = PROJECT_ROOT / \"data\" / \"raw\" / \"hapmap\"\n",
    "GENO_DIR   = RAW_HAPMAP / \"genotypes\"\n",
    "PHASE_DIR  = RAW_HAPMAP / \"phasing\" / \"HapMap3_r2\" / \"CEU\" / \"UNRELATED\"\n",
    "PHASE_META_DIR = RAW_HAPMAP / \"phasing\" / \"HapMap3_r2_meta\"\n",
    "PHASE_META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROC_DIR   = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\"\n",
    "REGION_DIR = PROC_DIR / \"regions\"\n",
    "BLOCK_DIR  = PROC_DIR / \"blocks\"\n",
    "COHORT_DIR = PROC_DIR / \"cohorts\"\n",
    "\n",
    "for d in [PROC_DIR, REGION_DIR, BLOCK_DIR, COHORT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\" Project root:\", PROJECT_ROOT)\n",
    "print(\" Raw HapMap  :\", RAW_HAPMAP)\n",
    "print(\" Genotypes   :\", GENO_DIR)\n",
    "print(\" Phasing     :\", PHASE_DIR)\n",
    "print(\" Processed   :\", PROC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7bd40fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Peeking: genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\n",
      "  Line 1: rs# alleles chrom pos strand assembly# center protLSID assayLSID panelLSID QCcode NA06984 NA06985 NA06986 NA06989 NA06991 NA06993 NA06994 NA06995 NA06997 NA07000 NA07014 NA07019 NA07022 NA07029 NA07031 NA07034 NA07037 NA\n",
      "  Line 2: rs10171150 A/G chr2 2091 + ncbi_b36 mcgill-gqic urn:LSID:illumina.hapmap.org:Protocol:Golden_Gate_1.0.0:1 urn:LSID:mcgill-gqic.hapmap.org:Assay:810448:1 urn:lsid:dcc.hapmap.org:Panel:CEPH-30-trios:1 QC+ NN GG NN NN GG GG\n",
      "\n",
      "ðŸ”Ž Peeking: genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\n",
      "  Line 1: rs# alleles chrom pos strand assembly# center protLSID assayLSID panelLSID QCcode NA06984 NA06985 NA06986 NA06989 NA06991 NA06993 NA06994 NA06995 NA06997 NA07000 NA07014 NA07019 NA07022 NA07029 NA07031 NA07034 NA07037 NA\n",
      "  Line 2: rs11511647 C/T chr10 62765 + ncbi_b36 sanger urn:lsid:illumina.hapmap.org:Protocol:Golden_Gate_1.0.0:1 urn:lsid:sanger.hapmap.org:Assay:4310385:1 urn:lsid:dcc.hapmap.org:Panel:CEPH-30-trios:1 QC+ NN CT NN NN CT CT CT NN\n",
      "\n",
      "ðŸ”Ž Peeking: hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      "  Line 1: rsID position_b36 NA06989_A NA06989_B NA10850_A NA10850_B NA06984_A NA06984_B NA11917_A NA11917_B NA12282_A NA12282_B NA12283_A NA12283_B NA11918_A NA11918_B NA12413_A NA12413_B NA07056_A NA07056_B NA12044_A NA12044_B NA\n",
      "  Line 2: rs10181821 5703 A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A\n",
      "\n",
      "ðŸ”Ž Peeking: hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n",
      "  Line 1: rsID position_b36 NA06989_A NA06989_B NA10850_A NA10850_B NA06984_A NA06984_B NA11917_A NA11917_B NA12282_A NA12282_B NA12283_A NA12283_B NA11918_A NA11918_B NA12413_A NA12413_B NA07056_A NA07056_B NA12044_A NA12044_B NA\n",
      "  Line 2: rs12255619 88481 A C A A A A A A A A A C A A A A A A A A A A A A A A A A A A A A A A\n",
      "\n",
      " If the header looks like: rs# alleles chrom pos ... QCcode NA06984 ... then weâ€™re good.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "geno_chr2  = GENO_DIR / \"genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\"\n",
    "geno_chr10 = GENO_DIR / \"genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\"\n",
    "phase_chr2  = PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\"\n",
    "phase_chr10 = PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\"\n",
    "\n",
    "def peek_gz(path, n_lines=2, maxchars=220):\n",
    "    print(f\"\\nðŸ”Ž Peeking: {path.name}\")\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for i in range(n_lines):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            print(f\"  Line {i+1}: {line[:maxchars].rstrip()}\")\n",
    "\n",
    "for p in [geno_chr2, geno_chr10, phase_chr2, phase_chr10]:\n",
    "    if p.exists():\n",
    "        peek_gz(p, n_lines=2)\n",
    "    else:\n",
    "        print(\" Missing:\", p)\n",
    "\n",
    "print(\"\\n If the header looks like: rs# alleles chrom pos ... QCcode NA06984 ... then weâ€™re good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bbbc1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reading rs# + pos from genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz (whitespace-separated)...\n",
      " Loaded 329831 SNP positions.\n",
      " Reading rs# + pos from genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz (whitespace-separated)...\n",
      " Loaded 211711 SNP positions.\n",
      "\n",
      " Proposal-style windows selected:\n",
      "chr2 (5Mb, ~311 SNPs): {'start_bp': 242050505, 'end_bp': 247050505, 'snps_in_window': 311, 'end_pos_actual': 242742878, 'total_snps_chr': 329831, 'min_pos': 2091, 'max_pos': 242742878}\n",
      "chr10 (1Mb, ~610 SNPs): {'start_bp': 38322261, 'end_bp': 39322261, 'snps_in_window': 610, 'end_pos_actual': 39194226, 'total_snps_chr': 211711, 'min_pos': 62765, 'max_pos': 135373179}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_positions_rsids(geno_path):\n",
    "    print(f\" Reading rs# + pos from {geno_path.name} (whitespace-separated)...\")\n",
    "    df = pd.read_csv(\n",
    "        geno_path,\n",
    "        sep=r\"\\s+\",\n",
    "        engine=\"python\",\n",
    "        compression=\"gzip\",\n",
    "        usecols=[\"rs#\", \"pos\"],\n",
    "        dtype={\"rs#\": str, \"pos\": int},\n",
    "    )\n",
    "    df = df.dropna().sort_values(\"pos\").reset_index(drop=True)\n",
    "    print(f\" Loaded {len(df)} SNP positions.\")\n",
    "    return df\n",
    "\n",
    "def choose_window_by_bp(df_pos, window_bp, target_snps):\n",
    "    pos = df_pos[\"pos\"].to_numpy(np.int64)\n",
    "    n = len(pos)\n",
    "    ends = np.searchsorted(pos, pos + window_bp, side=\"right\")\n",
    "    counts = ends - np.arange(n)\n",
    "\n",
    "    diff = np.abs(counts - target_snps)\n",
    "    best_i = int(np.argmin(diff))\n",
    "\n",
    "    start = int(pos[best_i])\n",
    "    end = int(start + window_bp)\n",
    "    count = int(counts[best_i])\n",
    "    end_idx = int(ends[best_i] - 1)\n",
    "    end_pos_actual = int(pos[end_idx]) if end_idx >= best_i else start\n",
    "\n",
    "    return {\n",
    "        \"start_bp\": start,\n",
    "        \"end_bp\": end,\n",
    "        \"snps_in_window\": count,\n",
    "        \"end_pos_actual\": end_pos_actual,\n",
    "        \"total_snps_chr\": int(n),\n",
    "        \"min_pos\": int(pos[0]),\n",
    "        \"max_pos\": int(pos[-1]),\n",
    "    }\n",
    "\n",
    "df2  = load_positions_rsids(geno_chr2)\n",
    "df10 = load_positions_rsids(geno_chr10)\n",
    "\n",
    "chr2_plan  = choose_window_by_bp(df2,  window_bp=5_000_000, target_snps=311)\n",
    "chr10_plan = choose_window_by_bp(df10, window_bp=1_000_000, target_snps=610)\n",
    "\n",
    "print(\"\\n Proposal-style windows selected:\")\n",
    "print(\"chr2 (5Mb, ~311 SNPs):\", chr2_plan)\n",
    "print(\"chr10 (1Mb, ~610 SNPs):\", chr10_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff17848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "MISSING_TOKENS = {\"\", \"NN\", \"NA\", \"N\", \"00\", \"--\", \"??\"}\n",
    "\n",
    "def _find_sample_columns(columns):\n",
    "    cols = list(columns)\n",
    "    if \"QCcode\" in cols:\n",
    "        qc_idx = cols.index(\"QCcode\")\n",
    "        return cols[:qc_idx + 1], cols[qc_idx + 1:]\n",
    "    # Fallback (rarely needed for HapMap)\n",
    "    return cols[:11], cols[11:]\n",
    "\n",
    "def genotype_row_to_counts(geno_strs, allele_a, allele_b, count_mode=\"minor\"):\n",
    "    \"\"\"\n",
    "    Convert HapMap genotype strings (e.g., 'AA', 'AG', 'GG', 'NN') to\n",
    "    allele-count dosages in {0,1,2} with -1 for missing.\n",
    "\n",
    "    count_mode:\n",
    "      - \"minor\": counts the minor allele within the population\n",
    "      - \"allele_a\": counts allele_a from the alleles column (consistent across pops)\n",
    "    \"\"\"\n",
    "    g = np.asarray(geno_strs, dtype=object)\n",
    "    g = np.array([x.strip() if isinstance(x, str) else \"\" for x in g], dtype=object)\n",
    "\n",
    "    missing = np.zeros(len(g), dtype=bool)\n",
    "    for t in MISSING_TOKENS:\n",
    "        missing |= (g == t)\n",
    "\n",
    "    # valid genotype strings are length 2 (e.g. \"AG\")\n",
    "    valid = (~missing) & np.array([len(x) == 2 for x in g], dtype=bool)\n",
    "\n",
    "    # count allele occurrences among valid entries\n",
    "    a_count = 0\n",
    "    b_count = 0\n",
    "    for x in g[valid]:\n",
    "        a_count += (x[0] == allele_a) + (x[1] == allele_a)\n",
    "        b_count += (x[0] == allele_b) + (x[1] == allele_b)\n",
    "\n",
    "    if count_mode == \"minor\":\n",
    "        if a_count < b_count:\n",
    "            counted, other = allele_a, allele_b\n",
    "        elif b_count < a_count:\n",
    "            counted, other = allele_b, allele_a\n",
    "        else:\n",
    "            counted, other = allele_b, allele_a\n",
    "    elif count_mode == \"allele_a\":\n",
    "        counted, other = allele_a, allele_b\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown count_mode={count_mode}\")\n",
    "\n",
    "    out = np.full(len(g), -1, dtype=np.int8)\n",
    "    for i, x in enumerate(g):\n",
    "        if (not isinstance(x, str)) or (x in MISSING_TOKENS) or len(x) != 2:\n",
    "            continue\n",
    "        out[i] = np.int8((x[0] == counted) + (x[1] == counted))\n",
    "\n",
    "    return out, counted, other\n",
    "\n",
    "def parse_genotypes_window(geno_path, start_bp, end_bp, chunksize=2000, count_mode=\"minor\"):\n",
    "    \"\"\"\n",
    "    Parses HapMap whitespace-separated genotype .txt.gz file and extracts SNPs\n",
    "    within [start_bp, end_bp]. Returns:\n",
    "      G (M x N): int8 allele-count dosages, missing=-1\n",
    "      sample_ids (M,)\n",
    "      snp_ids (N,)\n",
    "      positions (N,)\n",
    "      counted_alleles (N,)\n",
    "      other_alleles (N,)\n",
    "    \"\"\"\n",
    "    print(f\"Parsing genotypes from {geno_path.name}\")\n",
    "    print(f\"   Window: [{start_bp}, {end_bp}] bp (inclusive)\")\n",
    "    print(f\"   Count mode: {count_mode}\")\n",
    "\n",
    "    # Read header row only (detect columns + sample IDs)\n",
    "    header = pd.read_csv(\n",
    "        geno_path,\n",
    "        sep=r\"\\s+\",\n",
    "        engine=\"python\",\n",
    "        compression=\"gzip\",\n",
    "        nrows=1,\n",
    "        dtype=str\n",
    "    )\n",
    "    _, sample_cols = _find_sample_columns(header.columns)\n",
    "    sample_ids = np.array(sample_cols, dtype=object)\n",
    "    print(f\" Individuals detected: {len(sample_ids)}\")\n",
    "\n",
    "    G_cols, snp_ids, positions, counted_alleles, other_alleles = [], [], [], [], []\n",
    "\n",
    "    reader = pd.read_csv(\n",
    "        geno_path,\n",
    "        sep=r\"\\s+\",\n",
    "        engine=\"python\",\n",
    "        compression=\"gzip\",\n",
    "        dtype=str,\n",
    "        chunksize=chunksize,\n",
    "    )\n",
    "\n",
    "    kept = 0\n",
    "    for chunk in tqdm(reader, desc=f\"Reading {geno_path.name}\"):\n",
    "        # Filter by window using numeric positions\n",
    "        pos_int = pd.to_numeric(chunk[\"pos\"], errors=\"coerce\")\n",
    "        mask = (pos_int >= start_bp) & (pos_int <= end_bp)\n",
    "        chunk = chunk.loc[mask]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        for _, row in chunk.iterrows():\n",
    "            rsid = row.get(\"rs#\", None)\n",
    "            alleles = row.get(\"alleles\", None)\n",
    "            pos = row.get(\"pos\", None)\n",
    "\n",
    "            if rsid is None or alleles is None or pos is None or \"/\" not in alleles:\n",
    "                continue\n",
    "\n",
    "            a, b = [x.strip() for x in alleles.split(\"/\", 1)]\n",
    "            if len(a) != 1 or len(b) != 1:\n",
    "                continue\n",
    "\n",
    "            geno_strs = row[sample_cols].values\n",
    "            counts, counted, other = genotype_row_to_counts(geno_strs, a, b, count_mode=count_mode)\n",
    "\n",
    "            G_cols.append(counts)\n",
    "            snp_ids.append(rsid)\n",
    "            positions.append(int(pos))\n",
    "            counted_alleles.append(counted)\n",
    "            other_alleles.append(other)\n",
    "            kept += 1\n",
    "\n",
    "    if kept == 0:\n",
    "        raise RuntimeError(\"No SNPs were kept. Check start/end window values.\")\n",
    "\n",
    "    G = np.stack(G_cols, axis=1)  # (M, N)\n",
    "    positions = np.array(positions, dtype=np.int32)\n",
    "\n",
    "    # Sort by position (just in case)\n",
    "    order = np.argsort(positions)\n",
    "    G = G[:, order]\n",
    "    positions = positions[order]\n",
    "    snp_ids = np.array(snp_ids, dtype=object)[order]\n",
    "    counted_alleles = np.array(counted_alleles, dtype=object)[order]\n",
    "    other_alleles = np.array(other_alleles, dtype=object)[order]\n",
    "\n",
    "    print(f\" Kept SNPs: {G.shape[1]} | Individuals: {G.shape[0]}\")\n",
    "    print(f\" Missing rate: {float(np.mean(G == -1)):.4f}\")\n",
    "    print(f\" Kept position range: {int(positions.min())} .. {int(positions.max())}\")\n",
    "\n",
    "    return G, sample_ids, snp_ids, positions, counted_alleles, other_alleles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2b8decf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CEU/YRI raw region outputs are generated later in the alignment step.\n",
      " Region extraction complete.\n",
      " Check: data/processed/hapmap/regions/\n"
     ]
    }
   ],
   "source": [
    "import json, time\n",
    "\n",
    "# CEU/YRI raw region outputs are generated in the alignment cell below.\n",
    "print(\" CEU/YRI raw region outputs are generated later in the alignment step.\")\n",
    "print(\" Region extraction complete.\")\n",
    "print(\" Check: data/processed/hapmap/regions/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "910dfc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using existing CEU region file for chr2\n",
      " Using existing YRI region file for chr2\n",
      " Using existing aligned files for chr2\n",
      " Using existing CEU region file for chr10\n",
      " Using existing YRI region file for chr10\n",
      " Using existing aligned files for chr10\n",
      "Alignment complete: CEU/YRI common SNP files ready.\n",
      " Using existing YRI region file for chr2\n",
      " Using existing YRI region file for chr10\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Align CEU and YRI regions on common SNPs (allele_a coding)\n",
    "# -----------------------------\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "FORCE_REBUILD = False\n",
    "\n",
    "geno_chr2_ceu = GENO_DIR / \"genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\"\n",
    "geno_chr10_ceu = GENO_DIR / \"genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\"\n",
    "geno_chr2_yri = GENO_DIR / \"genotypes_chr2_YRI_r27_nr.b36_fwd.txt.gz\"\n",
    "geno_chr10_yri = GENO_DIR / \"genotypes_chr10_YRI_r27_nr.b36_fwd.txt.gz\"\n",
    "\n",
    "ALIGNED_CEU_CHR2 = REGION_DIR / \"CEU_chr2_5Mb.common_with_YRI.npz\"\n",
    "ALIGNED_YRI_CHR2 = REGION_DIR / \"YRI_chr2_5Mb.common_with_CEU.npz\"\n",
    "ALIGNED_CEU_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.common_with_YRI.npz\"\n",
    "ALIGNED_YRI_CHR10 = REGION_DIR / \"YRI_chr10_1Mb.common_with_CEU.npz\"\n",
    "\n",
    "\n",
    "CEU_REGION_CHR2 = REGION_DIR / \"CEU_chr2_5Mb.npz\"\n",
    "CEU_REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.npz\"\n",
    "\n",
    "\n",
    "YRI_REGION_CHR2 = REGION_DIR / \"YRI_chr2_5Mb.npz\"\n",
    "YRI_REGION_CHR10 = REGION_DIR / \"YRI_chr10_1Mb.npz\"\n",
    "\n",
    "REQUIRED_GENOS = [geno_chr2_ceu, geno_chr10_ceu, geno_chr2_yri, geno_chr10_yri]\n",
    "missing = [p for p in REQUIRED_GENOS if not p.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing genotype files: {[p.name for p in missing]}\")\n",
    "\n",
    "def _parse_region_for_alignment(geno_path, start_bp, end_bp):\n",
    "    G, sample_ids, snp_ids, positions, counted, other = parse_genotypes_window(\n",
    "        geno_path, start_bp, end_bp, count_mode=\"allele_a\"\n",
    "    )\n",
    "    return {\n",
    "        \"G\": G,\n",
    "        \"sample_ids\": sample_ids,\n",
    "        \"snp_ids\": snp_ids,\n",
    "        \"positions\": positions,\n",
    "        \"counted_alleles\": counted,\n",
    "        \"other_alleles\": other,\n",
    "    }\n",
    "\n",
    "def _align_by_snp(ceu, yri, region_tag):\n",
    "    ceu_rsids = [str(x) for x in ceu[\"snp_ids\"]]\n",
    "    yri_rsids = [str(x) for x in yri[\"snp_ids\"]]\n",
    "    ceu_idx = {rs: i for i, rs in enumerate(ceu_rsids)}\n",
    "    yri_idx = {rs: i for i, rs in enumerate(yri_rsids)}\n",
    "\n",
    "    common_rsids = [rs for rs in ceu_rsids if rs in yri_idx]\n",
    "    dropped_only_ceu = len(ceu_rsids) - len(common_rsids)\n",
    "    dropped_only_yri = len(yri_rsids) - len(common_rsids)\n",
    "\n",
    "    idx_ceu = []\n",
    "    idx_yri = []\n",
    "    mismatch = 0\n",
    "    for rs in common_rsids:\n",
    "        i = ceu_idx[rs]\n",
    "        j = yri_idx[rs]\n",
    "        if (ceu[\"positions\"][i] != yri[\"positions\"][j]):\n",
    "            mismatch += 1\n",
    "            continue\n",
    "        if (ceu[\"counted_alleles\"][i] != yri[\"counted_alleles\"][j]) or (ceu[\"other_alleles\"][i] != yri[\"other_alleles\"][j]):\n",
    "            mismatch += 1\n",
    "            continue\n",
    "        idx_ceu.append(i)\n",
    "        idx_yri.append(j)\n",
    "\n",
    "    print(f\"[{region_tag}] CEU SNPs: {len(ceu_rsids)} | YRI SNPs: {len(yri_rsids)}\")\n",
    "    print(f\"[{region_tag}] Common rsIDs: {len(common_rsids)} | mismatched dropped: {mismatch}\")\n",
    "    print(f\"[{region_tag}] Dropped only-in-CEU: {dropped_only_ceu} | only-in-YRI: {dropped_only_yri}\")\n",
    "\n",
    "    if len(idx_ceu) == 0:\n",
    "        raise RuntimeError(f\"No aligned SNPs remain for {region_tag} after filtering.\")\n",
    "\n",
    "    ceu_aligned = {\n",
    "        \"G\": ceu[\"G\"][:, idx_ceu],\n",
    "        \"sample_ids\": ceu[\"sample_ids\"],\n",
    "        \"snp_ids\": ceu[\"snp_ids\"][idx_ceu],\n",
    "        \"positions\": ceu[\"positions\"][idx_ceu],\n",
    "        \"counted_alleles\": ceu[\"counted_alleles\"][idx_ceu],\n",
    "        \"other_alleles\": ceu[\"other_alleles\"][idx_ceu],\n",
    "    }\n",
    "    yri_aligned = {\n",
    "        \"G\": yri[\"G\"][:, idx_yri],\n",
    "        \"sample_ids\": yri[\"sample_ids\"],\n",
    "        \"snp_ids\": yri[\"snp_ids\"][idx_yri],\n",
    "        \"positions\": yri[\"positions\"][idx_yri],\n",
    "        \"counted_alleles\": yri[\"counted_alleles\"][idx_yri],\n",
    "        \"other_alleles\": yri[\"other_alleles\"][idx_yri],\n",
    "    }\n",
    "\n",
    "    # Sanity checks\n",
    "    assert len(ceu_aligned[\"snp_ids\"]) == len(yri_aligned[\"snp_ids\"])\n",
    "    assert np.array_equal(ceu_aligned[\"snp_ids\"], yri_aligned[\"snp_ids\"])\n",
    "    assert np.array_equal(ceu_aligned[\"positions\"], yri_aligned[\"positions\"])\n",
    "    assert np.array_equal(ceu_aligned[\"counted_alleles\"], yri_aligned[\"counted_alleles\"])\n",
    "\n",
    "    return ceu_aligned, yri_aligned\n",
    "\n",
    "def _save_aligned_region(out_path, region_tag, chrom, plan, payload, other_pop):\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        G=payload[\"G\"].astype(np.int8),\n",
    "        sample_ids=payload[\"sample_ids\"],\n",
    "        snp_ids=payload[\"snp_ids\"],\n",
    "        positions=payload[\"positions\"],\n",
    "        counted_alleles=payload[\"counted_alleles\"],\n",
    "        other_alleles=payload[\"other_alleles\"],\n",
    "        minor_alleles=payload[\"counted_alleles\"],\n",
    "        major_alleles=payload[\"other_alleles\"],\n",
    "        chrom=str(chrom),\n",
    "    )\n",
    "    meta = {\n",
    "        \"region_name\": out_path.stem,\n",
    "        \"chrom\": str(chrom),\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"window_start_bp\": int(plan[\"start_bp\"]),\n",
    "        \"window_end_bp\": int(plan[\"end_bp\"]),\n",
    "        \"snps_in_window\": int(payload[\"G\"].shape[1]),\n",
    "        \"individuals\": int(payload[\"G\"].shape[0]),\n",
    "        \"count_mode\": \"allele_a\",\n",
    "        \"note\": f\"Aligned with {other_pop} on common SNPs; G counts allele_a from the alleles column.\",\n",
    "    }\n",
    "    out_meta = out_path.with_suffix(\".meta.json\")\n",
    "    out_meta.write_text(json.dumps(meta, indent=2))\n",
    "    print(f\" Saved aligned: {out_path.relative_to(PROJECT_ROOT)}\")\n",
    "\n",
    "\n",
    "\n",
    "def _save_region(out_path, region_tag, chrom, plan, payload, pop_label):\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        G=payload[\"G\"].astype(np.int8),\n",
    "        sample_ids=payload[\"sample_ids\"],\n",
    "        snp_ids=payload[\"snp_ids\"],\n",
    "        positions=payload[\"positions\"],\n",
    "        counted_alleles=payload[\"counted_alleles\"],\n",
    "        other_alleles=payload[\"other_alleles\"],\n",
    "        minor_alleles=payload[\"counted_alleles\"],\n",
    "        major_alleles=payload[\"other_alleles\"],\n",
    "        chrom=str(chrom),\n",
    "    )\n",
    "    meta = {\n",
    "        \"region_name\": out_path.stem,\n",
    "        \"chrom\": str(chrom),\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"window_start_bp\": int(plan[\"start_bp\"]),\n",
    "        \"window_end_bp\": int(plan[\"end_bp\"]),\n",
    "        \"snps_in_window\": int(payload[\"G\"].shape[1]),\n",
    "        \"individuals\": int(payload[\"G\"].shape[0]),\n",
    "        \"count_mode\": \"allele_a\",\n",
    "        \"note\": f\"Raw {pop_label} window using CEU-derived region bounds (not aligned).\",\n",
    "    }\n",
    "    out_meta = out_path.with_suffix(\".meta.json\")\n",
    "    out_meta.write_text(json.dumps(meta, indent=2))\n",
    "    print(f\" Saved region: {out_path.relative_to(PROJECT_ROOT)}\")\n",
    "\n",
    "# chr2\n",
    "need_ceu2_raw = (not CEU_REGION_CHR2.exists()) or FORCE_REBUILD\n",
    "need_yri2_raw = (not YRI_REGION_CHR2.exists()) or FORCE_REBUILD\n",
    "need_align_chr2 = (not ALIGNED_CEU_CHR2.exists()) or (not ALIGNED_YRI_CHR2.exists()) or FORCE_REBUILD\n",
    "\n",
    "ceu2_raw = None\n",
    "yri2_raw = None\n",
    "\n",
    "if need_ceu2_raw or need_align_chr2:\n",
    "    ceu2_raw = _parse_region_for_alignment(geno_chr2_ceu, chr2_plan[\"start_bp\"], chr2_plan[\"end_bp\"])\n",
    "if need_yri2_raw or need_align_chr2:\n",
    "    yri2_raw = _parse_region_for_alignment(geno_chr2_yri, chr2_plan[\"start_bp\"], chr2_plan[\"end_bp\"])\n",
    "\n",
    "if need_ceu2_raw:\n",
    "    _save_region(CEU_REGION_CHR2, \"chr2_5Mb\", 2, chr2_plan, ceu2_raw, \"CEU\")\n",
    "else:\n",
    "    print(\" Using existing CEU region file for chr2\")\n",
    "\n",
    "if need_yri2_raw:\n",
    "    _save_region(YRI_REGION_CHR2, \"chr2_5Mb\", 2, chr2_plan, yri2_raw, \"YRI\")\n",
    "else:\n",
    "    print(\" Using existing YRI region file for chr2\")\n",
    "\n",
    "if need_align_chr2:\n",
    "    ceu2_aligned, yri2_aligned = _align_by_snp(ceu2_raw, yri2_raw, \"chr2_5Mb\")\n",
    "    _save_aligned_region(ALIGNED_CEU_CHR2, \"chr2_5Mb\", 2, chr2_plan, ceu2_aligned, \"YRI\")\n",
    "    _save_aligned_region(ALIGNED_YRI_CHR2, \"chr2_5Mb\", 2, chr2_plan, yri2_aligned, \"CEU\")\n",
    "else:\n",
    "    print(\" Using existing aligned files for chr2\")\n",
    "    ceu2_aligned = np.load(ALIGNED_CEU_CHR2, allow_pickle=True)\n",
    "    yri2_aligned = np.load(ALIGNED_YRI_CHR2, allow_pickle=True)\n",
    "    assert np.array_equal(ceu2_aligned[\"snp_ids\"], yri2_aligned[\"snp_ids\"])\n",
    "    assert np.array_equal(ceu2_aligned[\"positions\"], yri2_aligned[\"positions\"])\n",
    "\n",
    "\n",
    "# chr10\n",
    "need_ceu10_raw = (not CEU_REGION_CHR10.exists()) or FORCE_REBUILD\n",
    "need_yri10_raw = (not YRI_REGION_CHR10.exists()) or FORCE_REBUILD\n",
    "need_align_chr10 = (not ALIGNED_CEU_CHR10.exists()) or (not ALIGNED_YRI_CHR10.exists()) or FORCE_REBUILD\n",
    "\n",
    "ceu10_raw = None\n",
    "yri10_raw = None\n",
    "\n",
    "if need_ceu10_raw or need_align_chr10:\n",
    "    ceu10_raw = _parse_region_for_alignment(geno_chr10_ceu, chr10_plan[\"start_bp\"], chr10_plan[\"end_bp\"])\n",
    "if need_yri10_raw or need_align_chr10:\n",
    "    yri10_raw = _parse_region_for_alignment(geno_chr10_yri, chr10_plan[\"start_bp\"], chr10_plan[\"end_bp\"])\n",
    "\n",
    "if need_ceu10_raw:\n",
    "    _save_region(CEU_REGION_CHR10, \"chr10_1Mb\", 10, chr10_plan, ceu10_raw, \"CEU\")\n",
    "else:\n",
    "    print(\" Using existing CEU region file for chr10\")\n",
    "\n",
    "if need_yri10_raw:\n",
    "    _save_region(YRI_REGION_CHR10, \"chr10_1Mb\", 10, chr10_plan, yri10_raw, \"YRI\")\n",
    "else:\n",
    "    print(\" Using existing YRI region file for chr10\")\n",
    "\n",
    "if need_align_chr10:\n",
    "    ceu10_aligned, yri10_aligned = _align_by_snp(ceu10_raw, yri10_raw, \"chr10_1Mb\")\n",
    "    _save_aligned_region(ALIGNED_CEU_CHR10, \"chr10_1Mb\", 10, chr10_plan, ceu10_aligned, \"YRI\")\n",
    "    _save_aligned_region(ALIGNED_YRI_CHR10, \"chr10_1Mb\", 10, chr10_plan, yri10_aligned, \"CEU\")\n",
    "else:\n",
    "    print(\" Using existing aligned files for chr10\")\n",
    "    ceu10_aligned = np.load(ALIGNED_CEU_CHR10, allow_pickle=True)\n",
    "    yri10_aligned = np.load(ALIGNED_YRI_CHR10, allow_pickle=True)\n",
    "    assert np.array_equal(ceu10_aligned[\"snp_ids\"], yri10_aligned[\"snp_ids\"])\n",
    "    assert np.array_equal(ceu10_aligned[\"positions\"], yri10_aligned[\"positions\"])\n",
    "\n",
    "\n",
    "print(\"Alignment complete: CEU/YRI common SNP files ready.\")\n",
    "\n",
    "\n",
    "# YRI raw regions (same window definitions as CEU)\n",
    "if (not YRI_REGION_CHR2.exists()) or FORCE_REBUILD:\n",
    "    yri2_raw = _parse_region_for_alignment(geno_chr2_yri, chr2_plan[\"start_bp\"], chr2_plan[\"end_bp\"])\n",
    "    _save_region(YRI_REGION_CHR2, \"chr2_5Mb\", 2, chr2_plan, yri2_raw, \"YRI\")\n",
    "else:\n",
    "    print(\" Using existing YRI region file for chr2\")\n",
    "\n",
    "if (not YRI_REGION_CHR10.exists()) or FORCE_REBUILD:\n",
    "    yri10_raw = _parse_region_for_alignment(geno_chr10_yri, chr10_plan[\"start_bp\"], chr10_plan[\"end_bp\"])\n",
    "    _save_region(YRI_REGION_CHR10, \"chr10_1Mb\", 10, chr10_plan, yri10_raw, \"YRI\")\n",
    "else:\n",
    "    print(\" Using existing YRI region file for chr10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70c6eb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading aligned regions for cohort split:\n",
      "  - data/processed/hapmap/regions/CEU_chr2_5Mb.common_with_YRI.npz\n",
      "  - data/processed/hapmap/regions/CEU_chr10_1Mb.common_with_YRI.npz\n",
      "  - data/processed/hapmap/regions/YRI_chr2_5Mb.common_with_CEU.npz\n",
      "  - data/processed/hapmap/regions/YRI_chr10_1Mb.common_with_CEU.npz\n",
      " CEU pool size (chr2 âˆ© chr10 sample_ids): 174\n",
      " YRI pool size (chr2 âˆ© chr10 sample_ids): 176\n",
      "Created CEU control/test + YRI case split\n",
      "   counts: {'control': 120, 'test': 54, 'case': 176}\n",
      " Saved: data/processed/hapmap/cohorts/hapmap_CEU_control_test__YRI_case.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- REQUIREMENTS ----\n",
    "# PROJECT_ROOT must already be defined correctly.\n",
    "\n",
    "REGION_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\" / \"regions\"\n",
    "COHORT_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\" / \"cohorts\"\n",
    "COHORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REGION_CEU_CHR2 = REGION_DIR / \"CEU_chr2_5Mb.common_with_YRI.npz\"\n",
    "REGION_CEU_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.common_with_YRI.npz\"\n",
    "REGION_YRI_CHR2 = REGION_DIR / \"YRI_chr2_5Mb.common_with_CEU.npz\"\n",
    "REGION_YRI_CHR10 = REGION_DIR / \"YRI_chr10_1Mb.common_with_CEU.npz\"\n",
    "\n",
    "for p in [REGION_CEU_CHR2, REGION_CEU_CHR10, REGION_YRI_CHR2, REGION_YRI_CHR10]:\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"Missing {p}. Run aligned region extraction first.\")\n",
    "\n",
    "print(\" Loading aligned regions for cohort split:\")\n",
    "print(\"  -\", REGION_CEU_CHR2.relative_to(PROJECT_ROOT))\n",
    "print(\"  -\", REGION_CEU_CHR10.relative_to(PROJECT_ROOT))\n",
    "print(\"  -\", REGION_YRI_CHR2.relative_to(PROJECT_ROOT))\n",
    "print(\"  -\", REGION_YRI_CHR10.relative_to(PROJECT_ROOT))\n",
    "\n",
    "# Load sample IDs from saved region files\n",
    "ceu2 = np.load(REGION_CEU_CHR2, allow_pickle=True)\n",
    "ceu10 = np.load(REGION_CEU_CHR10, allow_pickle=True)\n",
    "yri2 = np.load(REGION_YRI_CHR2, allow_pickle=True)\n",
    "yri10 = np.load(REGION_YRI_CHR10, allow_pickle=True)\n",
    "\n",
    "# Use intersection of individuals available in BOTH CEU regions\n",
    "ceu_ids2 = [str(x) for x in ceu2[\"sample_ids\"]]\n",
    "ceu_ids10 = set(str(x) for x in ceu10[\"sample_ids\"])\n",
    "ceu_pool = sorted(set(ceu_ids2).intersection(ceu_ids10))\n",
    "\n",
    "# Use intersection of individuals available in BOTH YRI regions\n",
    "yri_ids2 = [str(x) for x in yri2[\"sample_ids\"]]\n",
    "yri_ids10 = set(str(x) for x in yri10[\"sample_ids\"])\n",
    "yri_pool = sorted(set(yri_ids2).intersection(yri_ids10))\n",
    "\n",
    "print(f\" CEU pool size (chr2 âˆ© chr10 sample_ids): {len(ceu_pool)}\")\n",
    "print(f\" YRI pool size (chr2 âˆ© chr10 sample_ids): {len(yri_pool)}\")\n",
    "if len(ceu_pool) < 20 or len(yri_pool) < 20:\n",
    "    raise RuntimeError(\"Too few individuals for meaningful split. Check aligned regions.\")\n",
    "\n",
    "# ---- SPLIT CEU into control/test ----\n",
    "SEED = 0\n",
    "rng = np.random.default_rng(SEED)\n",
    "shuffled = list(rng.permutation(ceu_pool))\n",
    "n = len(shuffled)\n",
    "\n",
    "# default target for n=174 -> control=120, test=54\n",
    "TARGET_TOTAL = 174\n",
    "TARGET_CONTROL = 120\n",
    "\n",
    "if n == TARGET_TOTAL:\n",
    "    n_control = TARGET_CONTROL\n",
    "else:\n",
    "    p_control = TARGET_CONTROL / TARGET_TOTAL\n",
    "    n_control = int(round(n * p_control))\n",
    "    n_control = max(2, n_control)\n",
    "\n",
    "n_test = n - n_control\n",
    "if n_test < 2:\n",
    "    n_control = max(2, n - 2)\n",
    "    n_test = n - n_control\n",
    "\n",
    "control_ids = shuffled[:n_control]\n",
    "test_ids = shuffled[n_control:]\n",
    "\n",
    "assert len(set(control_ids) & set(test_ids)) == 0\n",
    "assert len(control_ids) + len(test_ids) == n\n",
    "\n",
    "# YRI case = all available YRI individuals\n",
    "case_ids = yri_pool\n",
    "\n",
    "# Indices in CEU and YRI matrices\n",
    "ceu_order = [str(x) for x in ceu2[\"sample_ids\"]]\n",
    "ceu_idx_map = {sid: i for i, sid in enumerate(ceu_order)}\n",
    "\n",
    "yri_order = [str(x) for x in yri2[\"sample_ids\"]]\n",
    "yri_idx_map = {sid: i for i, sid in enumerate(yri_order)}\n",
    "\n",
    "\n",
    "def ids_to_indices(ids, idx_map, label):\n",
    "    missing = [sid for sid in ids if sid not in idx_map]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Some {label} IDs not found in matrix order: {missing[:10]}\")\n",
    "    return [int(idx_map[sid]) for sid in ids]\n",
    "\n",
    "cohorts = {\n",
    "    \"notes\": {\n",
    "        \"case_source\": \"HapMap YRI (proxy phenotype)\",\n",
    "        \"control_source\": \"HapMap CEU (public reference)\",\n",
    "        \"test_source\": \"HapMap CEU (held-out negatives)\",\n",
    "        \"analysis_pool_ceu\": \"intersection of CEU sample_ids in aligned chr2/ch10 region NPZ files\",\n",
    "        \"analysis_pool_yri\": \"intersection of YRI sample_ids in aligned chr2/ch10 region NPZ files\",\n",
    "        \"seed\": SEED,\n",
    "        \"target_counts_if_n174\": {\"control\": TARGET_CONTROL, \"test\": TARGET_TOTAL - TARGET_CONTROL},\n",
    "    },\n",
    "    \"ceu_matrix_order_source\": \"CEU_chr2_5Mb.common_with_YRI.npz sample_ids order\",\n",
    "    \"yri_matrix_order_source\": \"YRI_chr2_5Mb.common_with_CEU.npz sample_ids order\",\n",
    "    \"control\": {\n",
    "        \"sample_ids\": [str(x) for x in control_ids],\n",
    "        \"indices_in_ceu_matrix\": ids_to_indices(control_ids, ceu_idx_map, \"CEU control\"),\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"sample_ids\": [str(x) for x in test_ids],\n",
    "        \"indices_in_ceu_matrix\": ids_to_indices(test_ids, ceu_idx_map, \"CEU test\"),\n",
    "    },\n",
    "    \"case\": {\n",
    "        \"sample_ids\": [str(x) for x in case_ids],\n",
    "        \"indices_in_yri_matrix\": ids_to_indices(case_ids, yri_idx_map, \"YRI case\"),\n",
    "    },\n",
    "    \"counts\": {\"control\": len(control_ids), \"test\": len(test_ids), \"case\": len(case_ids)},\n",
    "}\n",
    "\n",
    "out_path = COHORT_DIR / \"hapmap_CEU_control_test__YRI_case.json\"\n",
    "out_path.write_text(json.dumps(cohorts, indent=2))\n",
    "\n",
    "print(\"Created CEU control/test + YRI case split\")\n",
    "print(\"   counts:\", cohorts[\"counts\"])\n",
    "print(\" Saved:\", out_path.relative_to(PROJECT_ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31839894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded cohorts: data/processed/hapmap/cohorts/hapmap_CEU_control_test__YRI_case.json\n",
      "Counts: {'control': 120, 'test': 54, 'case': 176}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "COHORTS_PATH = COHORT_DIR / \"hapmap_CEU_control_test__YRI_case.json\"\n",
    "cohorts = json.loads(COHORTS_PATH.read_text())\n",
    "\n",
    "print(\" Loaded cohorts:\", COHORTS_PATH.relative_to(PROJECT_ROOT))\n",
    "print(\"Counts:\", cohorts[\"counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8061413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading aligned CEU region NPZ files + cohort split JSON from disk...\n",
      "chr2 : control n=120 | test n=54\n",
      "chr10: control n=120 | test n=54\n",
      " Computed and saved CEU MAF references (control + test)\n",
      " Saved: data/processed/hapmap/ceu_maf_reference.npz\n",
      "   chr2 mean control MAF = 0.4915183711612868\n",
      "   chr10 mean control MAF = 0.4902755962014695\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Robust PROJECT_ROOT ----\n",
    "def find_project_root(start=None):\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for parent in [p] + list(p.parents):\n",
    "        if (parent / \".git\").exists() or (parent / \"requirements.txt\").exists():\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "PROC_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\"\n",
    "REGION_DIR = PROC_DIR / \"regions\"\n",
    "COHORT_DIR = PROC_DIR / \"cohorts\"\n",
    "COHORTS_JSON = COHORT_DIR / \"hapmap_CEU_control_test__YRI_case.json\"\n",
    "\n",
    "REGION_CHR2 = REGION_DIR / \"CEU_chr2_5Mb.common_with_YRI.npz\"\n",
    "REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.common_with_YRI.npz\"\n",
    "\n",
    "for p in [REGION_CHR2, REGION_CHR10, COHORTS_JSON]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required file: {p}\")\n",
    "\n",
    "print(\" Loading aligned CEU region NPZ files + cohort split JSON from disk...\")\n",
    "\n",
    "# ---- Load region data (disk-backed; no in-memory dependencies) ----\n",
    "r2 = np.load(REGION_CHR2, allow_pickle=True)\n",
    "r10 = np.load(REGION_CHR10, allow_pickle=True)\n",
    "\n",
    "G2 = r2[\"G\"].astype(np.int8)\n",
    "rs2 = np.array(r2[\"snp_ids\"], dtype=object)\n",
    "pos2 = np.array(r2[\"positions\"], dtype=np.int32)\n",
    "ids2 = np.array([str(x) for x in r2[\"sample_ids\"]], dtype=object)\n",
    "\n",
    "G10 = r10[\"G\"].astype(np.int8)\n",
    "rs10 = np.array(r10[\"snp_ids\"], dtype=object)\n",
    "pos10 = np.array(r10[\"positions\"], dtype=np.int32)\n",
    "ids10 = np.array([str(x) for x in r10[\"sample_ids\"]], dtype=object)\n",
    "\n",
    "# ---- Load cohorts (CEU control/test) ----\n",
    "cohorts = json.loads(COHORTS_JSON.read_text())\n",
    "control_ids = set(map(str, cohorts[\"control\"][\"sample_ids\"]))\n",
    "test_ids = set(map(str, cohorts[\"test\"][\"sample_ids\"]))\n",
    "\n",
    "def indices_from_ids(all_ids, wanted_ids_set):\n",
    "    mask = np.array([i in wanted_ids_set for i in all_ids], dtype=bool)\n",
    "    return np.where(mask)[0]\n",
    "\n",
    "control_idx2 = indices_from_ids(ids2, control_ids)\n",
    "test_idx2    = indices_from_ids(ids2, test_ids)\n",
    "\n",
    "control_idx10 = indices_from_ids(ids10, control_ids)\n",
    "test_idx10    = indices_from_ids(ids10, test_ids)\n",
    "\n",
    "print(f\"chr2 : control n={len(control_idx2)} | test n={len(test_idx2)}\")\n",
    "print(f\"chr10: control n={len(control_idx10)} | test n={len(test_idx10)}\")\n",
    "\n",
    "if len(control_idx2) == 0 or len(test_idx2) == 0:\n",
    "    raise RuntimeError(\"chr2 cohort mapping failed: no matched control/test IDs in chr2 region sample_ids.\")\n",
    "if len(control_idx10) == 0 or len(test_idx10) == 0:\n",
    "    raise RuntimeError(\"chr10 cohort mapping failed: no matched control/test IDs in chr10 region sample_ids.\")\n",
    "\n",
    "def maf_from_G(G_sub):\n",
    "    # allele frequency from allele-count dosages {0,1,2}, missing=-1\n",
    "    mask = (G_sub >= 0)\n",
    "    col_sum = (G_sub * mask).sum(axis=0).astype(np.float64)\n",
    "    col_n = mask.sum(axis=0).astype(np.float64)\n",
    "\n",
    "    maf = np.full(G_sub.shape[1], np.nan, dtype=np.float64)\n",
    "    ok = col_n > 0\n",
    "    maf[ok] = (col_sum[ok] / col_n[ok]) / 2.0\n",
    "    return maf\n",
    "\n",
    "maf_ctrl_chr2 = maf_from_G(G2[control_idx2])\n",
    "maf_test_chr2 = maf_from_G(G2[test_idx2])\n",
    "\n",
    "maf_ctrl_chr10 = maf_from_G(G10[control_idx10])\n",
    "maf_test_chr10 = maf_from_G(G10[test_idx10])\n",
    "\n",
    "freq_out = PROC_DIR / \"ceu_maf_reference.npz\"\n",
    "np.savez_compressed(\n",
    "    freq_out,\n",
    "    chr2_control_maf=maf_ctrl_chr2,\n",
    "    chr2_test_maf=maf_test_chr2,\n",
    "    chr2_snp_ids=rs2,\n",
    "    chr2_positions=pos2,\n",
    "    chr10_control_maf=maf_ctrl_chr10,\n",
    "    chr10_test_maf=maf_test_chr10,\n",
    "    chr10_snp_ids=rs10,\n",
    "    chr10_positions=pos10,\n",
    ")\n",
    "\n",
    "print(\" Computed and saved CEU MAF references (control + test)\")\n",
    "print(\" Saved:\", freq_out.relative_to(PROJECT_ROOT))\n",
    "print(\"   chr2 mean control MAF =\", float(np.nanmean(maf_ctrl_chr2)))\n",
    "print(\"   chr10 mean control MAF =\", float(np.nanmean(maf_ctrl_chr10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32f2034d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found region NPZ files.\n",
      "  - data/processed/hapmap/regions/CEU_chr2_5Mb.common_with_YRI.npz\n",
      "  - data/processed/hapmap/regions/CEU_chr10_1Mb.common_with_YRI.npz\n",
      " Loaded regions: chr2 G=(174, 297), chr10 G=(174, 581), individuals=174\n",
      " Loaded cohorts JSON:\n",
      "  - data/processed/hapmap/cohorts/hapmap_CEU_control_test__YRI_case.json\n",
      " Cohorts mapped: control n=120 | test n=54\n",
      " Computed and saved CEU MAF references (control + test)\n",
      "   -> data/processed/hapmap/ceu_maf_reference.npz\n",
      "   chr2 mean control MAF = 0.4915183711612868\n",
      "   chr10 mean control MAF = 0.4902755962014695\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Robust PROJECT_ROOT ----\n",
    "def find_project_root(start=None):\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for parent in [p] + list(p.parents):\n",
    "        if (parent / \".git\").exists() or (parent / \"requirements.txt\").exists():\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "PROC_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\"\n",
    "REGION_DIR = PROC_DIR / \"regions\"\n",
    "COHORT_DIR = PROC_DIR / \"cohorts\"\n",
    "COHORTS_JSON = COHORT_DIR / \"hapmap_CEU_control_test__YRI_case.json\"\n",
    "\n",
    "REGION_CHR2 = REGION_DIR / \"CEU_chr2_5Mb.common_with_YRI.npz\"\n",
    "REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.common_with_YRI.npz\"\n",
    "\n",
    "# ---- Required region files must exist ----\n",
    "for p in [REGION_CHR2, REGION_CHR10]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing required region file: {p}\\n\"\n",
    "            f\"Run your preprocessing notebook to generate data/processed/hapmap/regions/*.npz\"\n",
    "        )\n",
    "\n",
    "print(\" Found region NPZ files.\")\n",
    "print(\"  -\", REGION_CHR2.relative_to(PROJECT_ROOT))\n",
    "print(\"  -\", REGION_CHR10.relative_to(PROJECT_ROOT))\n",
    "\n",
    "# ---- Load region data (disk-backed; no in-memory dependencies) ----\n",
    "r2 = np.load(REGION_CHR2, allow_pickle=True)\n",
    "r10 = np.load(REGION_CHR10, allow_pickle=True)\n",
    "\n",
    "G2 = r2[\"G\"].astype(np.int8)\n",
    "rs2 = np.array(r2[\"snp_ids\"], dtype=object)\n",
    "pos2 = np.array(r2[\"positions\"], dtype=np.int32)\n",
    "ids2 = np.array([str(x) for x in r2[\"sample_ids\"]], dtype=object)\n",
    "\n",
    "G10 = r10[\"G\"].astype(np.int8)\n",
    "rs10 = np.array(r10[\"snp_ids\"], dtype=object)\n",
    "pos10 = np.array(r10[\"positions\"], dtype=np.int32)\n",
    "ids10 = np.array([str(x) for x in r10[\"sample_ids\"]], dtype=object)\n",
    "\n",
    "# Sanity: sample order should match between chr2 and chr10 in your pipeline\n",
    "if len(ids2) != len(ids10) or not np.all(ids2 == ids10):\n",
    "    raise RuntimeError(\n",
    "        \"Sample ID mismatch between chr2 and chr10 region files.\\n\"\n",
    "        \"Your pipeline assumes they are the same people in the same order.\"\n",
    "    )\n",
    "\n",
    "print(f\" Loaded regions: chr2 G={G2.shape}, chr10 G={G10.shape}, individuals={len(ids2)}\")\n",
    "\n",
    "# ---- Load cohorts OR recreate if missing (because data/processed is gitignored) ----\n",
    "COHORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_and_save_default_cohorts(ids_all, out_path, n_control=80, n_case=50, n_test=44, seed=0):\n",
    "    n = len(ids_all)\n",
    "    if n_control + n_case + n_test != n:\n",
    "        raise ValueError(\n",
    "            f\"Cohort sizes must sum to n={n}. Got control={n_control}, case={n_case}, test={n_test}.\"\n",
    "        )\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(n)\n",
    "    shuffled_ids = ids_all[perm].tolist()\n",
    "\n",
    "    control_ids = shuffled_ids[:n_control]\n",
    "    case_ids    = shuffled_ids[n_control:n_control + n_case]\n",
    "    test_ids    = shuffled_ids[n_control + n_case:]\n",
    "\n",
    "    # Build index maps in ORIGINAL matrix order (ids_all order)\n",
    "    index_map = {sid: i for i, sid in enumerate(ids_all.tolist())}\n",
    "    control_idx = [index_map[sid] for sid in control_ids]\n",
    "    case_idx    = [index_map[sid] for sid in case_ids]\n",
    "    test_idx    = [index_map[sid] for sid in test_ids]\n",
    "\n",
    "    payload = {\n",
    "        \"notes\": {\n",
    "            \"why_file_may_be_missing\": \"data/processed is gitignored; this file was recreated locally from region sample_ids\",\n",
    "            \"seed\": seed,\n",
    "            \"split\": {\"control\": n_control, \"case\": n_case, \"test\": n_test},\n",
    "        },\n",
    "        \"order_reference\": \"indices_in_ceu_matrix refer to CEU_chr2_5Mb.common_with_YRI.npz / CEU_chr10_1Mb.common_with_YRI.npz sample_ids order\",\n",
    "        \"control\": {\"sample_ids\": control_ids, \"indices_in_ceu_matrix\": control_idx},\n",
    "        \"case\":    {\"sample_ids\": case_ids,    \"indices_in_ceu_matrix\": case_idx},\n",
    "        \"test\":    {\"sample_ids\": test_ids,    \"indices_in_ceu_matrix\": test_idx},\n",
    "        \"counts\": {\"control\": len(control_ids), \"case\": len(case_ids), \"test\": len(test_ids)},\n",
    "    }\n",
    "\n",
    "    out_path.write_text(json.dumps(payload, indent=2))\n",
    "    print(f\" Cohort JSON was missing, so I recreated it and saved:\\n  - {out_path.relative_to(PROJECT_ROOT)}\")\n",
    "    print(\"   counts:\", payload[\"counts\"])\n",
    "    return payload\n",
    "\n",
    "# Try to load; if missing, recreate deterministically\n",
    "if COHORTS_JSON.exists():\n",
    "    cohorts = json.loads(COHORTS_JSON.read_text())\n",
    "    print(f\" Loaded cohorts JSON:\\n  - {COHORTS_JSON.relative_to(PROJECT_ROOT)}\")\n",
    "else:\n",
    "    cohorts = build_and_save_default_cohorts(ids2, COHORTS_JSON, n_control=80, n_case=50, n_test=44, seed=0)\n",
    "\n",
    "# Get indices robustly (works whether file was old style or new style)\n",
    "def indices_from_cohorts(cohorts_dict, key, ids_all):\n",
    "    block = cohorts_dict.get(key, {})\n",
    "    if \"indices_in_ceu_matrix\" in block:\n",
    "        return np.array(block[\"indices_in_ceu_matrix\"], dtype=int)\n",
    "\n",
    "    # fallback: compute from sample_ids if indices missing\n",
    "    wanted = set(map(str, block.get(\"sample_ids\", [])))\n",
    "    if not wanted:\n",
    "        raise RuntimeError(f\"Cohorts JSON has no indices or sample_ids for '{key}'.\")\n",
    "    mask = np.array([sid in wanted for sid in ids_all], dtype=bool)\n",
    "    return np.where(mask)[0]\n",
    "\n",
    "control_idx = indices_from_cohorts(cohorts, \"control\", ids2)\n",
    "test_idx    = indices_from_cohorts(cohorts, \"test\", ids2)\n",
    "\n",
    "print(f\" Cohorts mapped: control n={len(control_idx)} | test n={len(test_idx)}\")\n",
    "\n",
    "if len(control_idx) == 0 or len(test_idx) == 0:\n",
    "    raise RuntimeError(\"Cohort mapping failed: control/test indices are empty.\")\n",
    "\n",
    "# ---- MAF computation ----\n",
    "def maf_from_G(G_sub):\n",
    "    # minor allele frequency from minor-allele counts {0,1,2}, missing=-1\n",
    "    mask = (G_sub >= 0)\n",
    "    col_sum = (G_sub * mask).sum(axis=0).astype(np.float64)\n",
    "    col_n = mask.sum(axis=0).astype(np.float64)\n",
    "\n",
    "    maf = np.full(G_sub.shape[1], np.nan, dtype=np.float64)\n",
    "    ok = col_n > 0\n",
    "    maf[ok] = (col_sum[ok] / col_n[ok]) / 2.0\n",
    "    return maf\n",
    "\n",
    "maf_ctrl_chr2 = maf_from_G(G2[control_idx])\n",
    "maf_test_chr2 = maf_from_G(G2[test_idx])\n",
    "\n",
    "maf_ctrl_chr10 = maf_from_G(G10[control_idx])\n",
    "maf_test_chr10 = maf_from_G(G10[test_idx])\n",
    "\n",
    "freq_out = PROC_DIR / \"ceu_maf_reference.npz\"\n",
    "np.savez_compressed(\n",
    "    freq_out,\n",
    "    chr2_control_maf=maf_ctrl_chr2,\n",
    "    chr2_test_maf=maf_test_chr2,\n",
    "    chr2_snp_ids=rs2,\n",
    "    chr2_positions=pos2,\n",
    "    chr10_control_maf=maf_ctrl_chr10,\n",
    "    chr10_test_maf=maf_test_chr10,\n",
    "    chr10_snp_ids=rs10,\n",
    "    chr10_positions=pos10,\n",
    ")\n",
    "\n",
    "print(\" Computed and saved CEU MAF references (control + test)\")\n",
    "print(\"   ->\", freq_out.relative_to(PROJECT_ROOT))\n",
    "print(\"   chr2 mean control MAF =\", float(np.nanmean(maf_ctrl_chr2)))\n",
    "print(\"   chr10 mean control MAF =\", float(np.nanmean(maf_ctrl_chr10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78c79ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading region NPZ files + cohort split JSON from disk...\n",
      "chr2 : control n=120 | test n=54\n",
      "chr10: control n=120 | test n=54\n",
      " Computed and saved CEU MAF references (control + test)\n",
      " Saved: data/processed/hapmap/ceu_maf_reference.npz\n",
      "   chr2 mean control MAF = 0.4915183711612868\n",
      "   chr10 mean control MAF = 0.4902755962014695\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Robust PROJECT_ROOT ----\n",
    "def find_project_root(start=None):\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for parent in [p] + list(p.parents):\n",
    "        if (parent / \".git\").exists() or (parent / \"requirements.txt\").exists():\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "PROC_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\"\n",
    "REGION_DIR = PROC_DIR / \"regions\"\n",
    "COHORT_DIR = PROC_DIR / \"cohorts\"\n",
    "COHORTS_JSON = COHORT_DIR / \"hapmap_CEU_control_test__YRI_case.json\"\n",
    "\n",
    "REGION_CHR2 = REGION_DIR / \"CEU_chr2_5Mb.common_with_YRI.npz\"\n",
    "REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.common_with_YRI.npz\"\n",
    "\n",
    "for p in [REGION_CHR2, REGION_CHR10, COHORTS_JSON]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required file: {p}\")\n",
    "\n",
    "print(\" Loading region NPZ files + cohort split JSON from disk...\")\n",
    "\n",
    "# ---- Load region data (disk-backed; no in-memory dependencies) ----\n",
    "r2 = np.load(REGION_CHR2, allow_pickle=True)\n",
    "r10 = np.load(REGION_CHR10, allow_pickle=True)\n",
    "\n",
    "G2 = r2[\"G\"].astype(np.int8)\n",
    "rs2 = np.array(r2[\"snp_ids\"], dtype=object)\n",
    "pos2 = np.array(r2[\"positions\"], dtype=np.int32)\n",
    "ids2 = np.array([str(x) for x in r2[\"sample_ids\"]], dtype=object)\n",
    "\n",
    "G10 = r10[\"G\"].astype(np.int8)\n",
    "rs10 = np.array(r10[\"snp_ids\"], dtype=object)\n",
    "pos10 = np.array(r10[\"positions\"], dtype=np.int32)\n",
    "ids10 = np.array([str(x) for x in r10[\"sample_ids\"]], dtype=object)\n",
    "\n",
    "# ---- Load cohorts (case/control/test) ----\n",
    "cohorts = json.loads(COHORTS_JSON.read_text())\n",
    "control_ids = set(map(str, cohorts[\"control\"][\"sample_ids\"]))\n",
    "test_ids = set(map(str, cohorts[\"test\"][\"sample_ids\"]))\n",
    "\n",
    "def indices_from_ids(all_ids, wanted_ids_set):\n",
    "    mask = np.array([i in wanted_ids_set for i in all_ids], dtype=bool)\n",
    "    return np.where(mask)[0]\n",
    "\n",
    "control_idx2 = indices_from_ids(ids2, control_ids)\n",
    "test_idx2    = indices_from_ids(ids2, test_ids)\n",
    "\n",
    "control_idx10 = indices_from_ids(ids10, control_ids)\n",
    "test_idx10    = indices_from_ids(ids10, test_ids)\n",
    "\n",
    "print(f\"chr2 : control n={len(control_idx2)} | test n={len(test_idx2)}\")\n",
    "print(f\"chr10: control n={len(control_idx10)} | test n={len(test_idx10)}\")\n",
    "\n",
    "if len(control_idx2) == 0 or len(test_idx2) == 0:\n",
    "    raise RuntimeError(\"chr2 cohort mapping failed: no matched control/test IDs in chr2 region sample_ids.\")\n",
    "if len(control_idx10) == 0 or len(test_idx10) == 0:\n",
    "    raise RuntimeError(\"chr10 cohort mapping failed: no matched control/test IDs in chr10 region sample_ids.\")\n",
    "\n",
    "def maf_from_G(G_sub):\n",
    "    # minor allele frequency from minor-allele counts {0,1,2}, missing=-1\n",
    "    mask = (G_sub >= 0)\n",
    "    col_sum = (G_sub * mask).sum(axis=0).astype(np.float64)\n",
    "    col_n = mask.sum(axis=0).astype(np.float64)\n",
    "\n",
    "    maf = np.full(G_sub.shape[1], np.nan, dtype=np.float64)\n",
    "    ok = col_n > 0\n",
    "    maf[ok] = (col_sum[ok] / col_n[ok]) / 2.0\n",
    "    return maf\n",
    "\n",
    "maf_ctrl_chr2 = maf_from_G(G2[control_idx2])\n",
    "maf_test_chr2 = maf_from_G(G2[test_idx2])\n",
    "\n",
    "maf_ctrl_chr10 = maf_from_G(G10[control_idx10])\n",
    "maf_test_chr10 = maf_from_G(G10[test_idx10])\n",
    "\n",
    "freq_out = PROC_DIR / \"ceu_maf_reference.npz\"\n",
    "np.savez_compressed(\n",
    "    freq_out,\n",
    "    chr2_control_maf=maf_ctrl_chr2,\n",
    "    chr2_test_maf=maf_test_chr2,\n",
    "    chr2_snp_ids=rs2,\n",
    "    chr2_positions=pos2,\n",
    "    chr10_control_maf=maf_ctrl_chr10,\n",
    "    chr10_test_maf=maf_test_chr10,\n",
    "    chr10_snp_ids=rs10,\n",
    "    chr10_positions=pos10,\n",
    ")\n",
    "\n",
    "print(\" Computed and saved CEU MAF references (control + test)\")\n",
    "print(\" Saved:\", freq_out.relative_to(PROJECT_ROOT))\n",
    "print(\"   chr2 mean control MAF =\", float(np.nanmean(maf_ctrl_chr2)))\n",
    "print(\"   chr10 mean control MAF =\", float(np.nanmean(maf_ctrl_chr10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a3205ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Project root: /Users/erkmenerken/Desktop/proje430\n",
      "âœ… Raw phasing dir: /Users/erkmenerken/Desktop/proje430/data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED\n",
      "âœ… Regions: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/regions\n",
      "âœ… Cohorts: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/cohorts\n",
      "âœ… Output haplotypes: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/haplotypes\n",
      "âœ… Output blocks: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/blocks\n",
      "Exists - data/processed/hapmap/regions/CEU_chr2_5Mb.common_with_YRI.npz\n",
      "Exists - data/processed/hapmap/regions/CEU_chr10_1Mb.common_with_YRI.npz\n",
      "Exists - data/processed/hapmap/cohorts/hapmap_CEU_control_test__YRI_case.json\n",
      "Exists - data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      "Exists - data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"\n",
    "    Find repo root robustly without relying on folder name.\n",
    "    We search upward from current working directory for common repo markers.\n",
    "    \"\"\"\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for p in [cwd] + list(cwd.parents):\n",
    "        if (p / \".git\").exists():\n",
    "            return p\n",
    "        if (p / \"requirements.txt\").exists():\n",
    "            return p\n",
    "        if (p / \"pyproject.toml\").exists():\n",
    "            return p\n",
    "        # fallback heuristic: repo root usually contains both src/ and data/ (or at least src/)\n",
    "        if (p / \"src\").exists() and (p / \"data\").exists():\n",
    "            return p\n",
    "        if (p / \"src\").exists() and (p / \"notebooks\").exists():\n",
    "            return p\n",
    "    # last resort: use the top-most parent that still contains \"src\"\n",
    "    for p in reversed(list(cwd.parents)):\n",
    "        if (p / \"src\").exists():\n",
    "            return p\n",
    "    return cwd\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "\n",
    "RAW_HAPMAP = PROJECT_ROOT / \"data\" / \"raw\" / \"hapmap\"\n",
    "PHASE_DIR  = RAW_HAPMAP / \"phasing\" / \"HapMap3_r2\" / \"CEU\" / \"UNRELATED\"\n",
    "\n",
    "PROC_DIR      = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\"\n",
    "REGION_DIR    = PROC_DIR / \"regions\"\n",
    "COHORT_DIR    = PROC_DIR / \"cohorts\"\n",
    "HAP_OUT_DIR   = PROC_DIR / \"haplotypes\"\n",
    "BLOCK_OUT_DIR = PROC_DIR / \"blocks\"\n",
    "\n",
    "for d in [RAW_HAPMAP, PHASE_DIR, PROC_DIR, REGION_DIR, COHORT_DIR, HAP_OUT_DIR, BLOCK_OUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REGION_CHR2  = REGION_DIR / \"CEU_chr2_5Mb.common_with_YRI.npz\"\n",
    "REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.common_with_YRI.npz\"\n",
    "COHORTS_JSON = COHORT_DIR / \"hapmap_CEU_control_test__YRI_case.json\"\n",
    "\n",
    "PHASE_CHR2  = PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\"\n",
    "PHASE_CHR10 = PHASE_DIR / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\"\n",
    "\n",
    "print(\"âœ… Project root:\", PROJECT_ROOT)\n",
    "print(\"âœ… Raw phasing dir:\", PHASE_DIR)\n",
    "print(\"âœ… Regions:\", REGION_DIR)\n",
    "print(\"âœ… Cohorts:\", COHORT_DIR)\n",
    "print(\"âœ… Output haplotypes:\", HAP_OUT_DIR)\n",
    "print(\"âœ… Output blocks:\", BLOCK_OUT_DIR)\n",
    "\n",
    "for p in [REGION_CHR2, REGION_CHR10, COHORTS_JSON, PHASE_CHR2, PHASE_CHR10]:\n",
    "    status = \"Exists\" if p.exists() else \"Missing\"\n",
    "    try:\n",
    "        rel = p.relative_to(PROJECT_ROOT)\n",
    "        print(status, \"-\", rel)\n",
    "    except Exception:\n",
    "        print(status, \"-\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a3f2d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded regions:\n",
      "  chr2 : (174, 297) | SNPs: 297 | individuals: 174\n",
      "  chr10: (174, 581) | SNPs: 581 | individuals: 174\n",
      "\n",
      " Loaded CEU cohorts (mapped to region sample order):\n",
      "  chr2 : control n = 120 | test n = 54 | case n = 0\n",
      "  chr10: control n = 120 | test n = 54 | case n = 0\n",
      "\n",
      "â„¹ï¸ Note: `control_idx` and `test_idx` are set to chr2 indices for backward compatibility.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_region_npz(path: Path):\n",
    "    z = np.load(path, allow_pickle=True)\n",
    "    out = {k: z[k] for k in z.files}\n",
    "    # make sure types are friendly\n",
    "    out[\"snp_ids\"] = out[\"snp_ids\"].astype(object)\n",
    "    out[\"sample_ids\"] = out[\"sample_ids\"].astype(object)\n",
    "    out[\"minor_alleles\"] = out[\"minor_alleles\"].astype(object)\n",
    "    out[\"major_alleles\"] = out[\"major_alleles\"].astype(object)\n",
    "    return out\n",
    "\n",
    "# Load regions\n",
    "r2 = load_region_npz(REGION_CHR2)\n",
    "r10 = load_region_npz(REGION_CHR10)\n",
    "\n",
    "# Load cohorts JSON (NEW schema: sample_ids only)\n",
    "with open(COHORTS_JSON, \"r\") as f:\n",
    "    cohorts = json.load(f)\n",
    "\n",
    "print(\"âœ… Loaded regions:\")\n",
    "print(\"  chr2 :\", r2[\"G\"].shape, \"| SNPs:\", len(r2[\"snp_ids\"]), \"| individuals:\", len(r2[\"sample_ids\"]))\n",
    "print(\"  chr10:\", r10[\"G\"].shape, \"| SNPs:\", len(r10[\"snp_ids\"]), \"| individuals:\", len(r10[\"sample_ids\"]))\n",
    "\n",
    "# Convert region sample IDs to strings for matching\n",
    "ids2 = np.array([str(x) for x in r2[\"sample_ids\"]], dtype=object)\n",
    "ids10 = np.array([str(x) for x in r10[\"sample_ids\"]], dtype=object)\n",
    "\n",
    "control_ids = set(map(str, cohorts[\"control\"][\"sample_ids\"]))\n",
    "test_ids    = set(map(str, cohorts[\"test\"][\"sample_ids\"]))\n",
    "case_ids    = set(map(str, cohorts[\"case\"][\"sample_ids\"])) if \"case\" in cohorts else set()\n",
    "\n",
    "def idx_from_ids(all_ids, wanted_set):\n",
    "    mask = np.array([x in wanted_set for x in all_ids], dtype=bool)\n",
    "    return np.where(mask)[0]\n",
    "\n",
    "# IMPORTANT: indices are region-specific (donâ€™t assume chr2 == chr10 ordering)\n",
    "control_idx2 = idx_from_ids(ids2, control_ids)\n",
    "test_idx2    = idx_from_ids(ids2, test_ids)\n",
    "case_idx2    = idx_from_ids(ids2, case_ids) if case_ids else np.array([], dtype=int)\n",
    "\n",
    "control_idx10 = idx_from_ids(ids10, control_ids)\n",
    "test_idx10    = idx_from_ids(ids10, test_ids)\n",
    "case_idx10    = idx_from_ids(ids10, case_ids) if case_ids else np.array([], dtype=int)\n",
    "\n",
    "print(\"\\n Loaded CEU cohorts (mapped to region sample order):\")\n",
    "print(\"  chr2 : control n =\", len(control_idx2), \"| test n =\", len(test_idx2), \"| case n =\", len(case_idx2))\n",
    "print(\"  chr10: control n =\", len(control_idx10), \"| test n =\", len(test_idx10), \"| case n =\", len(case_idx10))\n",
    "\n",
    "# Backward compatibility if old code still expects control_idx / test_idx\n",
    "control_idx = control_idx2\n",
    "test_idx = test_idx2\n",
    "print(\"\\nâ„¹ï¸ Note: `control_idx` and `test_idx` are set to chr2 indices for backward compatibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d433896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded regions from disk:\n",
      "  chr2 : (174, 297) | SNPs: 297\n",
      "  chr10: (174, 581) | SNPs: 581\n",
      "\n",
      "ðŸ”§ Building blocks for chr2 (region NPZ)\n",
      " chr2 inferred blocks: 4\n",
      " Saved blocks â†’ data/processed/hapmap/blocks/CEU_chr2_5Mb.common_with_YRI.blocks.json\n",
      "   CEU_chr2_5Mb.common_with_YRI: 4 blocks\n",
      "\n",
      "\n",
      "ðŸ”§ Building blocks for chr10 (region NPZ)\n",
      " chr10 inferred blocks: 8\n",
      " Saved blocks â†’ data/processed/hapmap/blocks/CEU_chr10_1Mb.common_with_YRI.blocks.json\n",
      "   CEU_chr10_1Mb.common_with_YRI: 8 blocks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Requirements: REGION_CHR2 / REGION_CHR10 already defined and exist ----\n",
    "for p in [REGION_CHR2, REGION_CHR10]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing region NPZ: {p}\")\n",
    "\n",
    "def load_region_npz(path: Path):\n",
    "    z = np.load(path, allow_pickle=True)\n",
    "    out = {k: z[k] for k in z.files}\n",
    "    out[\"snp_ids\"] = out[\"snp_ids\"].astype(object)\n",
    "    out[\"positions\"] = out[\"positions\"].astype(np.int32)\n",
    "    out[\"sample_ids\"] = out[\"sample_ids\"].astype(object)\n",
    "    return out\n",
    "\n",
    "def adjacent_r2_from_G(G):\n",
    "    \"\"\"\n",
    "    Compute r^2 between adjacent SNPs using correlation on dosage {0,1,2},\n",
    "    ignoring missing=-1. Returns length (N-1) array.\n",
    "    \"\"\"\n",
    "    X = G.astype(float)\n",
    "    M, N = X.shape\n",
    "    out = np.zeros(N - 1, dtype=float)\n",
    "\n",
    "    for j in range(N - 1):\n",
    "        x = X[:, j]\n",
    "        y = X[:, j + 1]\n",
    "        mask = (x >= 0) & (y >= 0)\n",
    "        if mask.sum() < 10:\n",
    "            out[j] = 0.0\n",
    "            continue\n",
    "\n",
    "        xv = x[mask] - x[mask].mean()\n",
    "        yv = y[mask] - y[mask].mean()\n",
    "        denom = np.sqrt((xv * xv).sum() * (yv * yv).sum())\n",
    "        if denom == 0:\n",
    "            out[j] = 0.0\n",
    "            continue\n",
    "\n",
    "        r = float((xv * yv).sum() / denom)\n",
    "        out[j] = r * r\n",
    "    return out\n",
    "\n",
    "def build_blocks_from_adjacent_r2(r2, threshold=0.8, min_snps=5, max_snps=80):\n",
    "    \"\"\"\n",
    "    Create blocks by cutting when adjacent r^2 < threshold.\n",
    "    Then:\n",
    "      - merge blocks smaller than min_snps into previous\n",
    "      - split blocks larger than max_snps\n",
    "    Returns list of (start_idx, end_idx) inclusive, in SNP-index space.\n",
    "    \"\"\"\n",
    "    N = len(r2) + 1\n",
    "    cuts = [0]\n",
    "    for j, v in enumerate(r2):\n",
    "        if v < threshold:\n",
    "            cuts.append(j + 1)\n",
    "    cuts.append(N)\n",
    "\n",
    "    blocks = [(cuts[i], cuts[i + 1] - 1) for i in range(len(cuts) - 1)]\n",
    "\n",
    "    # merge tiny blocks into previous\n",
    "    merged = []\n",
    "    for s, e in blocks:\n",
    "        if not merged:\n",
    "            merged.append((s, e))\n",
    "        else:\n",
    "            if (e - s + 1) < min_snps:\n",
    "                ps, pe = merged[-1]\n",
    "                merged[-1] = (ps, e)\n",
    "            else:\n",
    "                merged.append((s, e))\n",
    "\n",
    "    # split huge blocks\n",
    "    final = []\n",
    "    for s, e in merged:\n",
    "        while (e - s + 1) > max_snps:\n",
    "            final.append((s, s + max_snps - 1))\n",
    "            s = s + max_snps\n",
    "        final.append((s, e))\n",
    "\n",
    "    return final\n",
    "\n",
    "def save_blocks_json(region_name, snp_ids, positions, blocks, params):\n",
    "    \"\"\"\n",
    "    Saves blocks JSON using the region NPZ SNP list (disk-backed, reproducible).\n",
    "    \"\"\"\n",
    "    out = BLOCK_OUT_DIR / f\"{region_name}.blocks.json\"\n",
    "\n",
    "    payload = {\n",
    "        \"region_name\": region_name,\n",
    "        \"phased_compatible\": False,\n",
    "        \"block_params\": params,\n",
    "        \"num_snps\": int(len(snp_ids)),\n",
    "        \"num_blocks\": int(len(blocks)),\n",
    "        \"blocks\": [\n",
    "            {\n",
    "                \"block_id\": int(i),\n",
    "                \"start_snp_index\": int(s),\n",
    "                \"end_snp_index\": int(e),\n",
    "                \"num_snps\": int(e - s + 1),\n",
    "                \"start_pos\": int(positions[s]),\n",
    "                \"end_pos\": int(positions[e]),\n",
    "                \"snp_ids\": [str(x) for x in snp_ids[s:e+1]],\n",
    "            }\n",
    "            for i, (s, e) in enumerate(blocks)\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    out.write_text(json.dumps(payload, indent=2))\n",
    "    print(f\" Saved blocks â†’ {out.relative_to(PROJECT_ROOT)}\")\n",
    "    print(f\"   {region_name}: {len(blocks)} blocks\\n\")\n",
    "    return out\n",
    "\n",
    "BLOCK_PARAMS = {\"threshold\": 0.8, \"min_snps\": 5, \"max_snps\": 80}\n",
    "\n",
    "# ---- Load region data ----\n",
    "r2 = load_region_npz(REGION_CHR2)\n",
    "r10 = load_region_npz(REGION_CHR10)\n",
    "\n",
    "print(\" Loaded regions from disk:\")\n",
    "print(\"  chr2 :\", r2[\"G\"].shape, \"| SNPs:\", len(r2[\"snp_ids\"]))\n",
    "print(\"  chr10:\", r10[\"G\"].shape, \"| SNPs:\", len(r10[\"snp_ids\"]))\n",
    "\n",
    "# ---- Build blocks (region-based, reproducible) ----\n",
    "print(\"\\nðŸ”§ Building blocks for chr2 (region NPZ)\")\n",
    "r2_adj = adjacent_r2_from_G(r2[\"G\"])\n",
    "blocks_chr2 = build_blocks_from_adjacent_r2(r2_adj, **BLOCK_PARAMS)\n",
    "print(f\" chr2 inferred blocks: {len(blocks_chr2)}\")\n",
    "blocks_chr2_path = save_blocks_json(\"CEU_chr2_5Mb.common_with_YRI\", r2[\"snp_ids\"], r2[\"positions\"], blocks_chr2, BLOCK_PARAMS)\n",
    "\n",
    "print(\"\\nðŸ”§ Building blocks for chr10 (region NPZ)\")\n",
    "r10_adj = adjacent_r2_from_G(r10[\"G\"])\n",
    "blocks_chr10 = build_blocks_from_adjacent_r2(r10_adj, **BLOCK_PARAMS)\n",
    "print(f\" chr10 inferred blocks: {len(blocks_chr10)}\")\n",
    "blocks_chr10_path = save_blocks_json(\"CEU_chr10_1Mb.common_with_YRI\", r10[\"snp_ids\"], r10[\"positions\"], blocks_chr10, BLOCK_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1466136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Project root: /Users/erkmenerken/Desktop/proje430\n",
      " RAW_HAPMAP: /Users/erkmenerken/Desktop/proje430/data/raw/hapmap\n",
      " PROC_DIR: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap\n",
      " REGION_DIR: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/regions\n",
      " COHORT_DIR: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/cohorts\n",
      " BLOCK_OUT_DIR: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/blocks\n",
      " HAP_OUT_DIR: /Users/erkmenerken/Desktop/proje430/data/processed/hapmap/haplotypes\n",
      "Exists - data/processed/hapmap/regions/CEU_chr2_5Mb.common_with_YRI.npz\n",
      "Exists - data/processed/hapmap/regions/CEU_chr10_1Mb.common_with_YRI.npz\n",
      "Exists - data/processed/hapmap/cohorts/hapmap_CEU_control_test__YRI_case.json\n",
      "\n",
      " Using phased files:\n",
      "  chr2 : data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      "  chr10: data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# ---------- PROJECT ROOT ----------\n",
    "def find_project_root() -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for p in [cwd] + list(cwd.parents):\n",
    "        if (p / \".git\").exists() or (p / \"requirements.txt\").exists() or (p / \"pyproject.toml\").exists():\n",
    "            return p\n",
    "        if (p / \"src\").exists() and (p / \"data\").exists():\n",
    "            return p\n",
    "    return cwd\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "\n",
    "RAW_HAPMAP = PROJECT_ROOT / \"data\" / \"raw\" / \"hapmap\"\n",
    "PROC_DIR   = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\"\n",
    "REGION_DIR = PROC_DIR / \"regions\"\n",
    "COHORT_DIR = PROC_DIR / \"cohorts\"\n",
    "\n",
    "HAP_OUT_DIR   = PROC_DIR / \"haplotypes\"\n",
    "BLOCK_OUT_DIR = PROC_DIR / \"blocks\"\n",
    "\n",
    "for d in [RAW_HAPMAP, PROC_DIR, REGION_DIR, COHORT_DIR, HAP_OUT_DIR, BLOCK_OUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REGION_CHR2  = REGION_DIR / \"CEU_chr2_5Mb.common_with_YRI.npz\"\n",
    "REGION_CHR10 = REGION_DIR / \"CEU_chr10_1Mb.common_with_YRI.npz\"\n",
    "COHORTS_JSON = COHORT_DIR / \"hapmap_CEU_control_test__YRI_case.json\"\n",
    "\n",
    "# ---------- PHASED FILE RESOLUTION ----------\n",
    "PHASING_ROOT = RAW_HAPMAP / \"phasing\"\n",
    "\n",
    "PHASE_NAME_CHR2  = \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\"\n",
    "PHASE_NAME_CHR10 = \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\"\n",
    "\n",
    "def resolve_under(root: Path, filename: str) -> Path:\n",
    "    # first try the old \"expected\" location (fast)\n",
    "    expected = PHASING_ROOT / \"HapMap3_r2\" / \"CEU\" / \"UNRELATED\" / filename\n",
    "    if expected.exists():\n",
    "        return expected\n",
    "\n",
    "    # then search anywhere under data/raw/hapmap (robust)\n",
    "    hits = sorted(root.rglob(filename))\n",
    "    if hits:\n",
    "        # choose the shortest path (usually the intended one)\n",
    "        hits = sorted(hits, key=lambda p: len(str(p)))\n",
    "        return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find phased file '{filename}' under {root}.\\n\"\n",
    "        f\"Run this in terminal to confirm where it is:\\n\"\n",
    "        f\"  find {root} -name '{filename}'\\n\"\n",
    "        f\"If it truly doesn't exist, you must download it into:\\n\"\n",
    "        f\"  {PHASING_ROOT}\\n\"\n",
    "    )\n",
    "\n",
    "PHASE_CHR2  = resolve_under(RAW_HAPMAP, PHASE_NAME_CHR2)\n",
    "PHASE_CHR10 = resolve_under(RAW_HAPMAP, PHASE_NAME_CHR10)\n",
    "\n",
    "print(\" Project root:\", PROJECT_ROOT)\n",
    "print(\" RAW_HAPMAP:\", RAW_HAPMAP)\n",
    "print(\" PROC_DIR:\", PROC_DIR)\n",
    "print(\" REGION_DIR:\", REGION_DIR)\n",
    "print(\" COHORT_DIR:\", COHORT_DIR)\n",
    "print(\" BLOCK_OUT_DIR:\", BLOCK_OUT_DIR)\n",
    "print(\" HAP_OUT_DIR:\", HAP_OUT_DIR)\n",
    "\n",
    "for p in [REGION_CHR2, REGION_CHR10, COHORTS_JSON]:\n",
    "    print((\"Exists\" if p.exists() else \"Missing\"), \"-\", p.relative_to(PROJECT_ROOT))\n",
    "\n",
    "print(\"\\n Using phased files:\")\n",
    "print(\"  chr2 :\", PHASE_CHR2.relative_to(PROJECT_ROOT))\n",
    "print(\"  chr10:\", PHASE_CHR10.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e40fd4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¥ Loading phased file: hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      "   Phased rows loaded: 116430 | columns: 36\n",
      "\n",
      "[chr2] Individuals:\n",
      "  phased total: 17\n",
      "  phased âˆ© region: 17\n",
      "\n",
      "[chr2] SNPs:\n",
      "  region SNPs: 297\n",
      "  matched in phased by rsID: 113\n",
      "[chr2] Phased allele matrices: A=(113, 17), B=(113, 17)\n",
      " Created pc2\n",
      "\n",
      "ðŸ“¥ Loading phased file: hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n",
      "   Phased rows loaded: 73832 | columns: 36\n",
      "\n",
      "[chr10] Individuals:\n",
      "  phased total: 17\n",
      "  phased âˆ© region: 17\n",
      "\n",
      "[chr10] SNPs:\n",
      "  region SNPs: 581\n",
      "  matched in phased by rsID: 127\n",
      "[chr10] Phased allele matrices: A=(127, 17), B=(127, 17)\n",
      " Created pc10\n",
      "\n",
      " Building phased-compatible blocks for chr2\n",
      " chr2 phased-compatible SNPs: 113 | blocks: 3\n",
      "\n",
      " Building phased-compatible blocks for chr10\n",
      " chr10 phased-compatible SNPs: 127 | blocks: 2\n",
      "\n",
      " Building haplotype histograms for CEU_chr2_5Mb (phased-compatible)\n",
      "  phased-compatible individuals: 17\n",
      "  control individuals used:      11\n",
      "   block 0: SNPs 0-79 | unique=22 | top1=1\n",
      "\n",
      " Saved haplotype histograms â†’ data/processed/hapmap/haplotypes/CEU_chr2_5Mb.control_haplotypes.phased_compatible.json\n",
      "\n",
      " Building haplotype histograms for CEU_chr10_1Mb (phased-compatible)\n",
      "  phased-compatible individuals: 17\n",
      "  control individuals used:      11\n",
      "   block 0: SNPs 0-79 | unique=9 | top1=8\n",
      "\n",
      " Saved haplotype histograms â†’ data/processed/hapmap/haplotypes/CEU_chr10_1Mb.control_haplotypes.phased_compatible.json\n",
      "\n",
      " Done building phased-compatible control haplotype histograms.\n",
      " chr2 â†’ data/processed/hapmap/haplotypes/CEU_chr2_5Mb.control_haplotypes.phased_compatible.json\n",
      " chr10 â†’ data/processed/hapmap/haplotypes/CEU_chr10_1Mb.control_haplotypes.phased_compatible.json\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Helpers\n",
    "# -----------------------------\n",
    "def load_phased_df(phase_gz_path):\n",
    "    print(f\"\\nðŸ“¥ Loading phased file: {phase_gz_path.name}\")\n",
    "    df = pd.read_csv(\n",
    "        phase_gz_path,\n",
    "        sep=r\"\\s+\",\n",
    "        engine=\"python\",\n",
    "        compression=\"gzip\",\n",
    "        dtype=str,\n",
    "    )\n",
    "    print(f\"   Phased rows loaded: {len(df)} | columns: {len(df.columns)}\")\n",
    "    if \"rsID\" not in df.columns:\n",
    "        raise ValueError(\"Unexpected phased file columns. Expected 'rsID'.\")\n",
    "    # position column name differs across HapMap releases; try both\n",
    "    if \"position_b36\" not in df.columns and \"position\" not in df.columns:\n",
    "        print(\" No 'position_b36' or 'position' column found. We'll proceed by rsID alignment only.\")\n",
    "    return df\n",
    "\n",
    "def phased_individuals_from_columns(df):\n",
    "    hap_cols = [c for c in df.columns if c.endswith(\"_A\") or c.endswith(\"_B\")]\n",
    "    if len(hap_cols) == 0:\n",
    "        raise ValueError(\"No _A/_B haplotype columns found in phased file.\")\n",
    "    individuals = sorted(set(c[:-2] for c in hap_cols))\n",
    "    individuals = [i for i in individuals if f\"{i}_A\" in df.columns and f\"{i}_B\" in df.columns]\n",
    "    return individuals\n",
    "\n",
    "def make_phased_compatible_region(region, phased_df, region_name):\n",
    "    region_sample_ids = np.array([str(x) for x in region[\"sample_ids\"]], dtype=object)\n",
    "    region_snp_ids    = np.array([str(x) for x in region[\"snp_ids\"]], dtype=object)\n",
    "    region_positions  = np.array(region[\"positions\"], dtype=int)\n",
    "\n",
    "    phased_indivs_all = phased_individuals_from_columns(phased_df)\n",
    "    region_indiv_set = set(region_sample_ids.tolist())\n",
    "    phased_indivs = [i for i in phased_indivs_all if i in region_indiv_set]\n",
    "\n",
    "    print(f\"\\n[{region_name}] Individuals:\")\n",
    "    print(f\"  phased total: {len(phased_indivs_all)}\")\n",
    "    print(f\"  phased âˆ© region: {len(phased_indivs)}\")\n",
    "\n",
    "    if len(phased_indivs) == 0:\n",
    "        raise RuntimeError(f\"[{region_name}] No overlapping individuals between region and phased file.\")\n",
    "\n",
    "    phased_rsids_set = set(phased_df[\"rsID\"].astype(str).tolist())\n",
    "    keep_snp_mask = np.array([rs in phased_rsids_set for rs in region_snp_ids], dtype=bool)\n",
    "\n",
    "    snp_ids_sub = region_snp_ids[keep_snp_mask]\n",
    "    positions_sub = region_positions[keep_snp_mask]\n",
    "\n",
    "    print(f\"\\n[{region_name}] SNPs:\")\n",
    "    print(f\"  region SNPs: {len(region_snp_ids)}\")\n",
    "    print(f\"  matched in phased by rsID: {len(snp_ids_sub)}\")\n",
    "\n",
    "    if len(snp_ids_sub) == 0:\n",
    "        raise RuntimeError(f\"[{region_name}] No SNPs from region found in phased file by rsID.\")\n",
    "\n",
    "    indiv_idx_in_region = np.array([np.where(region_sample_ids == i)[0][0] for i in phased_indivs], dtype=int)\n",
    "    snp_idx_in_region = np.where(keep_snp_mask)[0]\n",
    "\n",
    "    minor_all = region.get(\"minor_alleles\", region.get(\"counted_alleles\"))\n",
    "    minor_alleles_sub = minor_all[snp_idx_in_region] if minor_all is not None else None\n",
    "\n",
    "    G_sub = region[\"G\"][indiv_idx_in_region][:, snp_idx_in_region]\n",
    "    sample_ids_sub = region_sample_ids[indiv_idx_in_region]\n",
    "\n",
    "    # subset phased rows and reorder to match region order\n",
    "    phased_sub = phased_df[phased_df[\"rsID\"].isin(set(snp_ids_sub.tolist()))].copy()\n",
    "    order_map = {rsid: i for i, rsid in enumerate(snp_ids_sub.tolist())}\n",
    "    phased_sub[\"__order\"] = phased_sub[\"rsID\"].map(order_map)\n",
    "    phased_sub = phased_sub.sort_values(\"__order\").drop(columns=\"__order\")\n",
    "\n",
    "    A_cols = [f\"{i}_A\" for i in phased_indivs]\n",
    "    B_cols = [f\"{i}_B\" for i in phased_indivs]\n",
    "    alleles_A = phased_sub[A_cols].to_numpy(dtype=object)  # (n_snps, n_indiv)\n",
    "    alleles_B = phased_sub[B_cols].to_numpy(dtype=object)\n",
    "\n",
    "    print(f\"[{region_name}] Phased allele matrices: A={alleles_A.shape}, B={alleles_B.shape}\")\n",
    "    return {\n",
    "        \"region_name\": region_name,\n",
    "        \"G_sub\": G_sub,\n",
    "        \"sample_ids_sub\": sample_ids_sub,\n",
    "        \"snp_ids_sub\": snp_ids_sub,\n",
    "        \"positions_sub\": positions_sub,\n",
    "        \"minor_alleles_sub\": minor_alleles_sub,\n",
    "        \"alleles_A\": alleles_A,\n",
    "        \"alleles_B\": alleles_B,\n",
    "        \"phased_individuals\": phased_indivs,\n",
    "        \"indiv_idx_in_region\": indiv_idx_in_region,\n",
    "        \"snp_idx_in_region\": snp_idx_in_region,\n",
    "    }\n",
    "\n",
    "def adjacent_r2_from_G(G):\n",
    "    X = G.astype(float)\n",
    "    M, N = X.shape\n",
    "    out = np.zeros(N - 1, dtype=float)\n",
    "    for j in range(N - 1):\n",
    "        x = X[:, j]\n",
    "        y = X[:, j + 1]\n",
    "        mask = (x >= 0) & (y >= 0)\n",
    "        if mask.sum() < 10:\n",
    "            out[j] = 0.0\n",
    "            continue\n",
    "        xv = x[mask] - x[mask].mean()\n",
    "        yv = y[mask] - y[mask].mean()\n",
    "        denom = np.sqrt((xv * xv).sum() * (yv * yv).sum())\n",
    "        out[j] = 0.0 if denom == 0 else float(((xv * yv).sum() / denom) ** 2)\n",
    "    return out\n",
    "\n",
    "def build_blocks_from_adjacent_r2(r2, threshold=0.8, min_snps=5, max_snps=80):\n",
    "    N = len(r2) + 1\n",
    "    cuts = [0]\n",
    "    for j, v in enumerate(r2):\n",
    "        if v < threshold:\n",
    "            cuts.append(j + 1)\n",
    "    cuts.append(N)\n",
    "\n",
    "    blocks = [(cuts[i], cuts[i + 1] - 1) for i in range(len(cuts) - 1)]\n",
    "\n",
    "    merged = []\n",
    "    for s, e in blocks:\n",
    "        if not merged:\n",
    "            merged.append((s, e))\n",
    "        else:\n",
    "            if (e - s + 1) < min_snps:\n",
    "                ps, pe = merged[-1]\n",
    "                merged[-1] = (ps, e)\n",
    "            else:\n",
    "                merged.append((s, e))\n",
    "\n",
    "    final = []\n",
    "    for s, e in merged:\n",
    "        while (e - s + 1) > max_snps:\n",
    "            final.append((s, s + max_snps - 1))\n",
    "            s = s + max_snps\n",
    "        final.append((s, e))\n",
    "    return final\n",
    "\n",
    "def hap_strings_for_block(allele_matrix, start, end):\n",
    "    block = allele_matrix[start:end+1, :]\n",
    "    return [\"\".join(block[:, j].tolist()) for j in range(block.shape[1])]\n",
    "\n",
    "def build_haplotype_histograms_from_pc(region_name, blocks, pc, cohorts, top_k=50):\n",
    "    print(f\"\\n Building haplotype histograms for {region_name} (phased-compatible)\")\n",
    "\n",
    "    phased_region_ids = [str(x) for x in pc[\"sample_ids_sub\"]]\n",
    "    control_ids_full = set(str(x) for x in cohorts[\"control\"][\"sample_ids\"])\n",
    "    control_cols = [j for j, sid in enumerate(phased_region_ids) if sid in control_ids_full]\n",
    "\n",
    "    print(f\"  phased-compatible individuals: {len(phased_region_ids)}\")\n",
    "    print(f\"  control individuals used:      {len(control_cols)}\")\n",
    "\n",
    "    if len(control_cols) == 0:\n",
    "        raise RuntimeError(\n",
    "            f\"No CONTROL individuals overlap phased subset for {region_name}. \"\n",
    "            \"Fix by redoing the cohort split to include UNRELATED phased people, or use only phased people for all cohorts.\"\n",
    "        )\n",
    "\n",
    "    alleles_A = pc[\"alleles_A\"]\n",
    "    alleles_B = pc[\"alleles_B\"]\n",
    "\n",
    "    block_payload = []\n",
    "    for block_id, (s, e) in enumerate(blocks):\n",
    "        hA_all = hap_strings_for_block(alleles_A, s, e)\n",
    "        hB_all = hap_strings_for_block(alleles_B, s, e)\n",
    "\n",
    "        ctr = Counter()\n",
    "        for col in control_cols:\n",
    "            ctr[hA_all[col]] += 1\n",
    "            ctr[hB_all[col]] += 1\n",
    "\n",
    "        total = int(sum(ctr.values()))\n",
    "        top = ctr.most_common(top_k)\n",
    "        top_haps = [h for h, _ in top]\n",
    "        top_counts = [int(c) for _, c in top]\n",
    "        other_count = int(total - sum(top_counts))\n",
    "\n",
    "        block_payload.append({\n",
    "            \"block_id\": int(block_id),\n",
    "            \"start_snp_index\": int(s),\n",
    "            \"end_snp_index\": int(e),\n",
    "            \"num_snps\": int(e - s + 1),\n",
    "            \"total_haplotypes_counted\": total,\n",
    "            \"top_k\": int(top_k),\n",
    "            \"top_haplotypes\": top_haps,\n",
    "            \"top_counts\": top_counts,\n",
    "            \"other_count\": other_count,\n",
    "        })\n",
    "\n",
    "        if block_id % 10 == 0:\n",
    "            top1 = top_counts[0] if top_counts else 0\n",
    "            print(f\"   block {block_id}: SNPs {s}-{e} | unique={len(ctr)} | top1={top1}\")\n",
    "\n",
    "    out = HAP_OUT_DIR / f\"{region_name}.control_haplotypes.phased_compatible.json\"\n",
    "    out.write_text(json.dumps({\n",
    "        \"region_name\": region_name,\n",
    "        \"note\": (\n",
    "            \"Built on intersection of (region SNPs âˆ© phased SNPs) and \"\n",
    "            \"(region individuals âˆ© phased UNRELATED CEU individuals).\"\n",
    "        ),\n",
    "        \"counts_from\": \"CONTROL cohort only, restricted to phased-compatible individuals\",\n",
    "        \"phased_compatible\": {\n",
    "            \"num_individuals_total\": int(len(phased_region_ids)),\n",
    "            \"num_control_individuals_used\": int(len(control_cols)),\n",
    "            \"num_snps_total\": int(len(pc[\"snp_ids_sub\"])),\n",
    "        },\n",
    "        \"blocks\": block_payload\n",
    "    }, indent=2))\n",
    "\n",
    "    print(f\"\\n Saved haplotype histograms â†’ {out.relative_to(PROJECT_ROOT)}\")\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Ensure pc2/pc10 exist (recompute if missing)\n",
    "# -----------------------------\n",
    "for name in [\"r2\", \"r10\", \"cohorts\"]:\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"Missing `{name}`. Run the region+cohorts loading cell first.\")\n",
    "\n",
    "if \"pc2\" not in globals():\n",
    "    df_phase_chr2 = load_phased_df(PHASE_CHR2)\n",
    "    pc2 = make_phased_compatible_region(r2, df_phase_chr2, \"chr2\")\n",
    "    print(\" Created pc2\")\n",
    "else:\n",
    "    print(\" pc2 already exists\")\n",
    "\n",
    "if \"pc10\" not in globals():\n",
    "    df_phase_chr10 = load_phased_df(PHASE_CHR10)\n",
    "    pc10 = make_phased_compatible_region(r10, df_phase_chr10, \"chr10\")\n",
    "    print(\" Created pc10\")\n",
    "else:\n",
    "    print(\" pc10 already exists\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Build BLOCKS ON phased-compatible SNP subset\n",
    "# -----------------------------\n",
    "BLOCK_PARAMS = {\"threshold\": 0.8, \"min_snps\": 5, \"max_snps\": 80}\n",
    "\n",
    "print(\"\\n Building phased-compatible blocks for chr2\")\n",
    "pc2_adj = adjacent_r2_from_G(pc2[\"G_sub\"])\n",
    "blocks_chr2 = build_blocks_from_adjacent_r2(pc2_adj, **BLOCK_PARAMS)\n",
    "print(f\" chr2 phased-compatible SNPs: {pc2['G_sub'].shape[1]} | blocks: {len(blocks_chr2)}\")\n",
    "\n",
    "print(\"\\n Building phased-compatible blocks for chr10\")\n",
    "pc10_adj = adjacent_r2_from_G(pc10[\"G_sub\"])\n",
    "blocks_chr10 = build_blocks_from_adjacent_r2(pc10_adj, **BLOCK_PARAMS)\n",
    "print(f\" chr10 phased-compatible SNPs: {pc10['G_sub'].shape[1]} | blocks: {len(blocks_chr10)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build control haplotype histograms\n",
    "# -----------------------------\n",
    "hap_chr2_out = build_haplotype_histograms_from_pc(\"CEU_chr2_5Mb\", blocks_chr2, pc2, cohorts, top_k=50)\n",
    "hap_chr10_out = build_haplotype_histograms_from_pc(\"CEU_chr10_1Mb\", blocks_chr10, pc10, cohorts, top_k=50)\n",
    "\n",
    "print(\"\\n Done building phased-compatible control haplotype histograms.\")\n",
    "print(\" chr2 â†’\", hap_chr2_out.relative_to(PROJECT_ROOT))\n",
    "print(\" chr10 â†’\", hap_chr10_out.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "296185c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SUMMARY OF WHAT WE NOW HAVE\n",
      "\n",
      "Raw downloads (existing):\n",
      " - data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      " - data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n",
      "\n",
      "Processed regions (aligned CEU):\n",
      " - data/processed/hapmap/regions/CEU_chr2_5Mb.common_with_YRI.npz\n",
      " - data/processed/hapmap/regions/CEU_chr10_1Mb.common_with_YRI.npz\n",
      "\n",
      "New outputs created now:\n",
      " - Blocks chr2: data/processed/hapmap/blocks/CEU_chr2_5Mb.common_with_YRI.blocks.json\n",
      " - Blocks chr10: data/processed/hapmap/blocks/CEU_chr10_1Mb.common_with_YRI.blocks.json\n",
      " - Hap hist chr2: data/processed/hapmap/haplotypes/CEU_chr2_5Mb.control_haplotypes.phased_compatible.json\n",
      " - Hap hist chr10: data/processed/hapmap/haplotypes/CEU_chr10_1Mb.control_haplotypes.phased_compatible.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SUMMARY OF WHAT WE NOW HAVE\")\n",
    "\n",
    "print(\"\\nRaw downloads (existing):\")\n",
    "print(\" -\", (PHASE_CHR2).relative_to(PROJECT_ROOT))\n",
    "print(\" -\", (PHASE_CHR10).relative_to(PROJECT_ROOT))\n",
    "\n",
    "print(\"\\nProcessed regions (aligned CEU):\")\n",
    "print(\" -\", (REGION_CHR2).relative_to(PROJECT_ROOT))\n",
    "print(\" -\", (REGION_CHR10).relative_to(PROJECT_ROOT))\n",
    "\n",
    "print(\"\\nNew outputs created now:\")\n",
    "print(\" - Blocks chr2:\", blocks_chr2_path.relative_to(PROJECT_ROOT))\n",
    "print(\" - Blocks chr10:\", blocks_chr10_path.relative_to(PROJECT_ROOT))\n",
    "print(\" - Hap hist chr2:\", hap_chr2_out.relative_to(PROJECT_ROOT))\n",
    "print(\" - Hap hist chr10:\", hap_chr10_out.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f3b43f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2: phased-compatible SNPs=113, individuals=17\n",
      "   sample_ids_sub=17, snp_ids_sub=113\n",
      "chr10: phased-compatible SNPs=127, individuals=17\n",
      "   sample_ids_sub=17, snp_ids_sub=127\n",
      "\n",
      " Haplotypes output dir: data/processed/hapmap/haplotypes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "HAP_OUT_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\" / \"haplotypes\"\n",
    "HAP_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "required_vars = [\"pc2\", \"pc10\", \"blocks_chr2\", \"blocks_chr10\", \"cohorts\", \"PROJECT_ROOT\"]\n",
    "missing = [v for v in required_vars if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing variables from earlier cells: {missing}\")\n",
    "\n",
    "def _check_pc(pc, name):\n",
    "    A = pc[\"alleles_A\"]; B = pc[\"alleles_B\"]\n",
    "    assert A.shape == B.shape, f\"{name}: A/B shape mismatch\"\n",
    "    n_snps, n_ind = A.shape\n",
    "    print(f\"{name}: phased-compatible SNPs={n_snps}, individuals={n_ind}\")\n",
    "    print(f\"   sample_ids_sub={len(pc['sample_ids_sub'])}, snp_ids_sub={len(pc['snp_ids_sub'])}\")\n",
    "    # Blocks must fit SNP count\n",
    "    max_end = max(e for s, e in (blocks_chr2 if name=='chr2' else blocks_chr10))\n",
    "    if max_end >= n_snps:\n",
    "        raise RuntimeError(f\"{name}: blocks don't fit SNP count (max_end={max_end}, n_snps={n_snps}). Rebuild Cell 3 on pc['G_sub'].\")\n",
    "\n",
    "_check_pc(pc2, \"chr2\")\n",
    "_check_pc(pc10, \"chr10\")\n",
    "\n",
    "print(\"\\n Haplotypes output dir:\", HAP_OUT_DIR.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "135a2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 50\n",
    "\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "def hap_strings_for_block(allele_matrix, start, end):\n",
    "    \"\"\"\n",
    "    allele_matrix: (N_snps, N_individuals) of single-letter alleles (strings)\n",
    "    returns list length N_individuals where each element is the haplotype string over SNPs [start:end]\n",
    "    \"\"\"\n",
    "    block = allele_matrix[start:end+1, :]  \n",
    "    return [\"\".join(block[:, j].tolist()) for j in range(block.shape[1])]\n",
    "\n",
    "def compute_control_cols_in_pc(pc, cohorts):\n",
    "    \"\"\"\n",
    "    pc has sample_ids_sub = individuals in phased-compatible subset (same order as allele columns).\n",
    "    cohorts['control']['sample_ids'] are the control IDs from full CEU (unphased split).\n",
    "    Return list of column indices in pc corresponding to CONTROL individuals.\n",
    "    \"\"\"\n",
    "    pc_ids = [str(x) for x in pc[\"sample_ids_sub\"]]\n",
    "    control_set = set(str(x) for x in cohorts[\"control\"][\"sample_ids\"])\n",
    "    control_cols = [j for j, sid in enumerate(pc_ids) if sid in control_set]\n",
    "    return control_cols\n",
    "\n",
    "def save_control_haplotype_histograms(region_name, pc, blocks, cohorts, top_k=TOP_K):\n",
    "    \"\"\"\n",
    "    Writes:\n",
    "      data/processed/hapmap/haplotypes/{region_name}.control_haplotypes.phased_compatible.json\n",
    "\n",
    "    Counts are over haplotypes => 2 per person (A and B haplotypes).\n",
    "    Uses CONTROL only (public reference), restricted to phased-compatible individuals.\n",
    "    \"\"\"\n",
    "    A = pc[\"alleles_A\"]\n",
    "    B = pc[\"alleles_B\"]\n",
    "    snp_ids = [str(x) for x in pc[\"snp_ids_sub\"]]\n",
    "    positions = [int(x) for x in pc[\"positions_sub\"]]\n",
    "    minor_alleles = pc.get(\"minor_alleles_sub\")\n",
    "    pc_ids = [str(x) for x in pc[\"sample_ids_sub\"]]\n",
    "\n",
    "    control_cols = compute_control_cols_in_pc(pc, cohorts)\n",
    "    if len(control_cols) == 0:\n",
    "        raise RuntimeError(\n",
    "            f\"{region_name}: No CONTROL individuals overlap with phased-compatible subset. \"\n",
    "            \"This can happen depending on your split + UNRELATED set.\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\n [{region_name}] CONTROL haplotype histograms (phased-compatible)\")\n",
    "    print(f\" phased-compatible individuals total: {len(pc_ids)}\")\n",
    "    print(f\" control individuals used: {len(control_cols)} (=> {2*len(control_cols)} haplotypes)\")\n",
    "    print(f\" SNPs used: {A.shape[0]}\")\n",
    "\n",
    "    block_payload = []\n",
    "    for block_id, (s, e) in enumerate(blocks):\n",
    "\n",
    "        hA_all = hap_strings_for_block(A, s, e)\n",
    "        hB_all = hap_strings_for_block(B, s, e)\n",
    "\n",
    "        ctr = Counter()\n",
    "        for col in control_cols:\n",
    "            ctr[hA_all[col]] += 1\n",
    "            ctr[hB_all[col]] += 1\n",
    "\n",
    "        total = int(sum(ctr.values()))\n",
    "        top = ctr.most_common(top_k)\n",
    "        top_haps = [h for h, _ in top]\n",
    "        top_counts = [int(c) for _, c in top]\n",
    "        other_count = int(total - sum(top_counts))\n",
    "\n",
    "\n",
    "        top_set = set(top_haps)\n",
    "\n",
    "        # OTHER minor allele profile (public, CEU control only)\n",
    "        block_len = int(e - s + 1)\n",
    "        other_minor_frac = []\n",
    "        if other_count > 0 and minor_alleles is not None:\n",
    "            minor_block = [str(x) for x in minor_alleles[s:e+1]]\n",
    "            other_minor_counts = [0] * block_len\n",
    "            for hap, cnt in ctr.items():\n",
    "                if hap in top_set:\n",
    "                    continue\n",
    "                for j, allele in enumerate(hap):\n",
    "                    if allele == minor_block[j]:\n",
    "                        other_minor_counts[j] += cnt\n",
    "            other_minor_frac = [c / other_count for c in other_minor_counts]\n",
    "        else:\n",
    "            # fallback: use control minor allele freq at each SNP\n",
    "            if minor_alleles is not None and len(control_cols) > 0:\n",
    "                minor_block = np.array([str(x) for x in minor_alleles[s:e+1]], dtype=object)\n",
    "                A_block = A[s:e+1, control_cols]\n",
    "                B_block = B[s:e+1, control_cols]\n",
    "                total_haps = 2 * len(control_cols)\n",
    "                if total_haps > 0:\n",
    "                    minor_counts = (A_block == minor_block[:, None]).sum(axis=1) + (B_block == minor_block[:, None]).sum(axis=1)\n",
    "                    other_minor_frac = (minor_counts / total_haps).tolist()\n",
    "                else:\n",
    "                    other_minor_frac = [0.0] * block_len\n",
    "            else:\n",
    "                other_minor_frac = [0.0] * block_len\n",
    "\n",
    "        block_payload.append({\n",
    "            \"block_id\": int(block_id),\n",
    "            \"start_snp_index\": int(s),\n",
    "            \"end_snp_index\": int(e),\n",
    "            \"num_snps\": int(e - s + 1),\n",
    "            \"start_pos\": int(positions[s]),\n",
    "            \"end_pos\": int(positions[e]),\n",
    "            \"total_haplotypes_counted\": total,  \n",
    "            \"top_k\": int(top_k),\n",
    "            \"top_haplotypes\": top_haps,\n",
    "            \"top_counts\": top_counts,\n",
    "            \"other_count\": other_count,\n",
    "            \"other_minor_frac\": other_minor_frac,\n",
    "            \"top_haplotypes_reduced\": top_haps,\n",
    "        })\n",
    "\n",
    "        if block_id % 10 == 0:\n",
    "            print(f\"   block {block_id}: SNPs {s}-{e} | unique={len(ctr)} | top1={top_counts[0] if top_counts else 0}\")\n",
    "\n",
    "    out_path = HAP_OUT_DIR / f\"{region_name}.control_haplotypes.phased_compatible.json\"\n",
    "    payload = {\n",
    "        \"region_name\": region_name,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"source\": \"HapMap3 r2 phased (CEU UNRELATED) + CEU control split from unphased genotypes\",\n",
    "        \"counts_from\": \"CONTROL cohort only (public reference), restricted to phased-compatible subset\",\n",
    "        \"phased_compatible\": {\n",
    "            \"num_snps\": int(A.shape[0]),\n",
    "            \"num_individuals_total\": int(A.shape[1]),\n",
    "            \"num_control_individuals_used\": int(len(control_cols)),\n",
    "        },\n",
    "        \"note\": \"Counts are over haplotypes (2 per person: A and B).\",\n",
    "        \"blocks\": block_payload,\n",
    "    }\n",
    "    out_path.write_text(json.dumps(payload, indent=2))\n",
    "    print(f\"\\n Saved â†’ {out_path.relative_to(PROJECT_ROOT)}\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f802b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [CEU_chr2_5Mb.common_with_YRI] CONTROL haplotype histograms (phased-compatible)\n",
      " phased-compatible individuals total: 17\n",
      " control individuals used: 11 (=> 22 haplotypes)\n",
      " SNPs used: 113\n",
      "   block 0: SNPs 0-79 | unique=22 | top1=1\n",
      "\n",
      " Saved â†’ data/processed/hapmap/haplotypes/CEU_chr2_5Mb.common_with_YRI.control_haplotypes.phased_compatible.json\n",
      "\n",
      " [CEU_chr10_1Mb.common_with_YRI] CONTROL haplotype histograms (phased-compatible)\n",
      " phased-compatible individuals total: 17\n",
      " control individuals used: 11 (=> 22 haplotypes)\n",
      " SNPs used: 127\n",
      "   block 0: SNPs 0-79 | unique=9 | top1=8\n",
      "\n",
      " Saved â†’ data/processed/hapmap/haplotypes/CEU_chr10_1Mb.common_with_YRI.control_haplotypes.phased_compatible.json\n",
      "\n",
      " Preprocessing milestone complete for Method 2.\n",
      "chr2: data/processed/hapmap/haplotypes/CEU_chr2_5Mb.common_with_YRI.control_haplotypes.phased_compatible.json\n",
      "chr10: data/processed/hapmap/haplotypes/CEU_chr10_1Mb.common_with_YRI.control_haplotypes.phased_compatible.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hap_chr2_out = save_control_haplotype_histograms(\n",
    "    region_name=\"CEU_chr2_5Mb.common_with_YRI\",\n",
    "    pc=pc2,\n",
    "    blocks=blocks_chr2,\n",
    "    cohorts=cohorts,\n",
    "    top_k=TOP_K\n",
    ")\n",
    "\n",
    "hap_chr10_out = save_control_haplotype_histograms(\n",
    "    region_name=\"CEU_chr10_1Mb.common_with_YRI\",\n",
    "    pc=pc10,\n",
    "    blocks=blocks_chr10,\n",
    "    cohorts=cohorts,\n",
    "    top_k=TOP_K\n",
    ")\n",
    "\n",
    "print(\"\\n Preprocessing milestone complete for Method 2.\")\n",
    "print(\"chr2:\", hap_chr2_out.relative_to(PROJECT_ROOT))\n",
    "print(\"chr10:\", hap_chr10_out.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c52aa4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Rebuilding CONTROL haplotype histograms using the NEW cohort split...\n",
      "\n",
      " [CEU_chr2_5Mb.common_with_YRI] CONTROL haplotype histograms (phased-compatible)\n",
      " phased-compatible individuals total: 17\n",
      " control individuals used: 11 (=> 22 haplotypes)\n",
      " SNPs used: 113\n",
      "   block 0: SNPs 0-79 | unique=22 | top1=1\n",
      "\n",
      " Saved â†’ data/processed/hapmap/haplotypes/CEU_chr2_5Mb.common_with_YRI.control_haplotypes.phased_compatible.json\n",
      "\n",
      " [CEU_chr10_1Mb.common_with_YRI] CONTROL haplotype histograms (phased-compatible)\n",
      " phased-compatible individuals total: 17\n",
      " control individuals used: 11 (=> 22 haplotypes)\n",
      " SNPs used: 127\n",
      "   block 0: SNPs 0-79 | unique=9 | top1=8\n",
      "\n",
      " Saved â†’ data/processed/hapmap/haplotypes/CEU_chr10_1Mb.common_with_YRI.control_haplotypes.phased_compatible.json\n",
      "\n",
      " Updated control haplotype histograms:\n",
      "chr2 â†’ data/processed/hapmap/haplotypes/CEU_chr2_5Mb.common_with_YRI.control_haplotypes.phased_compatible.json\n",
      "chr10 â†’ data/processed/hapmap/haplotypes/CEU_chr10_1Mb.common_with_YRI.control_haplotypes.phased_compatible.json\n"
     ]
    }
   ],
   "source": [
    "# REQUIREMENTS:\n",
    "# save_control_haplotype_histograms must exist (you created it earlier)\n",
    "# blocks_chr2, blocks_chr10 must exist\n",
    "# pc2, pc10 must exist\n",
    "\n",
    "needed = [\"save_control_haplotype_histograms\", \"blocks_chr2\", \"blocks_chr10\", \"pc2\", \"pc10\"]\n",
    "missing = [x for x in needed if x not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required variables/functions: {missing}\")\n",
    "\n",
    "print(\"\\n Rebuilding CONTROL haplotype histograms using the NEW cohort split...\")\n",
    "\n",
    "out_chr2 = save_control_haplotype_histograms(\n",
    "    region_name=\"CEU_chr2_5Mb.common_with_YRI\",\n",
    "    pc=pc2,\n",
    "    blocks=blocks_chr2,\n",
    "    cohorts=cohorts,\n",
    "    top_k=TOP_K\n",
    ")\n",
    "\n",
    "out_chr10 = save_control_haplotype_histograms(\n",
    "    region_name=\"CEU_chr10_1Mb.common_with_YRI\",\n",
    "    pc=pc10,\n",
    "    blocks=blocks_chr10,\n",
    "    cohorts=cohorts,\n",
    "    top_k=TOP_K\n",
    ")\n",
    "\n",
    "print(\"\\n Updated control haplotype histograms:\")\n",
    "print(\"chr2 â†’\", out_chr2.relative_to(PROJECT_ROOT))\n",
    "print(\"chr10 â†’\", out_chr10.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "061b5190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FILE EXISTENCE CHECK ===\n",
      "âœ… data/raw/hapmap/genotypes/genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\n",
      "âœ… data/raw/hapmap/genotypes/genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\n",
      "âœ… data/raw/hapmap/genotypes/genotypes_chr2_YRI_r27_nr.b36_fwd.txt.gz\n",
      "âœ… data/raw/hapmap/genotypes/genotypes_chr10_YRI_r27_nr.b36_fwd.txt.gz\n",
      "âœ… data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\n",
      "âœ… data/raw/hapmap/phasing/HapMap3_r2/CEU/UNRELATED/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\n",
      "âœ… data/processed/hapmap/regions/CEU_chr2_5Mb.common_with_YRI.npz\n",
      "âœ… data/processed/hapmap/regions/CEU_chr10_1Mb.common_with_YRI.npz\n",
      "âœ… data/processed/hapmap/regions/YRI_chr2_5Mb.common_with_CEU.npz\n",
      "âœ… data/processed/hapmap/regions/YRI_chr10_1Mb.common_with_CEU.npz\n",
      "âœ… data/processed/hapmap/cohorts/hapmap_CEU_control_test__YRI_case.json\n",
      "âœ… data/processed/hapmap/blocks/CEU_chr2_5Mb.common_with_YRI.blocks.json\n",
      "âœ… data/processed/hapmap/blocks/CEU_chr10_1Mb.common_with_YRI.blocks.json\n",
      "âœ… data/processed/hapmap/haplotypes/CEU_chr2_5Mb.common_with_YRI.control_haplotypes.phased_compatible.json\n",
      "âœ… data/processed/hapmap/haplotypes/CEU_chr10_1Mb.common_with_YRI.control_haplotypes.phased_compatible.json\n",
      "=== COHORT SPLIT CHECK ===\n",
      "control/case/test = 120 176 54\n",
      "has indices_in_ceu_matrix? True True\n",
      "has indices_in_yri_matrix? True\n",
      "FINAL: âœ… preprocessing outputs are complete\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_HAPMAP_DIR = DATA_DIR / \"raw\" / \"hapmap\"\n",
    "PROC_DIR = DATA_DIR / \"processed\" / \"hapmap\"\n",
    "\n",
    "GENO_DIR = RAW_HAPMAP_DIR / \"genotypes\"\n",
    "PHASE_DIR = RAW_HAPMAP_DIR / \"phasing\" / \"HapMap3_r2\"\n",
    "\n",
    "REGION_DIR = PROC_DIR / \"regions\"\n",
    "COHORT_DIR = PROC_DIR / \"cohorts\"\n",
    "BLOCK_DIR  = PROC_DIR / \"blocks\"\n",
    "HAP_DIR    = PROC_DIR / \"haplotypes\"\n",
    "\n",
    "expected = [\n",
    "    # raw downloads\n",
    "    GENO_DIR / \"genotypes_chr2_CEU_r27_nr.b36_fwd.txt.gz\",\n",
    "    GENO_DIR / \"genotypes_chr10_CEU_r27_nr.b36_fwd.txt.gz\",\n",
    "    GENO_DIR / \"genotypes_chr2_YRI_r27_nr.b36_fwd.txt.gz\",\n",
    "    GENO_DIR / \"genotypes_chr10_YRI_r27_nr.b36_fwd.txt.gz\",\n",
    "    PHASE_DIR / \"CEU\" / \"UNRELATED\" / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_ceu.unr.phased.gz\",\n",
    "    PHASE_DIR / \"CEU\" / \"UNRELATED\" / \"hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_ceu.unr.phased.gz\",\n",
    "\n",
    "    # processed regions\n",
    "    REGION_DIR / \"CEU_chr2_5Mb.common_with_YRI.npz\",\n",
    "    REGION_DIR / \"CEU_chr10_1Mb.common_with_YRI.npz\",\n",
    "    REGION_DIR / \"YRI_chr2_5Mb.common_with_CEU.npz\",\n",
    "    REGION_DIR / \"YRI_chr10_1Mb.common_with_CEU.npz\",\n",
    "\n",
    "    # cohort split\n",
    "    COHORT_DIR / \"hapmap_CEU_control_test__YRI_case.json\",\n",
    "\n",
    "    # blocks + haplotype outputs (names may vary slightly; adjust if yours differ)\n",
    "    BLOCK_DIR / \"CEU_chr2_5Mb.common_with_YRI.blocks.json\",\n",
    "    BLOCK_DIR / \"CEU_chr10_1Mb.common_with_YRI.blocks.json\",\n",
    "    HAP_DIR / \"CEU_chr2_5Mb.common_with_YRI.control_haplotypes.phased_compatible.json\",\n",
    "    HAP_DIR / \"CEU_chr10_1Mb.common_with_YRI.control_haplotypes.phased_compatible.json\",\n",
    "]\n",
    "\n",
    "print(\"=== FILE EXISTENCE CHECK ===\")\n",
    "ok = True\n",
    "for p in expected:\n",
    "    exists = p.exists()\n",
    "    ok &= exists\n",
    "    print((\"âœ…\" if exists else \"âŒ\"), p.relative_to(PROJECT_ROOT))\n",
    "\n",
    "print(\"=== COHORT SPLIT CHECK ===\")\n",
    "coh_path = COHORT_DIR / \"hapmap_CEU_control_test__YRI_case.json\"\n",
    "if coh_path.exists():\n",
    "    cohorts = json.loads(coh_path.read_text())\n",
    "    n_control = len(cohorts[\"control\"][\"sample_ids\"])\n",
    "    n_case    = len(cohorts[\"case\"][\"sample_ids\"])\n",
    "    n_test    = len(cohorts[\"test\"][\"sample_ids\"])\n",
    "    print(\"control/case/test =\", n_control, n_case, n_test)\n",
    "\n",
    "    print(\"has indices_in_ceu_matrix?\",\n",
    "          \"indices_in_ceu_matrix\" in cohorts[\"control\"],\n",
    "          \"indices_in_ceu_matrix\" in cohorts[\"test\"])\n",
    "    print(\"has indices_in_yri_matrix?\",\n",
    "          \"indices_in_yri_matrix\" in cohorts[\"case\"])\n",
    "else:\n",
    "    ok = False\n",
    "    print(\"âŒ Missing cohort split JSON.\")\n",
    "\n",
    "print(\"FINAL:\", \"âœ… preprocessing outputs are complete\" if ok else \"âŒ something is missing above\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "438ba894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… YRI genotypes chr2: data/raw/hapmap/genotypes/genotypes_chr2_YRI_r27_nr.b36_fwd.txt.gz\n",
      "âœ… YRI genotypes chr10: data/raw/hapmap/genotypes/genotypes_chr10_YRI_r27_nr.b36_fwd.txt.gz\n",
      "âœ… YRI region chr2: data/processed/hapmap/regions/YRI_chr2_5Mb.npz\n",
      "âœ… YRI region chr10: data/processed/hapmap/regions/YRI_chr10_1Mb.npz\n",
      "âœ… YRI phased resolved chr2: data/raw/hapmap/phasing/HapMap3_r2/YRI/TRIOS/hapmap3_r2_b36_fwd.consensus.qc.poly.chr2_yri.phased.gz\n",
      "âœ… YRI phased resolved chr10: data/raw/hapmap/phasing/HapMap3_r2/YRI/TRIOS/hapmap3_r2_b36_fwd.consensus.qc.poly.chr10_yri.phased.gz\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# FILE EXISTENCE CHECK (YRI)\n",
    "# -----------------------------\n",
    "import json\n",
    "\n",
    "\n",
    "def _check(path: Path, label: str):\n",
    "    ok = path.exists()\n",
    "    status = \"âœ…\" if ok else \"âŒ\"\n",
    "    print(f\"{status} {label}: {path.relative_to(PROJECT_ROOT)}\")\n",
    "\n",
    "_check(GENO_DIR / \"genotypes_chr2_YRI_r27_nr.b36_fwd.txt.gz\", \"YRI genotypes chr2\")\n",
    "_check(GENO_DIR / \"genotypes_chr10_YRI_r27_nr.b36_fwd.txt.gz\", \"YRI genotypes chr10\")\n",
    "_check(REGION_DIR / \"YRI_chr2_5Mb.npz\", \"YRI region chr2\")\n",
    "_check(REGION_DIR / \"YRI_chr10_1Mb.npz\", \"YRI region chr10\")\n",
    "\n",
    "YRI_PHASED_JSON = PROC_DIR / \"phasing\" / \"yri_phased_paths.json\"\n",
    "if YRI_PHASED_JSON.exists():\n",
    "    info = json.loads(YRI_PHASED_JSON.read_text())\n",
    "    for chrom in [\"chr2\", \"chr10\"]:\n",
    "        p = Path(info[chrom][\"local_path\"])\n",
    "        _check(p, f\"YRI phased resolved {chrom}\")\n",
    "else:\n",
    "    print(\"âŒ Missing YRI phased path JSON:\", YRI_PHASED_JSON.relative_to(PROJECT_ROOT))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
