{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3: Local DP via GRR (per-individual single-SNP reporting)\n",
    "\n",
    "Implements the proposal's LDP GRR mechanism where each YRI case individual reports a single SNP genotype (0/1/2) with generalized randomized response. The server debiases per-SNP genotype distributions to estimate case MAF.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# Robust PROJECT_ROOT detection\n",
    "# -------------------------\n",
    "def find_project_root(start=None) -> Path:\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for parent in [p] + list(p.parents):\n",
    "        if (parent / \".git\").exists() or (parent / \"requirements.txt\").exists():\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "PROC_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"hapmap\"\n",
    "REGIONS_DIR = PROC_DIR / \"regions\"\n",
    "COHORTS_PATH = PROC_DIR / \"cohorts\" / \"hapmap_CEU_control_test__YRI_case.json\n",
    "\n",
    "DERIVED_DIR = PROJECT_ROOT / \"data\" / \"derived\" / \"method3\"\n",
    "FIG_DIR = PROJECT_ROOT / \"results\" / \"figures\" / \"method3\"\n",
    "TABLE_DIR = PROJECT_ROOT / \"results\" / \"tables\" / \"method3\"\n",
    "\n",
    "for d in [DERIVED_DIR, FIG_DIR, TABLE_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "EPS_LIST = [0.03, 0.1, 0.3, 1.0, 3.0, 10.0]\n",
    "SEED_LIST = list(range(10))\n",
    "PVALUE_CUTOFF = 1e-3\n",
    "ALPHA_TEST_EXCEED = 0.05\n",
    "P_CLIP = 1e-6\n",
    "REPORT_K = 1\n",
    "\n",
    "REGION_FILES = {\n",
    "    \"chr2_5Mb\": {\n",
    "        \"ceu\": REGIONS_DIR / \"CEU_chr2_5Mb.common_with_YRI.npz\",\n",
    "        \"yri\": REGIONS_DIR / \"YRI_chr2_5Mb.common_with_CEU.npz\",\n",
    "    },\n",
    "    \"chr10_1Mb\": {\n",
    "        \"ceu\": REGIONS_DIR / \"CEU_chr10_1Mb.common_with_YRI.npz\",\n",
    "        \"yri\": REGIONS_DIR / \"YRI_chr10_1Mb.common_with_CEU.npz\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "print(\"COHORTS_PATH =\", COHORTS_PATH)\n",
    "print(\"EPS_LIST =\", EPS_LIST)\n",
    "print(\"SEED_LIST =\", SEED_LIST)\n",
    "print(\"REPORT_K =\", REPORT_K)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Load regions + cohorts\n",
    "# -------------------------\n",
    "def load_region_npz(path: Path) -> Dict[str, np.ndarray]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing region file: {path}\")\n",
    "    z = np.load(path, allow_pickle=True)\n",
    "    return {k: z[k] for k in z.files}\n",
    "\n",
    "def _pick_genotype_matrix(region: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    for key in [\"G\", \"G_sub\", \"genotypes\", \"X\"]:\n",
    "        if key in region:\n",
    "            G = region[key]\n",
    "            if isinstance(G, np.ndarray) and G.ndim == 2:\n",
    "                return G\n",
    "    for k, v in region.items():\n",
    "        if isinstance(v, np.ndarray) and v.ndim == 2:\n",
    "            return v\n",
    "    raise ValueError(\"No 2D genotype matrix found in region npz.\")\n",
    "\n",
    "def _ensure_snps_by_individuals(G: np.ndarray) -> np.ndarray:\n",
    "    if G.shape[0] < G.shape[1]:\n",
    "        return G.T\n",
    "    return G\n",
    "\n",
    "regions = {}\n",
    "for region_tag, paths in REGION_FILES.items():\n",
    "    regions[region_tag] = {\n",
    "        \"ceu\": load_region_npz(paths[\"ceu\"]),\n",
    "        \"yri\": load_region_npz(paths[\"yri\"]),\n",
    "    }\n",
    "    print(f\"Loaded {region_tag} CEU: {paths['ceu'].relative_to(PROJECT_ROOT)}\")\n",
    "    print(f\"Loaded {region_tag} YRI: {paths['yri'].relative_to(PROJECT_ROOT)}\")\n",
    "\n",
    "cohorts = json.loads(COHORTS_PATH.read_text())\n",
    "for k in [\"control\", \"test\", \"case\"]:\n",
    "    if k not in cohorts:\n",
    "        raise KeyError(f\"Missing '{k}' in cohorts JSON\")\n",
    "\n",
    "ceu_control_idx = list(cohorts[\"control\"][\"indices_in_ceu_matrix\"])\n",
    "yri_case_idx_all = list(cohorts[\"case\"][\"indices_in_yri_matrix\"])\n",
    "\n",
    "print(\"CEU control idx count:\", len(ceu_control_idx))\n",
    "print(\"YRI case idx count (pool):\", len(yri_case_idx_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def compute_counts_and_maf(G_snps_x_ind: np.ndarray, cols: List[int]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    sub = G_snps_x_ind[:, cols]\n",
    "    called = sub != -1\n",
    "    called_n = called.sum(axis=1).astype(np.int32)\n",
    "    sub_nonmiss = np.where(called, sub, 0)\n",
    "    mac = sub_nonmiss.sum(axis=1).astype(np.float64)\n",
    "    denom = 2.0 * np.maximum(called_n, 1)\n",
    "    maf = mac / denom\n",
    "    return called_n, mac, maf\n",
    "\n",
    "def noisy_counts_from_maf(maf: np.ndarray, called_n: np.ndarray) -> np.ndarray:\n",
    "    raw = np.rint(maf * (2.0 * called_n))\n",
    "    return np.clip(raw, 0.0, 2.0 * called_n).astype(np.float64)\n",
    "\n",
    "def chisq_pvalues_case_vs_control(mac_case: np.ndarray, called_case: np.ndarray,\n",
    "                                 mac_ctrl: np.ndarray, called_ctrl: np.ndarray) -> np.ndarray:\n",
    "    n = mac_case.shape[0]\n",
    "    pvals = np.ones(n, dtype=np.float64)\n",
    "    for j in range(n):\n",
    "        minor_case = mac_case[j]\n",
    "        major_case = 2.0 * called_case[j] - minor_case\n",
    "        minor_ctrl = mac_ctrl[j]\n",
    "        major_ctrl = 2.0 * called_ctrl[j] - minor_ctrl\n",
    "        if called_case[j] <= 0 or called_ctrl[j] <= 0:\n",
    "            pvals[j] = 1.0\n",
    "            continue\n",
    "        if major_case < 0 or major_ctrl < 0:\n",
    "            pvals[j] = 1.0\n",
    "            continue\n",
    "        table = np.array([[minor_case, major_case], [minor_ctrl, major_ctrl]], dtype=np.float64)\n",
    "        if np.any(table.sum(axis=1) <= 0) or np.any(table.sum(axis=0) <= 0):\n",
    "            pvals[j] = 1.0\n",
    "            continue\n",
    "        try:\n",
    "            _, p, _, _ = chi2_contingency(table, correction=False)\n",
    "            pvals[j] = float(p)\n",
    "        except Exception:\n",
    "            pvals[j] = 1.0\n",
    "    return pvals\n",
    "\n",
    "def overlap_metrics(sig_true: np.ndarray, sig_noisy: np.ndarray) -> Dict[str, float]:\n",
    "    a = sig_true.astype(bool)\n",
    "    b = sig_noisy.astype(bool)\n",
    "    tp = int(np.sum(a & b))\n",
    "    fp = int(np.sum(~a & b))\n",
    "    fn = int(np.sum(a & ~b))\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    jaccard = tp / (tp + fp + fn) if (tp + fp + fn) else 0.0\n",
    "    return {\n",
    "        \"true_sig\": int(np.sum(a)),\n",
    "        \"noisy_sig\": int(np.sum(b)),\n",
    "        \"tp\": tp,\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"jaccard\": float(jaccard),\n",
    "    }\n",
    "\n",
    "def genotype_log_prob_hwe(g: np.ndarray, p: np.ndarray) -> np.ndarray:\n",
    "    p = np.clip(p, P_CLIP, 1.0 - P_CLIP)\n",
    "    q = 1.0 - p\n",
    "    logP0 = 2.0 * np.log(q)\n",
    "    logP1 = np.log(2.0) + np.log(p) + np.log(q)\n",
    "    logP2 = 2.0 * np.log(p)\n",
    "    out = np.zeros_like(p, dtype=np.float64)\n",
    "    out = np.where(g == 0, logP0, out)\n",
    "    out = np.where(g == 1, logP1, out)\n",
    "    out = np.where(g == 2, logP2, out)\n",
    "    return out\n",
    "\n",
    "def lr_scores(G_snps_x_ind: np.ndarray, cols: List[int], p_case: np.ndarray, p_ctrl: np.ndarray) -> np.ndarray:\n",
    "    G = G_snps_x_ind[:, cols]\n",
    "    n_ind = G.shape[1]\n",
    "    scores = np.zeros(n_ind, dtype=np.float64)\n",
    "    for i in range(n_ind):\n",
    "        g = G[:, i]\n",
    "        mask = g >= 0\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        log_case = genotype_log_prob_hwe(g[mask], p_case[mask])\n",
    "        log_ctrl = genotype_log_prob_hwe(g[mask], p_ctrl[mask])\n",
    "        scores[i] = float(np.sum(log_case - log_ctrl))\n",
    "    return scores\n",
    "\n",
    "def split_train_test(indices: List[int], seed: int, frac_train: float = 0.7) -> Tuple[List[int], List[int]]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    shuffled = list(rng.permutation(indices))\n",
    "    n = len(shuffled)\n",
    "    n_train = int(round(n * frac_train))\n",
    "    n_train = max(1, min(n - 1, n_train))\n",
    "    return shuffled[:n_train], shuffled[n_train:]\n",
    "\n",
    "def grr_probs(eps: float, k: int = 3) -> Tuple[float, float]:\n",
    "    ee = np.exp(eps)\n",
    "    p = ee / (ee + k - 1)\n",
    "    q = (1.0 - p) / (k - 1)\n",
    "    return p, q\n",
    "\n",
    "def ldp_grr_estimate_maf(G_snps_x_ind: np.ndarray, train_idx: List[int], eps: float, rng: np.random.Generator,\n",
    "                         report_k: int = 1, fallback_maf: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray, float, float, int]:\n",
    "    # Returns: noisy_maf, report_counts_per_snp, p, q, zero_reports\n",
    "    Gsub = G_snps_x_ind[:, train_idx]\n",
    "    n_snps, n_ind = Gsub.shape\n",
    "    p, q = grr_probs(eps, k=3)\n",
    "    counts = np.zeros((n_snps, 3), dtype=np.int32)\n",
    "    report_counts = np.zeros(n_snps, dtype=np.int32)\n",
    "\n",
    "    k = min(report_k, n_snps)\n",
    "    for i in range(n_ind):\n",
    "        snps = rng.choice(n_snps, size=k, replace=False)\n",
    "        for j in snps:\n",
    "            g = int(Gsub[j, i])\n",
    "            if g < 0:\n",
    "                continue\n",
    "            report_counts[j] += 1\n",
    "            if rng.random() < p:\n",
    "                r = g\n",
    "            else:\n",
    "                others = [0, 1, 2]\n",
    "                others.remove(g)\n",
    "                r = others[rng.integers(0, 2)]\n",
    "            counts[j, r] += 1\n",
    "\n",
    "    noisy_maf = np.zeros(n_snps, dtype=np.float64)\n",
    "    zero_reports = 0\n",
    "    for j in range(n_snps):\n",
    "        m = report_counts[j]\n",
    "        if m <= 0:\n",
    "            zero_reports += 1\n",
    "            noisy_maf[j] = fallback_maf[j] if fallback_maf is not None else 0.5\n",
    "            continue\n",
    "        f_hat = counts[j].astype(np.float64) / float(m)\n",
    "        est = (f_hat - q) / (p - q)\n",
    "        est = np.clip(est, 0.0, 1.0)\n",
    "        s = est.sum()\n",
    "        if s <= 0:\n",
    "            est = np.array([1/3, 1/3, 1/3], dtype=np.float64)\n",
    "        else:\n",
    "            est = est / s\n",
    "        noisy_maf[j] = (est[1] + 2.0 * est[2]) / 2.0\n",
    "\n",
    "    noisy_maf = np.clip(noisy_maf, 0.0, 1.0)\n",
    "    return noisy_maf, report_counts, p, q, zero_reports\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Main loop: Method 3 (LDP-GRR)\n",
    "# -------------------------\n",
    "def eps_dir(eps: float) -> Path:\n",
    "    s = str(eps).replace(\".\", \"p\")\n",
    "    return DERIVED_DIR / f\"eps_{s}\"\n",
    "\n",
    "records = []\n",
    "hist_cache = {}\n",
    "\n",
    "for region_tag, region_pair in regions.items():\n",
    "    G_ceu = _ensure_snps_by_individuals(_pick_genotype_matrix(region_pair[\"ceu\"]))\n",
    "    G_yri = _ensure_snps_by_individuals(_pick_genotype_matrix(region_pair[\"yri\"]))\n",
    "\n",
    "    n_snps, n_ceu = G_ceu.shape\n",
    "    n_snps_yri, n_yri = G_yri.shape\n",
    "    if n_snps != n_snps_yri:\n",
    "        raise RuntimeError(f\"SNP count mismatch in {region_tag}: CEU={n_snps}, YRI={n_snps_yri}\")\n",
    "\n",
    "    ceu_ctrl_idx = [i for i in ceu_control_idx if i < n_ceu]\n",
    "    yri_pool = [i for i in yri_case_idx_all if i < n_yri]\n",
    "\n",
    "    if len(ceu_ctrl_idx) == 0:\n",
    "        raise RuntimeError(f\"No CEU control indices for {region_tag}\")\n",
    "    if len(yri_pool) == 0:\n",
    "        raise RuntimeError(f\"No YRI case indices for {region_tag}\")\n",
    "\n",
    "    print(f\"[{region_tag}] n_snps={n_snps} | CEU ctrl={len(ceu_ctrl_idx)} | YRI pool={len(yri_pool)}\")\n",
    "\n",
    "    # Control MAF (public reference)\n",
    "    called_ctrl, mac_ctrl, maf_ctrl = compute_counts_and_maf(G_ceu, ceu_ctrl_idx)\n",
    "\n",
    "    for eps in EPS_LIST:\n",
    "        outdir = eps_dir(eps)\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for seed in SEED_LIST:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            yri_train_idx, yri_test_idx = split_train_test(yri_pool, seed=seed, frac_train=0.7)\n",
    "\n",
    "            # True case from YRI train\n",
    "            called_case, mac_case, maf_case = compute_counts_and_maf(G_yri, yri_train_idx)\n",
    "\n",
    "            # LDP-GRR release\n",
    "            noisy_maf, report_counts, p_grr, q_grr, zero_reports = ldp_grr_estimate_maf(\n",
    "                G_yri, yri_train_idx, eps, rng, report_k=REPORT_K, fallback_maf=maf_ctrl\n",
    "            )\n",
    "            noisy_mac_safe = noisy_counts_from_maf(noisy_maf, called_case)\n",
    "\n",
    "            # Privacy: LR scores within YRI\n",
    "            scores_member = lr_scores(G_yri, yri_train_idx, p_case=noisy_maf, p_ctrl=maf_ctrl)\n",
    "            scores_nonmember = lr_scores(G_yri, yri_test_idx, p_case=noisy_maf, p_ctrl=maf_ctrl)\n",
    "            thresh = float(np.quantile(scores_nonmember, 1.0 - ALPHA_TEST_EXCEED))\n",
    "            lr_power = float(np.mean(scores_member >= thresh))\n",
    "\n",
    "            # Utility: GWAS overlap (true vs noisy)\n",
    "            p_true = chisq_pvalues_case_vs_control(mac_case, called_case, mac_ctrl, called_ctrl)\n",
    "            p_noisy = chisq_pvalues_case_vs_control(noisy_mac_safe, called_case, mac_ctrl, called_ctrl)\n",
    "            sig_true = p_true <= PVALUE_CUTOFF\n",
    "            sig_noisy = p_noisy <= PVALUE_CUTOFF\n",
    "            ov = overlap_metrics(sig_true, sig_noisy)\n",
    "\n",
    "            rmse = float(np.sqrt(np.mean((noisy_maf - maf_case) ** 2)))\n",
    "            maf_mae = float(np.mean(np.abs(noisy_maf - maf_case)))\n",
    "            corr = float(np.corrcoef(noisy_maf, maf_case)[0, 1]) if noisy_maf.size > 1 else 0.0\n",
    "\n",
    "            # Save per-seed outputs\n",
    "            seed_dir = outdir / f\"seed_{seed}\"\n",
    "            seed_dir.mkdir(parents=True, exist_ok=True)\n",
    "            out_path = seed_dir / f\"{region_tag}.case_maf_ldp.npz\"\n",
    "            np.savez_compressed(\n",
    "                out_path,\n",
    "                noisy_maf=noisy_maf,\n",
    "                noisy_mac_safe=noisy_mac_safe,\n",
    "                eps=float(eps),\n",
    "                seed=int(seed),\n",
    "                region_tag=region_tag,\n",
    "                report_k=int(REPORT_K),\n",
    "                p_grr=float(p_grr),\n",
    "                q_grr=float(q_grr),\n",
    "                zero_reports=int(zero_reports),\n",
    "                report_counts=report_counts,\n",
    "                yri_train_idx=np.array(yri_train_idx, dtype=np.int32),\n",
    "                yri_test_idx=np.array(yri_test_idx, dtype=np.int32),\n",
    "            )\n",
    "\n",
    "            # Cache histogram inputs for eps=1.0 seed=0\n",
    "            if abs(eps - 1.0) < 1e-9 and seed == 0:\n",
    "                hist_cache[region_tag] = (scores_member, scores_nonmember)\n",
    "\n",
    "            rec = {\n",
    "                \"seed\": int(seed),\n",
    "                \"eps\": float(eps),\n",
    "                \"region\": region_tag,\n",
    "                \"n_snps\": int(n_snps),\n",
    "                \"n_train\": int(len(yri_train_idx)),\n",
    "                \"n_test\": int(len(yri_test_idx)),\n",
    "                \"n_ctrl\": int(len(ceu_ctrl_idx)),\n",
    "                \"report_k\": int(REPORT_K),\n",
    "                \"zero_reports\": int(zero_reports),\n",
    "                # privacy\n",
    "                \"lr_threshold_test_95pct\": thresh,\n",
    "                \"lr_power\": lr_power,\n",
    "                \"lr_member_mean\": float(np.mean(scores_member)),\n",
    "                \"lr_nonmember_mean\": float(np.mean(scores_nonmember)),\n",
    "                # utility\n",
    "                **ov,\n",
    "                \"maf_mae\": maf_mae,\n",
    "                \"rmse_maf\": rmse,\n",
    "                \"corr_maf\": corr,\n",
    "            }\n",
    "            records.append(rec)\n",
    "\n",
    "df_per_seed = pd.DataFrame(records)\n",
    "df_per_seed = df_per_seed.sort_values([\"region\", \"eps\", \"seed\"]).reset_index(drop=True)\n",
    "\n",
    "per_seed_csv = TABLE_DIR / \"method3_per_seed.csv\"\n",
    "df_per_seed.to_csv(per_seed_csv, index=False)\n",
    "print(\"Saved per-seed table ->\", per_seed_csv.relative_to(PROJECT_ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Aggregate summary + plots\n",
    "# -------------------------\n",
    "group_cols = [\"region\", \"eps\"]\n",
    "agg_cols = [\"lr_power\", \"jaccard\", \"maf_mae\", \"rmse_maf\", \"corr_maf\", \"true_sig\", \"noisy_sig\"]\n",
    "\n",
    "summary = df_per_seed.groupby(group_cols)[agg_cols].agg([\"mean\", \"std\"]).reset_index()\n",
    "summary.columns = [c[0] if c[1] == \"\" else f\"{c[0]}_{c[1]}\" for c in summary.columns.to_flat_index()]\n",
    "\n",
    "summary_csv = TABLE_DIR / \"method3_summary.csv\"\n",
    "summary.to_csv(summary_csv, index=False)\n",
    "print(\"Saved summary table ->\", summary_csv.relative_to(PROJECT_ROOT))\n",
    "\n",
    "for region_tag in summary[\"region\"].unique():\n",
    "    sub = summary[summary[\"region\"] == region_tag].sort_values(\"eps\")\n",
    "    eps = sub[\"eps\"].to_numpy()\n",
    "\n",
    "    # Privacy curve\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(eps, sub[\"lr_power_mean\"], marker=\"o\", label=\"LR power\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"epsilon (log scale)\")\n",
    "    plt.ylabel(\"LR power (members above 95% nonmember threshold)\")\n",
    "    plt.title(f\"Method 3 Privacy (LR power) \u2014 {region_tag}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{region_tag}.privacy_lr_power.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Utility curve\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(eps, sub[\"jaccard_mean\"], marker=\"o\", label=\"Jaccard\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"epsilon (log scale)\")\n",
    "    plt.ylabel(\"Jaccard(sig_true, sig_noisy)\")\n",
    "    plt.title(f\"Method 3 Utility (GWAS overlap) \u2014 {region_tag}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{region_tag}.utility_jaccard.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # DP distortion curve\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(eps, sub[\"maf_mae_mean\"], marker=\"o\", label=\"MAF MAE\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"epsilon (log scale)\")\n",
    "    plt.ylabel(\"MAF MAE\")\n",
    "    plt.title(f\"Method 3 DP Distortion \u2014 {region_tag}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{region_tag}.dp_distortion.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # LR histogram for eps=1.0 seed=0\n",
    "    if region_tag in hist_cache:\n",
    "        scores_member, scores_nonmember = hist_cache[region_tag]\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(scores_member, bins=25, alpha=0.6, label=\"YRI train (members)\")\n",
    "        plt.hist(scores_nonmember, bins=25, alpha=0.6, label=\"YRI test (nonmembers)\")\n",
    "        plt.xlabel(\"LR score\")\n",
    "        plt.ylabel(\"count\")\n",
    "        plt.title(f\"LR score distributions \u2014 {region_tag} (eps=1.0)\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIG_DIR / f\"{region_tag}.lr_hist_eps_1p0.png\")\n",
    "        plt.close()\n",
    "\n",
    "print(\"Plots saved under\", FIG_DIR.relative_to(PROJECT_ROOT))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}